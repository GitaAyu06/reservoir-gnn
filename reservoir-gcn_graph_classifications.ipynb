{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-geometric\n",
    "%pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import psutil\n",
    "import pynvml\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'using {device}')\n",
    "\n",
    "def set_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/Users/gitaayusalsabila/Documents/0thesis/code/sandbox/dataset/'\n",
    "dataset_path = '/notebooks/dataset/'\n",
    "\n",
    "\n",
    "def data_cleansing(dataset):\n",
    "    # Replace negative values with 0\n",
    "    dataset[dataset < 0] = 0\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    dataset = np.nan_to_num(dataset, nan=0)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def check_and_drop_invalid_graphs(graph_dataset):\n",
    "    num_graphs, num_timepoints, num_nodes, _ = graph_dataset.shape\n",
    "    num_dimensions = 1\n",
    "    \n",
    "    valid_graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        is_valid = True\n",
    "        for t in range(num_timepoints):\n",
    "            adj_matrix = graph_dataset[i, t, :, :]\n",
    "            num_edges = np.sum(adj_matrix > 0)\n",
    "            if num_edges == 0:\n",
    "                is_valid = False\n",
    "                break\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_graphs.append(i)\n",
    "    \n",
    "    cleaned_dataset = graph_dataset[valid_graphs, :, :, :]\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "def convert_to_pyg_data(adj_matrices, inp_features):\n",
    "    pyg_data_list = []\n",
    "    \n",
    "    num_graphs = adj_matrices.shape[0]\n",
    "    num_timepoints = adj_matrices.shape[1]\n",
    "    \n",
    "    for i in range(num_graphs):\n",
    "        for t in range(num_timepoints):\n",
    "            adj_matrix = adj_matrices[i, t]\n",
    "            features = inp_features[i, t]\n",
    "            \n",
    "            # Get edge indices\n",
    "            edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
    "            \n",
    "            # Get edge attributes\n",
    "            edge_attr = torch.tensor(adj_matrix[adj_matrix.nonzero()], dtype=torch.float)\n",
    "            \n",
    "            # Node features\n",
    "            x = torch.tensor(features, dtype=torch.float)\n",
    "            \n",
    "            # Label (timepoint)\n",
    "            y = torch.tensor([t], dtype=torch.long)\n",
    "            \n",
    "            pyg_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            pyg_data_list.append(pyg_data)\n",
    "    \n",
    "    return pyg_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG Dataset\n",
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "MUTAG Dataset -> Num Features = 7, Num Classes = 2\n",
      "\n",
      "Simulated Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1190], edge_attr=[1190], y=[1])\n",
      "Num Features = 8, Num Classes = 8\n",
      "\n",
      "EMCI-AD Dataset\n",
      "Data(x=[35, 8], edge_index=[2, 1178], edge_attr=[1178], y=[1])\n",
      "Num Features = 8, Num Classes = 2\n",
      "\n",
      "SLIM160 Dataset\n",
      "Data(x=[160, 8], edge_index=[2, 19994], edge_attr=[19994], y=[1])\n",
      "Num Features = 8, Num Classes = 3\n"
     ]
    }
   ],
   "source": [
    "## MUTAG\n",
    "print('MUTAG Dataset')\n",
    "mutag_data = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "print(mutag_data[0])\n",
    "\n",
    "train_val_data, test_data = train_test_split(mutag_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.1, random_state=42)\n",
    "mutag_train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "mutag_val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "mutag_test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "mutag_num_features = mutag_data.num_features\n",
    "mutag_num_classes = mutag_data.num_classes\n",
    "print(f'MUTAG Dataset -> Num Features = {mutag_num_features}, Num Classes = {mutag_num_classes}')\n",
    "\n",
    "\n",
    "## Simulated\n",
    "print('\\nSimulated Dataset')\n",
    "simulated_adj = np.load(dataset_path + 'simulated_adj.npy')\n",
    "simulated_adj = simulated_adj[:, :, :, :,0] #only take the domain 1\n",
    "simulated_features = np.load(dataset_path + 'simulated_laplacian_features.npy')\n",
    "simulated_features = simulated_features[:, :, :, :,0]\n",
    "\n",
    "simulated_pyg_data_list = convert_to_pyg_data(simulated_adj, simulated_features)\n",
    "print(simulated_pyg_data_list[0])\n",
    "\n",
    "simulated_num_features = simulated_pyg_data_list[0].x.shape[1]\n",
    "all_labels = torch.cat([data.y for data in simulated_pyg_data_list])\n",
    "simulated_num_classes = torch.unique(all_labels).numel()\n",
    "print(f'Num Features = {simulated_num_features}, Num Classes = {simulated_num_features}')\n",
    "\n",
    "\n",
    "## EMCI-AD\n",
    "print('\\nEMCI-AD Dataset')\n",
    "emci = np.load(dataset_path + 'emci-ad_adj.npy')\n",
    "emci_cleaned = data_cleansing(emci)\n",
    "emci_cleaned = check_and_drop_invalid_graphs(emci_cleaned)\n",
    "emci_features = np.load(dataset_path + 'emci-ad_laplacian_features.npy') \n",
    "emci_pyg_data_list = convert_to_pyg_data(emci_cleaned, emci_features)\n",
    "print(emci_pyg_data_list[0])\n",
    "\n",
    "emci_num_features = emci_pyg_data_list[0].x.shape[1]\n",
    "all_labels = torch.cat([data.y for data in emci_pyg_data_list])\n",
    "emci_num_classes = torch.unique(all_labels).numel()\n",
    "print(f'Num Features = {emci_num_features}, Num Classes = {emci_num_classes}')\n",
    "\n",
    "batch_size = 10\n",
    "set_seed(42)\n",
    "emci_train_val_data, emci_test_data = train_test_split(emci_pyg_data_list, test_size=0.2, random_state=42)\n",
    "emci_train_data, emci_val_data= train_test_split(emci_train_val_data, test_size=0.1, random_state=42)\n",
    "emci_train_loader = DataLoader(emci_train_data, batch_size=batch_size, shuffle=True)\n",
    "emci_val_loader = DataLoader(emci_val_data, batch_size=batch_size, shuffle=False)\n",
    "emci_test_loader = DataLoader(emci_test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "## SLIM160\n",
    "print('\\nSLIM160 Dataset')\n",
    "slim160 = np.load(dataset_path + 'slim160_adj.npy')\n",
    "slim160_cleaned = data_cleansing(slim160)\n",
    "slim160_cleaned = check_and_drop_invalid_graphs(slim160_cleaned)\n",
    "slim160_features = np.load(dataset_path + 'slim160_laplacian_features_8.npy') \n",
    "slim160_pyg_data_list = convert_to_pyg_data(slim160_cleaned, slim160_features)\n",
    "print(slim160_pyg_data_list[0])\n",
    "\n",
    "slim160_num_features = slim160_pyg_data_list[0].x.shape[1]\n",
    "all_labels = torch.cat([data.y for data in slim160_pyg_data_list])\n",
    "slim160_num_classes = torch.unique(all_labels).numel()\n",
    "print(f'Num Features = {slim160_num_features}, Num Classes = {slim160_num_classes}')\n",
    "\n",
    "batch_size = 32\n",
    "set_seed(42)\n",
    "slim160_train_val_data, slim160_test_data = train_test_split(slim160_pyg_data_list, test_size=0.2, random_state=42)\n",
    "slim160_train_data, slim160_val_data= train_test_split(slim160_train_val_data, test_size=0.1, random_state=42)\n",
    "slim160_train_loader = DataLoader(slim160_train_data, batch_size=batch_size, shuffle=True)\n",
    "slim160_val_loader = DataLoader(slim160_val_data, batch_size=batch_size, shuffle=False)\n",
    "slim160_test_loader = DataLoader(slim160_test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters() \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "## GCN Models\n",
    "class GCN1Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN1Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.fc = torch.nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.gcn1(x, adj))\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class GCN2Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN2Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.gcn2 = GCN(hidden_features, hidden_features*2)\n",
    "        self.fc = torch.nn.Linear(hidden_features*2, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.gcn1(x, adj))\n",
    "        x = F.relu(self.gcn2(x, adj))\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "## GCESN Model\n",
    "class RidgeLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha=1):\n",
    "        super(RidgeLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def ridge_regularization(self):\n",
    "        return self.alpha * torch.sum(self.linear.weight ** 2)\n",
    "\n",
    "class GCESN_1layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(GCESN_1layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.ridge_layer = RidgeLayer(hidden_features, hidden_features, ridge_alpha)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / rhoW\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W))))\n",
    "        h = self.ridge_layer(h)\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class GCESN_2layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(GCESN_2layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.ridge_layer_1 = RidgeLayer(hidden_features, 2*hidden_features, ridge_alpha)\n",
    "        self.ridge_layer_2 = RidgeLayer(2*hidden_features, hidden_features, 0.7)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win_1 = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_1 = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_2 = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues_1, _ = torch.linalg.eig(self.W_1)\n",
    "        rhoW_1 = max(abs(eigenvalues_1))\n",
    "        self.W_1 *= self.spectral_radius / rhoW_1\n",
    "\n",
    "        eigenvalues_2, _ = torch.linalg.eig(self.W_2)\n",
    "        rhoW_2 = max(abs(eigenvalues_2))\n",
    "        self.W_2 *= self.spectral_radius / rhoW_2\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win_1)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W_1))))\n",
    "        h = self.ridge_layer_1(h)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W_2))))\n",
    "        h = self.ridge_layer_2(h)\n",
    "\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "## TrainableGCESN Model\n",
    "class SpectralRadiusOptimizerHook:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self):\n",
    "        self.model.adjust_spectral_radius()\n",
    "\n",
    "class TrainableGCESN_1layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(TrainableGCESN_1layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.ridge_layer = RidgeLayer(hidden_features, hidden_features, ridge_alpha)\n",
    "        \n",
    "        self.Win = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1)\n",
    "        self.W = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "    \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W.data *= self.spectral_radius / rhoW\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h_new = F.relu(torch.mm(adj, torch.mm(h, self.W)))\n",
    "            h = (1 - self.leaky_rate) * h + self.leaky_rate * h_new\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "## Decoupled GCESN Model\n",
    "class decoupledGCESN_1layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=1, ridge_alpha=0.9):\n",
    "        super(decoupledGCESN_1layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.spectral_radius = 0.9\n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.gcn1 = GCN(in_features, hidden_features)\n",
    "        self.ridge_layer = RidgeLayer(hidden_features, hidden_features, ridge_alpha)\n",
    "        self.fc = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W   = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / rhoW\n",
    "        \n",
    "    def forward(self, x, adj, batch):\n",
    "        n_node, _ = x.shape\n",
    "        # set_seed(42) \n",
    "        state = torch.zeros(n_node, self.hidden_features).to(device)\n",
    "        h = x\n",
    "\n",
    "        h = F.relu(self.gcn1(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win) + torch.mm(state, self.W)))\n",
    "        \n",
    "        x = self.ridge_layer(state)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class decoupledGCESN_2layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=1, ridge_alpha=0.9):\n",
    "        super(decoupledGCESN_2layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.spectral_radius = 0.9\n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.gcn1 = GCN(in_features, hidden_features)\n",
    "        self.ridge_layer_1 = RidgeLayer(hidden_features, 2*hidden_features, ridge_alpha)\n",
    "        \n",
    "        self.gcn2 = GCN(hidden_features, 2*hidden_features)\n",
    "        self.fc = nn.Linear(2*hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win_1 = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_1   = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.Win_2 = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_2   = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues_1, _ = torch.linalg.eig(self.W_1)\n",
    "        rhoW_1 = max(abs(eigenvalues_1))\n",
    "        self.W_1 *= self.spectral_radius / rhoW_1\n",
    "\n",
    "        eigenvalues_2, _ = torch.linalg.eig(self.W_2)\n",
    "        rhoW_2 = max(abs(eigenvalues_2))\n",
    "        self.W_2 *= self.spectral_radius / rhoW_2\n",
    "        \n",
    "    def forward(self, x, adj, batch):\n",
    "        n_node, _ = x.shape\n",
    "        state = torch.zeros(n_node, self.hidden_features).to(device)\n",
    "        h = x\n",
    "\n",
    "        h = F.relu(self.gcn1(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win_1) + torch.mm(state, self.W_1)))\n",
    "        state = self.ridge_layer_1(state)\n",
    "\n",
    "        h = F.relu(self.gcn2(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win_2) + torch.mm(state, self.W_2)))\n",
    "        \n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings & Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single Functions\n",
    "def single_train(model, loader, val_loader, lr=0.001, num_epochs=100, patience=5, \n",
    "                step_size=50, gamma=0.1, save_path='models/gcn_x.pth', \n",
    "                binary_classification=True, is_esn=False):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    spectral_hook = SpectralRadiusOptimizerHook(model)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage before the epoch\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        for data in loader:\n",
    "            x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if is_esn==True:\n",
    "                spectral_hook()\n",
    "\n",
    "            epoch_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        \n",
    "        # Measure CPU and GPU usage after the epoch\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during the epoch\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                x, edge_index, batch, y = val_data.x.to(device), val_data.edge_index.to(device), val_data.batch.to(device), val_data.y.to(device)\n",
    "                adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "                val_output = model(x, adj_matrix, batch)\n",
    "                val_loss += criterion(val_output, y).item() * val_data.num_graphs\n",
    "\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Early stopping logic considering both training and validation loss\n",
    "        if val_loss < best_val_loss or epoch_loss < best_train_loss:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            total_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), save_path)  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                total_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {epoch_loss}, Val Loss: {val_loss}')\n",
    "        print(f'Time: {epoch_time[-1]:.2f}s, CPU: {cpu_usage_percent[-1]:.2f}%, Memory: {memory_usage[-1]:.2f}GB, GPU: {gpu_usage[-1]:.2f}GB, GPU Util: {gpu_usage_percent[-1]:.2f}%')\n",
    "\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "    avg_epoch_time = np.mean(epoch_time)\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_gpu_usage_percent = np.mean(gpu_usage_percent)\n",
    "    total_training_time = np.sum(epoch_time)\n",
    "    max_cpu_usage_percent = np.max(cpu_usage_percent)\n",
    "    max_memory_usage = np.max(memory_usage)\n",
    "    max_gpu_usage = np.max(gpu_usage)\n",
    "    max_gpu_usage_percent = np.max(gpu_usage_percent)\n",
    "\n",
    "    print(f'Average Time per Epoch: {avg_epoch_time:.2f}s')\n",
    "    print(f'Average CPU Usage: {avg_cpu_usage_percent:.2f}%')\n",
    "    print(f'Average Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'Average GPU Usage: {avg_gpu_usage:.2f}GB')\n",
    "    print(f'Average GPU Utilization: {avg_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    print(f'\\nTotal Training Time: {total_training_time:.2f}s')\n",
    "    print(f'Max CPU Usage: {max_cpu_usage_percent:.2f}%')\n",
    "    print(f'Max Memory Usage: {max_memory_usage:.2f}GB')\n",
    "    print(f'Max GPU Usage: {max_gpu_usage:.2f}GB')\n",
    "    print(f'Max GPU Utilization: {max_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    return\n",
    "\n",
    "def binary_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate True Positives, True Negatives, False Positives, False Negatives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    \n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "def multiclass_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # print(f'Confusion Matrix:\\n{cm}')\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    sensitivity = np.zeros(cm.shape[0])\n",
    "    specificity = np.zeros(cm.shape[0])\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        TP = cm[i, i]\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        TN = np.sum(cm) - (TP + FN + FP)\n",
    "\n",
    "        sensitivity[i] = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity[i] = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    avg_sensitivity = np.mean(sensitivity)\n",
    "    avg_specificity = np.mean(specificity)\n",
    "\n",
    "    return accuracy, avg_sensitivity, avg_specificity\n",
    "    \n",
    "def single_test(model, loader, binary_classification=True):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            pred = output.argmax(dim=1)\n",
    "            y_true.extend(data.y.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    if binary_classification:\n",
    "        accuracy, sensitivity, specificity = binary_evaluation(y_true, y_pred)\n",
    "    else:\n",
    "        accuracy, sensitivity, specificity = multiclass_evaluation(y_true, y_pred)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Average Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Average Specificity: {specificity:.4f}')\n",
    "    return \n",
    "\n",
    "def inference_performance(model, loader):\n",
    "    model.eval()\n",
    "    total_inference_time = 0\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for data in loader:\n",
    "        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "        adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "        \n",
    "        # Measure CPU and GPU usage before inference\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model(x, adj_matrix, batch)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage after inference\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during inference\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        # Calculate inference time\n",
    "        inference_time = end_time - start_time\n",
    "        total_inference_time += inference_time\n",
    "\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_gpu_usage_percent = np.mean(gpu_usage_percent)\n",
    "    total_inference_time = total_inference_time / len(loader)  # Average inference time per batch\n",
    "\n",
    "    print(f'\\nAverage Inference Time per Batch: {total_inference_time:.4f}s')\n",
    "    print(f'Average CPU Usage: {avg_cpu_usage_percent:.2f}%')\n",
    "    print(f'Average Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'Average GPU Usage: {avg_gpu_usage:.2f}GB')\n",
    "    print(f'Average GPU Utilization: {avg_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    return \n",
    "\n",
    "## Multiple Runs\n",
    "def multi_train(model, loader, val_loader, lr=0.001, num_epochs=100, patience=5, \n",
    "                step_size=50, gamma=0.1, binary_classification=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage before the epoch\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        for data in loader:\n",
    "            x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        \n",
    "        # Measure CPU and GPU usage after the epoch\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during the epoch\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                x, edge_index, batch, y = val_data.x.to(device), val_data.edge_index.to(device), val_data.batch.to(device), val_data.y.to(device)\n",
    "                adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "                val_output = model(x, adj_matrix, batch)\n",
    "                val_loss += criterion(val_output, y).item() * val_data.num_graphs\n",
    "\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Early stopping logic considering both training and validation loss\n",
    "        if val_loss < best_val_loss or epoch_loss < best_train_loss:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            total_epoch = epoch + 1\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                total_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "    total_training_time = np.sum(epoch_time)\n",
    "    avg_epoch_time = np.mean(epoch_time)\n",
    "\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "\n",
    "    max_cpu_usage_percent = np.max(cpu_usage_percent)\n",
    "    max_gpu_usage = np.max(gpu_usage)\n",
    "    max_memory_usage = np.max(memory_usage)\n",
    "\n",
    "    return training_loss[-1], total_epoch, total_training_time, avg_epoch_time, avg_cpu_usage_percent, avg_gpu_usage, avg_memory_usage, max_cpu_usage_percent, max_gpu_usage, max_memory_usage \n",
    "   \n",
    "def multi_test(model, loader, binary_classification=True):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            pred = output.argmax(dim=1)\n",
    "            y_true.extend(data.y.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    if binary_classification:\n",
    "        accuracy, sensitivity, specificity = binary_evaluation(y_true, y_pred)\n",
    "    else:\n",
    "        accuracy, sensitivity, specificity = multiclass_evaluation(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "def multi_train_test(model, train_loader, val_loader, test_loader, num_runs=10, \n",
    "                     lr=0.001, num_epochs=100, patience=5, step_size=50, gamma=0.1, \n",
    "                     binary_classification=True, best_model_path='models/best_model.pth'):\n",
    "\n",
    "    all_accuracies = []\n",
    "    all_sensitivities = []\n",
    "    all_specificities = []\n",
    "\n",
    "    all_training_times = []\n",
    "    all_epoch_time = []\n",
    "    all_epoch = []\n",
    "\n",
    "    all_avg_cpu_usages = []\n",
    "    all_avg_gpu_usages = []\n",
    "    all_avg_memory_usages = []\n",
    "\n",
    "    all_max_cpu_usages = []\n",
    "    all_max_gpu_usages = []\n",
    "    all_max_memory_usages = []\n",
    "    \n",
    "    best_accuracy = -float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        set_seed(run)\n",
    "        model.initialize_weights()\n",
    "        loss, total_epoch, training_time, epoch_time, avg_cpu, avg_gpu, avg_memory, max_cpu, max_gpu, max_memory = multi_train(model, train_loader, \n",
    "                                                                                                                val_loader, lr=lr, num_epochs=num_epochs)\n",
    "\n",
    "        accuracy, sensitivity, specificity = multi_test(model, test_loader, binary_classification)\n",
    "\n",
    "        # Check if current model is the best so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Store results\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_sensitivities.append(sensitivity)\n",
    "        all_specificities.append(specificity)\n",
    "\n",
    "        all_training_times.append(training_time)\n",
    "        all_epoch_time.append(epoch_time)\n",
    "        all_epoch.append(total_epoch)\n",
    "\n",
    "        all_avg_cpu_usages.append(avg_cpu)\n",
    "        all_avg_gpu_usages.append(avg_gpu)\n",
    "        all_avg_memory_usages.append(avg_memory)\n",
    "\n",
    "        all_max_cpu_usages.append(max_cpu)\n",
    "        all_max_gpu_usages.append(max_gpu)\n",
    "        all_max_memory_usages.append(max_memory)\n",
    "        \n",
    "        print(f'\\nRun {run+1}/{num_runs} -> Loss: {loss:.5f}, Total Training Time: {training_time:.2f}s')\n",
    "        print(f'  Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    # Save the best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, best_model_path)\n",
    "        print(f'Best model saved to {best_model_path} with accuracy {best_accuracy:.4f}')\n",
    "\n",
    "    # Compute average values across all runs\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_sensitivity = np.mean(all_sensitivities)\n",
    "    avg_specificity = np.mean(all_specificities)\n",
    "\n",
    "    std_accuracy = np.std(all_accuracies)\n",
    "    std_sensitivity = np.std(all_sensitivities)\n",
    "    std_specificity = np.std(all_specificities)\n",
    "\n",
    "    max_accuracy = np.max(all_accuracies)\n",
    "    max_sensitivity = np.max(all_sensitivities)\n",
    "    max_specificity = np.max(all_specificities)\n",
    "\n",
    "    avg_training_time = np.mean(all_training_times)\n",
    "    avg_epoch_time = np.mean(all_epoch_time)\n",
    "    avg_num_epoch = np.mean(all_epoch)\n",
    "\n",
    "    avg_cpu_usage = np.mean(all_avg_cpu_usages)\n",
    "    avg_gpu_usage = np.mean(all_avg_gpu_usages)\n",
    "    avg_memory_usage = np.mean(all_avg_memory_usages)\n",
    "\n",
    "    avg_max_cpu_usage = np.mean(all_max_cpu_usages)\n",
    "    avg_max_gpu_usage = np.mean(all_max_gpu_usages)\n",
    "    avg_max_memory_usage = np.mean(all_max_memory_usages)\n",
    "    \n",
    "\n",
    "    print('Overall Results:')\n",
    "    print(f'  Avg Accuracy: {avg_accuracy:.4f} ± {std_accuracy:.2f}, Avg Sensitivity: {avg_sensitivity:.4f} ± {std_sensitivity:.2f}, Avg Specificity: {avg_specificity:.4f} ± {std_specificity:.2f}')\n",
    "    print(f'  Max Accuracy: {max_accuracy:.4f}, Max Sensitivity: {max_sensitivity:.4f}, Max Specificity: {max_specificity:.4f}')\n",
    "    print(f'  Avg Num Epoch: {avg_num_epoch:.2f}, Avg Training Time: {avg_training_time:.2f}s, Avg Epoch Time: {avg_epoch_time:.2f}s') \n",
    "    print(f'  Avg CPU Usage: {avg_cpu_usage:.2f}%, Avg GPU Usage: {avg_gpu_usage:.2f}%, Avg Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'  Avg Max CPU Usage: {avg_max_cpu_usage:.2f}%, Avg Max GPU Usage: {avg_max_gpu_usage:.2f}GB, Avg Max Memory Usage: {avg_max_memory_usage:.2f}GB')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN 1-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN1Layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 284\n",
      "\n",
      "Epoch 1, Train Loss: 94.14867669343948, Val Loss: 10.549120903015137\n",
      "Time: 0.65s, CPU: 14.30%, Memory: 2.96GB, GPU: 0.01GB, GPU Util: 2.50%\n",
      "Epoch 2, Train Loss: 84.44917124509811, Val Loss: 11.329366564750671\n",
      "Time: 0.02s, CPU: 12.45%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 3, Train Loss: 81.2327116727829, Val Loss: 11.756115853786469\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 4, Train Loss: 80.24670839309692, Val Loss: 11.568962037563324\n",
      "Time: 0.03s, CPU: 46.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 5, Train Loss: 79.62163537740707, Val Loss: 11.206330955028534\n",
      "Time: 0.03s, CPU: 18.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 6, Train Loss: 79.45931965112686, Val Loss: 10.95899909734726\n",
      "Time: 0.03s, CPU: 23.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 7, Train Loss: 79.3338468670845, Val Loss: 10.866144597530365\n",
      "Time: 0.02s, CPU: 22.95%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.50%\n",
      "Epoch 8, Train Loss: 78.86435133218765, Val Loss: 10.888813734054565\n",
      "Time: 0.03s, CPU: 28.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 9, Train Loss: 78.12593364715576, Val Loss: 10.946557223796844\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 10, Train Loss: 77.3753160238266, Val Loss: 10.955286026000977\n",
      "Time: 0.03s, CPU: 19.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 11, Train Loss: 76.7306746840477, Val Loss: 10.897074043750763\n",
      "Time: 0.02s, CPU: 5.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 12, Train Loss: 76.18613654375076, Val Loss: 10.813263058662415\n",
      "Time: 0.02s, CPU: 16.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 13, Train Loss: 75.69723284244537, Val Loss: 10.75046181678772\n",
      "Time: 0.03s, CPU: 14.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 14, Train Loss: 75.17762821912766, Val Loss: 10.717265903949738\n",
      "Time: 0.02s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 15, Train Loss: 74.62532132863998, Val Loss: 10.70331484079361\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 16, Train Loss: 74.05801922082901, Val Loss: 10.704942047595978\n",
      "Time: 0.02s, CPU: 5.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 17, Train Loss: 73.52021759748459, Val Loss: 10.69248229265213\n",
      "Time: 0.02s, CPU: 8.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 18, Train Loss: 73.03481090068817, Val Loss: 10.647140443325043\n",
      "Time: 0.02s, CPU: 6.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 19, Train Loss: 72.61591851711273, Val Loss: 10.577104389667511\n",
      "Time: 0.02s, CPU: 18.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 20, Train Loss: 72.24941998720169, Val Loss: 10.520010888576508\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 21, Train Loss: 71.88123792409897, Val Loss: 10.49483835697174\n",
      "Time: 0.02s, CPU: 11.45%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 22, Train Loss: 71.47626131772995, Val Loss: 10.472456216812134\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 23, Train Loss: 71.08599668741226, Val Loss: 10.458588302135468\n",
      "Time: 0.02s, CPU: 12.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 24, Train Loss: 70.69366067647934, Val Loss: 10.439585745334625\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 25, Train Loss: 70.34002500772476, Val Loss: 10.399213135242462\n",
      "Time: 0.02s, CPU: 15.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 26, Train Loss: 70.03741896152496, Val Loss: 10.385195910930634\n",
      "Time: 0.02s, CPU: 11.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 27, Train Loss: 69.71278196573257, Val Loss: 10.381034910678864\n",
      "Time: 0.03s, CPU: 4.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 28, Train Loss: 69.3977597951889, Val Loss: 10.367452204227448\n",
      "Time: 0.02s, CPU: 14.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 29, Train Loss: 69.11588132381439, Val Loss: 10.33398449420929\n",
      "Time: 0.02s, CPU: 11.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 30, Train Loss: 68.88074815273285, Val Loss: 10.303735435009003\n",
      "Time: 0.02s, CPU: 5.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 31, Train Loss: 68.65177059173584, Val Loss: 10.276626348495483\n",
      "Time: 0.02s, CPU: 22.20%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 32, Train Loss: 68.42995876073837, Val Loss: 10.251536965370178\n",
      "Time: 0.02s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 33, Train Loss: 68.21338987350464, Val Loss: 10.23234486579895\n",
      "Time: 0.02s, CPU: 10.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 34, Train Loss: 67.99820959568024, Val Loss: 10.206809341907501\n",
      "Time: 0.03s, CPU: 15.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 35, Train Loss: 67.80241572856903, Val Loss: 10.170038044452667\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 36, Train Loss: 67.62667125463486, Val Loss: 10.15133947134018\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 37, Train Loss: 67.4356814622879, Val Loss: 10.133846998214722\n",
      "Time: 0.02s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 38, Train Loss: 67.25000822544098, Val Loss: 10.124900043010712\n",
      "Time: 0.02s, CPU: 8.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 39, Train Loss: 67.06902647018433, Val Loss: 10.107819736003876\n",
      "Time: 0.02s, CPU: 11.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 40, Train Loss: 66.91164147853851, Val Loss: 10.082689225673676\n",
      "Time: 0.03s, CPU: 7.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 41, Train Loss: 66.77872562408447, Val Loss: 10.080616772174835\n",
      "Time: 0.02s, CPU: 16.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 42, Train Loss: 66.62415450811386, Val Loss: 10.069080591201782\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 43, Train Loss: 66.48994266986847, Val Loss: 10.03538578748703\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 44, Train Loss: 66.38509142398834, Val Loss: 10.022538006305695\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 45, Train Loss: 66.25860446691513, Val Loss: 10.018356442451477\n",
      "Time: 0.02s, CPU: 6.95%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 46, Train Loss: 66.1278430223465, Val Loss: 10.007299482822418\n",
      "Time: 0.02s, CPU: 10.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 47, Train Loss: 66.0111511349678, Val Loss: 9.98768001794815\n",
      "Time: 0.02s, CPU: 15.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 48, Train Loss: 65.90902823209763, Val Loss: 9.968271553516388\n",
      "Time: 0.02s, CPU: 11.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 49, Train Loss: 65.81068056821823, Val Loss: 9.953934252262115\n",
      "Time: 0.02s, CPU: 8.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 50, Train Loss: 65.71012991666794, Val Loss: 9.94335651397705\n",
      "Time: 0.02s, CPU: 14.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 51, Train Loss: 65.60927158594131, Val Loss: 9.93163526058197\n",
      "Time: 0.03s, CPU: 10.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 52, Train Loss: 65.5132646560669, Val Loss: 9.917327463626862\n",
      "Time: 0.03s, CPU: 16.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 53, Train Loss: 65.42378503084183, Val Loss: 9.902792572975159\n",
      "Time: 0.03s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 54, Train Loss: 65.3376817703247, Val Loss: 9.890028834342957\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 55, Train Loss: 65.25250309705734, Val Loss: 9.878521263599396\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 56, Train Loss: 65.16890716552734, Val Loss: 9.867084324359894\n",
      "Time: 0.03s, CPU: 16.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 57, Train Loss: 65.08844655752182, Val Loss: 9.854976832866669\n",
      "Time: 0.03s, CPU: 12.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 58, Train Loss: 65.01163846254349, Val Loss: 9.842908680438995\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 59, Train Loss: 64.93795162439346, Val Loss: 9.831391274929047\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 60, Train Loss: 64.86566430330276, Val Loss: 9.820055365562439\n",
      "Time: 0.02s, CPU: 11.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 61, Train Loss: 64.7949845790863, Val Loss: 9.808138310909271\n",
      "Time: 0.02s, CPU: 12.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 62, Train Loss: 64.72720247507095, Val Loss: 9.79850023984909\n",
      "Time: 0.03s, CPU: 11.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 63, Train Loss: 64.6604774594307, Val Loss: 9.788969457149506\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 64, Train Loss: 64.59692448377609, Val Loss: 9.77939486503601\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 65, Train Loss: 64.53516662120819, Val Loss: 9.768784940242767\n",
      "Time: 0.03s, CPU: 8.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 66, Train Loss: 64.47936064004898, Val Loss: 9.7579425573349\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 67, Train Loss: 64.42160105705261, Val Loss: 9.74710464477539\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 68, Train Loss: 64.36625915765762, Val Loss: 9.736607372760773\n",
      "Time: 0.03s, CPU: 11.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 69, Train Loss: 64.31163030862808, Val Loss: 9.726476669311523\n",
      "Time: 0.03s, CPU: 15.95%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 70, Train Loss: 64.25781744718552, Val Loss: 9.71640944480896\n",
      "Time: 0.03s, CPU: 13.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 71, Train Loss: 64.2054700255394, Val Loss: 9.70660775899887\n",
      "Time: 0.02s, CPU: 15.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 72, Train Loss: 64.1539198756218, Val Loss: 9.696963429450989\n",
      "Time: 0.02s, CPU: 9.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 73, Train Loss: 64.10389339923859, Val Loss: 9.687154591083527\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 74, Train Loss: 64.05490297079086, Val Loss: 9.6774423122406\n",
      "Time: 0.04s, CPU: 6.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 75, Train Loss: 64.00702410936356, Val Loss: 9.66776579618454\n",
      "Time: 0.02s, CPU: 17.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 76, Train Loss: 63.95981538295746, Val Loss: 9.658611416816711\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 77, Train Loss: 63.91347682476044, Val Loss: 9.649260342121124\n",
      "Time: 0.02s, CPU: 13.45%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 78, Train Loss: 63.8683465719223, Val Loss: 9.639765322208405\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 79, Train Loss: 63.82386785745621, Val Loss: 9.630285501480103\n",
      "Time: 0.02s, CPU: 10.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 80, Train Loss: 63.77995181083679, Val Loss: 9.620512425899506\n",
      "Time: 0.02s, CPU: 10.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 81, Train Loss: 63.7366418838501, Val Loss: 9.610053598880768\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 82, Train Loss: 63.69410294294357, Val Loss: 9.603572487831116\n",
      "Time: 0.02s, CPU: 15.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 83, Train Loss: 63.65177434682846, Val Loss: 9.594095349311829\n",
      "Time: 0.02s, CPU: 5.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 84, Train Loss: 63.612408220767975, Val Loss: 9.584325850009918\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 85, Train Loss: 63.5723916888237, Val Loss: 9.575238525867462\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 86, Train Loss: 63.53133112192154, Val Loss: 9.566178917884827\n",
      "Time: 0.02s, CPU: 10.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 87, Train Loss: 63.49112778902054, Val Loss: 9.556339681148529\n",
      "Time: 0.02s, CPU: 5.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 88, Train Loss: 63.45204722881317, Val Loss: 9.546103477478027\n",
      "Time: 0.02s, CPU: 10.45%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 89, Train Loss: 63.4135959148407, Val Loss: 9.53624814748764\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 90, Train Loss: 63.373966693878174, Val Loss: 9.526624381542206\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 91, Train Loss: 63.335973143577576, Val Loss: 9.517236649990082\n",
      "Time: 0.02s, CPU: 11.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 92, Train Loss: 63.298880875110626, Val Loss: 9.507868587970734\n",
      "Time: 0.03s, CPU: 15.85%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 93, Train Loss: 63.26274412870407, Val Loss: 9.497864842414856\n",
      "Time: 0.02s, CPU: 12.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 94, Train Loss: 63.22554898262024, Val Loss: 9.485034942626953\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 95, Train Loss: 63.19453424215317, Val Loss: 9.4813933968544\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 96, Train Loss: 63.152174055576324, Val Loss: 9.471276104450226\n",
      "Time: 0.02s, CPU: 13.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 97, Train Loss: 63.119642555713654, Val Loss: 9.460289776325226\n",
      "Time: 0.02s, CPU: 13.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 98, Train Loss: 63.08667916059494, Val Loss: 9.452396929264069\n",
      "Time: 0.02s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 99, Train Loss: 63.052121341228485, Val Loss: 9.443501830101013\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 100, Train Loss: 63.01838266849518, Val Loss: 9.432616531848907\n",
      "Time: 0.02s, CPU: 16.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 101, Train Loss: 62.98463064432144, Val Loss: 9.421521127223969\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 102, Train Loss: 62.94965785741806, Val Loss: 9.410912096500397\n",
      "Time: 0.03s, CPU: 24.20%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 103, Train Loss: 62.91418093442917, Val Loss: 9.401190876960754\n",
      "Time: 0.03s, CPU: 24.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 104, Train Loss: 62.87825459241867, Val Loss: 9.391384720802307\n",
      "Time: 0.02s, CPU: 24.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 105, Train Loss: 62.84550142288208, Val Loss: 9.38027411699295\n",
      "Time: 0.03s, CPU: 19.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 106, Train Loss: 62.81181666254997, Val Loss: 9.371829628944397\n",
      "Time: 0.03s, CPU: 28.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 107, Train Loss: 62.780350744724274, Val Loss: 9.36247318983078\n",
      "Time: 0.03s, CPU: 42.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 108, Train Loss: 62.74883192777634, Val Loss: 9.353112280368805\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 109, Train Loss: 62.71777302026749, Val Loss: 9.34404194355011\n",
      "Time: 0.03s, CPU: 21.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 110, Train Loss: 62.6900572180748, Val Loss: 9.333562552928925\n",
      "Time: 0.03s, CPU: 29.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 111, Train Loss: 62.65926742553711, Val Loss: 9.322536885738373\n",
      "Time: 0.03s, CPU: 26.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 112, Train Loss: 62.62611308693886, Val Loss: 9.311008751392365\n",
      "Time: 0.02s, CPU: 15.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 113, Train Loss: 62.59152910113335, Val Loss: 9.297446608543396\n",
      "Time: 0.02s, CPU: 12.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 114, Train Loss: 62.559482127428055, Val Loss: 9.292682111263275\n",
      "Time: 0.03s, CPU: 9.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 115, Train Loss: 62.5304369032383, Val Loss: 9.282491505146027\n",
      "Time: 0.02s, CPU: 17.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 116, Train Loss: 62.50218251347542, Val Loss: 9.27137553691864\n",
      "Time: 0.02s, CPU: 11.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 117, Train Loss: 62.47050580382347, Val Loss: 9.260353446006775\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 118, Train Loss: 62.435924381017685, Val Loss: 9.248276352882385\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 119, Train Loss: 62.40160375833511, Val Loss: 9.239409863948822\n",
      "Time: 0.02s, CPU: 5.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 120, Train Loss: 62.372284501791, Val Loss: 9.228917062282562\n",
      "Time: 0.03s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 121, Train Loss: 62.34287065267563, Val Loss: 9.219450652599335\n",
      "Time: 0.03s, CPU: 11.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 122, Train Loss: 62.31606459617615, Val Loss: 9.210998117923737\n",
      "Time: 0.02s, CPU: 12.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 123, Train Loss: 62.292576402425766, Val Loss: 9.200119972229004\n",
      "Time: 0.02s, CPU: 12.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 124, Train Loss: 62.264599710702896, Val Loss: 9.188136756420135\n",
      "Time: 0.02s, CPU: 8.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 125, Train Loss: 62.23322159051895, Val Loss: 9.17571097612381\n",
      "Time: 0.02s, CPU: 15.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 126, Train Loss: 62.1992851793766, Val Loss: 9.155939519405365\n",
      "Time: 0.02s, CPU: 5.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 127, Train Loss: 62.18389651179314, Val Loss: 9.162333905696869\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 128, Train Loss: 62.1408648788929, Val Loss: 9.149519205093384\n",
      "Time: 0.02s, CPU: 8.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 129, Train Loss: 62.12468704581261, Val Loss: 9.137948155403137\n",
      "Time: 0.02s, CPU: 7.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 130, Train Loss: 62.10132893919945, Val Loss: 9.138131439685822\n",
      "Time: 0.02s, CPU: 14.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 131, Train Loss: 62.05970627069473, Val Loss: 9.156758487224579\n",
      "Time: 0.03s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 132, Train Loss: 61.994183868169785, Val Loss: 9.172351956367493\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 133, Train Loss: 61.93703451752663, Val Loss: 9.056245386600494\n",
      "Time: 0.03s, CPU: 27.95%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 134, Train Loss: 62.02668911218643, Val Loss: 9.057861864566803\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 135, Train Loss: 61.97646179795265, Val Loss: 9.112514555454254\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 136, Train Loss: 61.9110743701458, Val Loss: 9.08584713935852\n",
      "Time: 0.03s, CPU: 16.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 137, Train Loss: 61.9181686937809, Val Loss: 9.051375389099121\n",
      "Time: 0.02s, CPU: 13.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 138, Train Loss: 61.92189210653305, Val Loss: 9.069034159183502\n",
      "Time: 0.03s, CPU: 15.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 139, Train Loss: 61.85966399312019, Val Loss: 9.07349020242691\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 140, Train Loss: 61.8144513964653, Val Loss: 9.015443623065948\n",
      "Time: 0.03s, CPU: 24.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 141, Train Loss: 61.83093038201332, Val Loss: 9.002160429954529\n",
      "Time: 0.03s, CPU: 25.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 142, Train Loss: 61.79627773165703, Val Loss: 9.016369879245758\n",
      "Time: 0.03s, CPU: 11.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 143, Train Loss: 61.73446720838547, Val Loss: 8.996098637580872\n",
      "Time: 0.02s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 144, Train Loss: 61.718107998371124, Val Loss: 8.974083065986633\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 145, Train Loss: 61.691285103559494, Val Loss: 8.9745032787323\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 146, Train Loss: 61.663282722234726, Val Loss: 8.966074883937836\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 147, Train Loss: 61.64080095291138, Val Loss: 8.950619101524353\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 148, Train Loss: 61.618238896131516, Val Loss: 8.93987774848938\n",
      "Time: 0.03s, CPU: 14.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 149, Train Loss: 61.58613845705986, Val Loss: 8.929573595523834\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 150, Train Loss: 61.55570289492607, Val Loss: 8.91859084367752\n",
      "Time: 0.03s, CPU: 10.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 151, Train Loss: 61.52408131957054, Val Loss: 8.908325135707855\n",
      "Time: 0.03s, CPU: 11.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 152, Train Loss: 61.50291755795479, Val Loss: 8.896726369857788\n",
      "Time: 0.03s, CPU: 31.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 153, Train Loss: 61.50034913420677, Val Loss: 8.893183171749115\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 154, Train Loss: 61.44788956642151, Val Loss: 8.884987235069275\n",
      "Time: 0.03s, CPU: 11.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 155, Train Loss: 61.437067806720734, Val Loss: 8.870898485183716\n",
      "Time: 0.02s, CPU: 10.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 156, Train Loss: 61.42144659161568, Val Loss: 8.876758217811584\n",
      "Time: 0.02s, CPU: 11.45%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 157, Train Loss: 61.37317895889282, Val Loss: 8.856067657470703\n",
      "Time: 0.02s, CPU: 8.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 158, Train Loss: 61.35044264793396, Val Loss: 8.809177279472351\n",
      "Time: 0.02s, CPU: 14.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 159, Train Loss: 61.38061413168907, Val Loss: 8.83877545595169\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 160, Train Loss: 61.31821757555008, Val Loss: 8.830117285251617\n",
      "Time: 0.02s, CPU: 23.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 161, Train Loss: 61.31372803449631, Val Loss: 8.817961513996124\n",
      "Time: 0.03s, CPU: 12.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 162, Train Loss: 61.30112341046333, Val Loss: 8.837732970714569\n",
      "Time: 0.03s, CPU: 15.65%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 163, Train Loss: 61.24008125066757, Val Loss: 8.807953298091888\n",
      "Time: 0.03s, CPU: 13.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 164, Train Loss: 61.228355407714844, Val Loss: 8.766397833824158\n",
      "Time: 0.03s, CPU: 11.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 165, Train Loss: 61.22773551940918, Val Loss: 8.771030008792877\n",
      "Time: 0.03s, CPU: 17.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 166, Train Loss: 61.17715427279472, Val Loss: 8.786773681640625\n",
      "Time: 0.03s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 167, Train Loss: 61.11574026942253, Val Loss: 8.763602077960968\n",
      "Time: 0.03s, CPU: 8.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 168, Train Loss: 61.10948419570923, Val Loss: 8.731364607810974\n",
      "Time: 0.03s, CPU: 14.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 169, Train Loss: 61.11580300331116, Val Loss: 8.723604083061218\n",
      "Time: 0.02s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 170, Train Loss: 61.09249418973923, Val Loss: 8.764243125915527\n",
      "Time: 0.03s, CPU: 9.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 171, Train Loss: 61.04987174272537, Val Loss: 8.742389380931854\n",
      "Time: 0.03s, CPU: 15.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 172, Train Loss: 61.04542112350464, Val Loss: 8.69470864534378\n",
      "Time: 0.02s, CPU: 10.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 173, Train Loss: 61.0650155544281, Val Loss: 8.682659268379211\n",
      "Time: 0.03s, CPU: 13.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 174, Train Loss: 61.03719049692154, Val Loss: 8.694086372852325\n",
      "Time: 0.02s, CPU: 13.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 175, Train Loss: 60.98197013139725, Val Loss: 8.671385049819946\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 176, Train Loss: 60.960063844919205, Val Loss: 8.645739555358887\n",
      "Time: 0.02s, CPU: 17.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 177, Train Loss: 60.946383625268936, Val Loss: 8.660602569580078\n",
      "Time: 0.02s, CPU: 10.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 178, Train Loss: 60.92096695303917, Val Loss: 8.649330139160156\n",
      "Time: 0.02s, CPU: 12.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 179, Train Loss: 60.912458926439285, Val Loss: 8.634181022644043\n",
      "Time: 0.03s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 180, Train Loss: 60.895797312259674, Val Loss: 8.628939986228943\n",
      "Time: 0.03s, CPU: 9.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 181, Train Loss: 60.87890312075615, Val Loss: 8.625616729259491\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 182, Train Loss: 60.84349125623703, Val Loss: 8.619647026062012\n",
      "Time: 0.02s, CPU: 13.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 183, Train Loss: 60.846104085445404, Val Loss: 8.626115620136261\n",
      "Time: 0.02s, CPU: 8.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 184, Train Loss: 60.81626492738724, Val Loss: 8.6128368973732\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 185, Train Loss: 60.7973296046257, Val Loss: 8.580983877182007\n",
      "Time: 0.02s, CPU: 11.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 186, Train Loss: 60.794563382864, Val Loss: 8.596168756484985\n",
      "Time: 0.02s, CPU: 11.10%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 187, Train Loss: 60.73200038075447, Val Loss: 8.606106340885162\n",
      "Time: 0.03s, CPU: 13.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 188, Train Loss: 60.702091723680496, Val Loss: 8.619647920131683\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 189, Train Loss: 60.631634056568146, Val Loss: 8.579069674015045\n",
      "Time: 0.02s, CPU: 12.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 190, Train Loss: 60.66954690217972, Val Loss: 8.619976043701172\n",
      "Time: 0.02s, CPU: 8.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 191, Train Loss: 60.614553928375244, Val Loss: 8.553805947303772\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 192, Train Loss: 60.66496983170509, Val Loss: 8.570186197757721\n",
      "Time: 0.03s, CPU: 15.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 193, Train Loss: 60.61143830418587, Val Loss: 8.59003096818924\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 194, Train Loss: 60.54936310648918, Val Loss: 8.560990691184998\n",
      "Time: 0.02s, CPU: 8.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 195, Train Loss: 60.54649320244789, Val Loss: 8.597238957881927\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 196, Train Loss: 60.51593479514122, Val Loss: 8.477285206317902\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 197, Train Loss: 60.62631869316101, Val Loss: 8.4882652759552\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 198, Train Loss: 60.59133914113045, Val Loss: 8.601125478744507\n",
      "Time: 0.02s, CPU: 18.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 199, Train Loss: 60.4575982093811, Val Loss: 8.482614755630493\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 200, Train Loss: 60.550100803375244, Val Loss: 8.481019735336304\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 201, Train Loss: 60.551843136548996, Val Loss: 8.52477103471756\n",
      "Time: 0.02s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 202, Train Loss: 60.497717797756195, Val Loss: 8.498629331588745\n",
      "Time: 0.02s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 203, Train Loss: 60.49383965134621, Val Loss: 8.466332852840424\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 204, Train Loss: 60.4963521361351, Val Loss: 8.473839461803436\n",
      "Time: 0.03s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 205, Train Loss: 60.448533713817596, Val Loss: 8.436841070652008\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 206, Train Loss: 60.450977981090546, Val Loss: 8.414645791053772\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 207, Train Loss: 60.45568370819092, Val Loss: 8.430298268795013\n",
      "Time: 0.02s, CPU: 18.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 208, Train Loss: 60.38402634859085, Val Loss: 8.396111726760864\n",
      "Time: 0.02s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 209, Train Loss: 60.39801099896431, Val Loss: 8.393759429454803\n",
      "Time: 0.02s, CPU: 13.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 210, Train Loss: 60.39083582162857, Val Loss: 8.435886204242706\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 211, Train Loss: 60.33306497335434, Val Loss: 8.410532176494598\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 212, Train Loss: 60.33298748731613, Val Loss: 8.402305841445923\n",
      "Time: 0.03s, CPU: 15.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 213, Train Loss: 60.30906963348389, Val Loss: 8.421527445316315\n",
      "Time: 0.03s, CPU: 12.20%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 214, Train Loss: 60.26225155591965, Val Loss: 8.370256125926971\n",
      "Time: 0.03s, CPU: 18.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 215, Train Loss: 60.278247624635696, Val Loss: 8.351077437400818\n",
      "Time: 0.04s, CPU: 24.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 216, Train Loss: 60.324557304382324, Val Loss: 8.37927907705307\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 217, Train Loss: 60.29467025399208, Val Loss: 8.355807065963745\n",
      "Time: 0.04s, CPU: 29.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 218, Train Loss: 60.30117407441139, Val Loss: 8.368400931358337\n",
      "Time: 0.03s, CPU: 10.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 219, Train Loss: 60.26324841380119, Val Loss: 8.361918032169342\n",
      "Time: 0.03s, CPU: 21.60%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 220, Train Loss: 60.23408108949661, Val Loss: 8.329817354679108\n",
      "Time: 0.02s, CPU: 18.05%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 221, Train Loss: 60.238045424222946, Val Loss: 8.433980941772461\n",
      "Time: 0.03s, CPU: 6.80%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 222, Train Loss: 60.102597415447235, Val Loss: 8.417833149433136\n",
      "Time: 0.02s, CPU: 17.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 223, Train Loss: 60.127210050821304, Val Loss: 8.385029733181\n",
      "Time: 0.03s, CPU: 10.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 3.50%\n",
      "Epoch 224, Train Loss: 60.16584715247154, Val Loss: 8.402775228023529\n",
      "Time: 0.02s, CPU: 22.20%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 225, Train Loss: 60.12325739860535, Val Loss: 8.373871743679047\n",
      "Time: 0.03s, CPU: 17.75%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 226, Train Loss: 60.13825711607933, Val Loss: 8.462563455104828\n",
      "Time: 0.02s, CPU: 5.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 227, Train Loss: 60.02703419327736, Val Loss: 8.426467180252075\n",
      "Time: 0.02s, CPU: 22.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 228, Train Loss: 60.03209027647972, Val Loss: 8.425876200199127\n",
      "Time: 0.03s, CPU: 11.00%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 229, Train Loss: 60.025031834840775, Val Loss: 8.473316431045532\n",
      "Time: 0.03s, CPU: 7.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 230, Train Loss: 59.966664254665375, Val Loss: 8.449560105800629\n",
      "Time: 0.02s, CPU: 7.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 231, Train Loss: 59.97473520040512, Val Loss: 8.366891741752625\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 232, Train Loss: 60.03498628735542, Val Loss: 8.316011130809784\n",
      "Time: 0.02s, CPU: 5.90%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 233, Train Loss: 60.07251140475273, Val Loss: 8.385509848594666\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 234, Train Loss: 60.00608357787132, Val Loss: 8.458980023860931\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 235, Train Loss: 59.909486055374146, Val Loss: 8.413023054599762\n",
      "Time: 0.03s, CPU: 17.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 236, Train Loss: 59.93892166018486, Val Loss: 8.427515923976898\n",
      "Time: 0.03s, CPU: 24.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 237, Train Loss: 59.92392855882645, Val Loss: 8.433598279953003\n",
      "Time: 0.02s, CPU: 24.25%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 238, Train Loss: 59.8944847881794, Val Loss: 8.412443697452545\n",
      "Time: 0.03s, CPU: 36.55%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 239, Train Loss: 59.922219067811966, Val Loss: 8.41002345085144\n",
      "Time: 0.03s, CPU: 23.35%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 240, Train Loss: 59.91087454557419, Val Loss: 8.393070101737976\n",
      "Time: 0.03s, CPU: 30.95%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 241, Train Loss: 59.96546667814255, Val Loss: 8.394164443016052\n",
      "Time: 0.03s, CPU: 26.50%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 242, Train Loss: 59.993457078933716, Val Loss: 8.45321238040924\n",
      "Time: 0.03s, CPU: 8.70%, Memory: 3.20GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Early stopping at epoch 243\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_1l_mutag.pth\n",
      "Average Time per Epoch: 0.03s\n",
      "Average CPU Usage: 12.95%\n",
      "Average Memory Usage: 3.20GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 3.70%\n",
      "\n",
      "Total Training Time: 6.72s\n",
      "Max CPU Usage: 46.65%\n",
      "Max Memory Usage: 3.20GB\n",
      "Max GPU Usage: 0.02GB\n",
      "Max GPU Utilization: 5.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn1_mutag = GCN1Layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes)\n",
    "print(gcn1_mutag)\n",
    "print(f\"Total number of trainable parameters: {(gcn1_mutag.count_parameters())*2}\\n\")\n",
    "single_train(gcn1_mutag, mutag_train_loader, mutag_val_loader, lr=0.01, num_epochs=500, step_size=500, save_path='models/gcn_1l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8158\n",
      "Average Sensitivity (Recall): 0.8462\n",
      "Average Specificity: 0.7500\n",
      "\n",
      "Average Inference Time per Batch: 0.0006s\n",
      "Average CPU Usage: 11.52%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.13GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "gcn1_mutag = GCN1Layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes)\n",
    "gcn1_mutag.load_state_dict(torch.load('models/gcn_1l_mutag.pth'))\n",
    "single_test(gcn1_mutag.to(device), mutag_test_loader)\n",
    "inference_performance(gcn1_mutag.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN1Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 356\n",
      "\n",
      "Epoch 1, Train Loss: 66.93576407432556, Val Loss: 7.578530550003052\n",
      "Time: 0.04s, CPU: 11.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 66.36269629001617, Val Loss: 7.604725003242493\n",
      "Time: 0.04s, CPU: 8.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 3, Train Loss: 66.47167122364044, Val Loss: 7.6201446652412415\n",
      "Time: 0.04s, CPU: 6.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 4, Train Loss: 66.5421656370163, Val Loss: 7.611989200115204\n",
      "Time: 0.05s, CPU: 21.85%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 5, Train Loss: 66.51615309715271, Val Loss: 7.603678464889526\n",
      "Time: 0.04s, CPU: 15.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 6, Train Loss: 66.48370838165283, Val Loss: 7.601607263088226\n",
      "Time: 0.05s, CPU: 18.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 7, Train Loss: 66.46588158607483, Val Loss: 7.602658987045288\n",
      "Time: 0.06s, CPU: 5.70%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 8, Train Loss: 66.45971190929413, Val Loss: 7.604780077934265\n",
      "Time: 0.05s, CPU: 15.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 9, Train Loss: 66.45922017097473, Val Loss: 7.6059828996658325\n",
      "Time: 0.05s, CPU: 6.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 10, Train Loss: 66.45712757110596, Val Loss: 7.606324315071106\n",
      "Time: 0.04s, CPU: 7.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 11, Train Loss: 66.45115315914154, Val Loss: 7.606453776359558\n",
      "Time: 0.04s, CPU: 17.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 12, Train Loss: 66.44426381587982, Val Loss: 7.60684198141098\n",
      "Time: 0.04s, CPU: 16.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 13, Train Loss: 66.43500852584839, Val Loss: 7.607680976390839\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 14, Train Loss: 66.42723047733307, Val Loss: 7.60807603597641\n",
      "Time: 0.05s, CPU: 15.50%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 15, Train Loss: 66.41572344303131, Val Loss: 7.609185099601746\n",
      "Time: 0.04s, CPU: 18.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 16, Train Loss: 66.40833687782288, Val Loss: 7.609852612018585\n",
      "Time: 0.04s, CPU: 26.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 17, Train Loss: 66.39599311351776, Val Loss: 7.6104302406311035\n",
      "Time: 0.05s, CPU: 8.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 18, Train Loss: 66.38390350341797, Val Loss: 7.610936343669891\n",
      "Time: 0.05s, CPU: 13.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 19, Train Loss: 66.36997842788696, Val Loss: 7.612630486488342\n",
      "Time: 0.04s, CPU: 15.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 20, Train Loss: 66.35224604606628, Val Loss: 7.613407492637634\n",
      "Time: 0.04s, CPU: 15.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 21, Train Loss: 66.3158050775528, Val Loss: 7.613374590873718\n",
      "Time: 0.04s, CPU: 14.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 22, Train Loss: 66.31282866001129, Val Loss: 7.613250911235809\n",
      "Time: 0.04s, CPU: 8.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 23, Train Loss: 66.30976831912994, Val Loss: 7.61308079957962\n",
      "Time: 0.04s, CPU: 12.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 24, Train Loss: 66.30698442459106, Val Loss: 7.61310750246048\n",
      "Time: 0.05s, CPU: 12.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 25, Train Loss: 66.3046543598175, Val Loss: 7.6132089495658875\n",
      "Time: 0.04s, CPU: 6.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 26, Train Loss: 66.3025575876236, Val Loss: 7.613247334957123\n",
      "Time: 0.04s, CPU: 16.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 27, Train Loss: 66.30055284500122, Val Loss: 7.613265514373779\n",
      "Time: 0.05s, CPU: 20.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 28, Train Loss: 66.29863262176514, Val Loss: 7.6132824420928955\n",
      "Time: 0.04s, CPU: 12.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 29, Train Loss: 66.29676795005798, Val Loss: 7.6133264899253845\n",
      "Time: 0.05s, CPU: 11.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 30, Train Loss: 66.29499661922455, Val Loss: 7.613353610038757\n",
      "Time: 0.05s, CPU: 8.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 31, Train Loss: 66.29332208633423, Val Loss: 7.613390147686005\n",
      "Time: 0.06s, CPU: 12.50%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 32, Train Loss: 66.29143249988556, Val Loss: 7.613485276699066\n",
      "Time: 0.04s, CPU: 10.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 33, Train Loss: 66.28974997997284, Val Loss: 7.613555073738098\n",
      "Time: 0.04s, CPU: 11.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 34, Train Loss: 66.28802263736725, Val Loss: 7.613565742969513\n",
      "Time: 0.05s, CPU: 7.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 35, Train Loss: 66.2862639427185, Val Loss: 7.613559246063232\n",
      "Time: 0.05s, CPU: 6.10%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 36, Train Loss: 66.28457450866699, Val Loss: 7.613597214221954\n",
      "Time: 0.04s, CPU: 10.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 37, Train Loss: 66.28294014930725, Val Loss: 7.613614559173584\n",
      "Time: 0.04s, CPU: 11.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 38, Train Loss: 66.28117799758911, Val Loss: 7.613649666309357\n",
      "Time: 0.04s, CPU: 14.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 39, Train Loss: 66.27957391738892, Val Loss: 7.613708794116974\n",
      "Time: 0.04s, CPU: 9.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 40, Train Loss: 66.27798533439636, Val Loss: 7.6137277483940125\n",
      "Time: 0.05s, CPU: 15.10%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 41, Train Loss: 66.27423977851868, Val Loss: 7.613728284835815\n",
      "Time: 0.05s, CPU: 44.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 42, Train Loss: 66.27407264709473, Val Loss: 7.613729774951935\n",
      "Time: 0.05s, CPU: 22.20%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 43, Train Loss: 66.2738927602768, Val Loss: 7.613732218742371\n",
      "Time: 0.04s, CPU: 21.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 44, Train Loss: 66.27370977401733, Val Loss: 7.613733768463135\n",
      "Time: 0.05s, CPU: 29.10%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 45, Train Loss: 66.27354431152344, Val Loss: 7.613732755184174\n",
      "Time: 0.04s, CPU: 24.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 46, Train Loss: 66.27337980270386, Val Loss: 7.613733232021332\n",
      "Time: 0.04s, CPU: 25.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 47, Train Loss: 66.27320981025696, Val Loss: 7.613734841346741\n",
      "Time: 0.04s, CPU: 18.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 48, Train Loss: 66.27304887771606, Val Loss: 7.613736629486084\n",
      "Time: 0.05s, CPU: 11.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 49, Train Loss: 66.27288770675659, Val Loss: 7.613739788532257\n",
      "Time: 0.05s, CPU: 7.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 50, Train Loss: 66.27273845672607, Val Loss: 7.613740026950836\n",
      "Time: 0.05s, CPU: 13.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 51, Train Loss: 66.2725760936737, Val Loss: 7.613743245601654\n",
      "Time: 0.05s, CPU: 6.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 52, Train Loss: 66.27241373062134, Val Loss: 7.613749206066132\n",
      "Time: 0.04s, CPU: 12.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 53, Train Loss: 66.27226161956787, Val Loss: 7.613750457763672\n",
      "Time: 0.04s, CPU: 11.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 54, Train Loss: 66.27210593223572, Val Loss: 7.613753318786621\n",
      "Time: 0.04s, CPU: 6.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 55, Train Loss: 66.27195084095001, Val Loss: 7.613759458065033\n",
      "Time: 0.05s, CPU: 9.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 56, Train Loss: 66.27179419994354, Val Loss: 7.61376416683197\n",
      "Time: 0.04s, CPU: 15.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 57, Train Loss: 66.27164316177368, Val Loss: 7.6137672662734985\n",
      "Time: 0.05s, CPU: 6.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 58, Train Loss: 66.271484375, Val Loss: 7.613771200180054\n",
      "Time: 0.05s, CPU: 11.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 59, Train Loss: 66.27132904529572, Val Loss: 7.613776803016663\n",
      "Time: 0.05s, CPU: 16.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 60, Train Loss: 66.27118027210236, Val Loss: 7.613781929016113\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 61, Train Loss: 66.27081775665283, Val Loss: 7.613782525062561\n",
      "Time: 0.04s, CPU: 12.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 62, Train Loss: 66.27080297470093, Val Loss: 7.613782048225403\n",
      "Time: 0.04s, CPU: 6.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 63, Train Loss: 66.27078568935394, Val Loss: 7.613781690597534\n",
      "Time: 0.04s, CPU: 12.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 64, Train Loss: 66.27077031135559, Val Loss: 7.613781273365021\n",
      "Time: 0.04s, CPU: 17.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 65, Train Loss: 66.27075266838074, Val Loss: 7.613782048225403\n",
      "Time: 0.04s, CPU: 14.10%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 66, Train Loss: 66.27073740959167, Val Loss: 7.613782823085785\n",
      "Time: 0.04s, CPU: 10.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 67, Train Loss: 66.27072060108185, Val Loss: 7.6137824058532715\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 68, Train Loss: 66.27070438861847, Val Loss: 7.613782525062561\n",
      "Time: 0.04s, CPU: 10.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 69, Train Loss: 66.27069139480591, Val Loss: 7.613783061504364\n",
      "Time: 0.04s, CPU: 12.50%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 70, Train Loss: 66.27067482471466, Val Loss: 7.6137837171554565\n",
      "Time: 0.04s, CPU: 13.50%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 71, Train Loss: 66.27065801620483, Val Loss: 7.6137837171554565\n",
      "Time: 0.04s, CPU: 11.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 72, Train Loss: 66.27064299583435, Val Loss: 7.613783657550812\n",
      "Time: 0.04s, CPU: 13.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 73, Train Loss: 66.27062857151031, Val Loss: 7.61378413438797\n",
      "Time: 0.04s, CPU: 10.70%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 74, Train Loss: 66.2706116437912, Val Loss: 7.613785147666931\n",
      "Time: 0.04s, CPU: 15.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 75, Train Loss: 66.27059984207153, Val Loss: 7.613785207271576\n",
      "Time: 0.04s, CPU: 13.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 76, Train Loss: 66.27058446407318, Val Loss: 7.613785266876221\n",
      "Time: 0.05s, CPU: 10.70%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 77, Train Loss: 66.27056980133057, Val Loss: 7.613785684108734\n",
      "Time: 0.05s, CPU: 13.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 78, Train Loss: 66.27055311203003, Val Loss: 7.613786101341248\n",
      "Time: 0.05s, CPU: 7.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 79, Train Loss: 66.27053892612457, Val Loss: 7.613787114620209\n",
      "Time: 0.05s, CPU: 11.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 80, Train Loss: 66.27052593231201, Val Loss: 7.613787651062012\n",
      "Time: 0.04s, CPU: 19.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 81, Train Loss: 66.27049088478088, Val Loss: 7.613787651062012\n",
      "Time: 0.04s, CPU: 13.15%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 82, Train Loss: 66.27048444747925, Val Loss: 7.6137882471084595\n",
      "Time: 0.04s, CPU: 11.40%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 83, Train Loss: 66.27048444747925, Val Loss: 7.6137882471084595\n",
      "Time: 0.05s, CPU: 21.10%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 84, Train Loss: 66.27048325538635, Val Loss: 7.6137882471084595\n",
      "Time: 0.04s, CPU: 6.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 85, Train Loss: 66.2704826593399, Val Loss: 7.6137877106666565\n",
      "Time: 0.04s, CPU: 13.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 86, Train Loss: 66.27047991752625, Val Loss: 7.613788187503815\n",
      "Time: 0.04s, CPU: 12.20%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 87, Train Loss: 66.27047848701477, Val Loss: 7.6137882471084595\n",
      "Time: 0.05s, CPU: 22.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 88, Train Loss: 66.2704781293869, Val Loss: 7.613788306713104\n",
      "Time: 0.05s, CPU: 6.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 89, Train Loss: 66.270476937294, Val Loss: 7.613788306713104\n",
      "Time: 0.04s, CPU: 11.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 90, Train Loss: 66.27047443389893, Val Loss: 7.613788306713104\n",
      "Time: 0.04s, CPU: 8.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 91, Train Loss: 66.27047276496887, Val Loss: 7.613788187503815\n",
      "Time: 0.04s, CPU: 11.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 92, Train Loss: 66.27047085762024, Val Loss: 7.613788306713104\n",
      "Time: 0.04s, CPU: 18.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 93, Train Loss: 66.2704690694809, Val Loss: 7.613788306713104\n",
      "Time: 0.04s, CPU: 6.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 94, Train Loss: 66.27046847343445, Val Loss: 7.6137882471084595\n",
      "Time: 0.04s, CPU: 12.70%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 95, Train Loss: 66.27046847343445, Val Loss: 7.6137882471084595\n",
      "Time: 0.04s, CPU: 11.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 96, Train Loss: 66.27046608924866, Val Loss: 7.613788187503815\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 97, Train Loss: 66.27046751976013, Val Loss: 7.61378812789917\n",
      "Time: 0.04s, CPU: 13.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 98, Train Loss: 66.27046358585358, Val Loss: 7.613788068294525\n",
      "Time: 0.05s, CPU: 15.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 99, Train Loss: 66.27046298980713, Val Loss: 7.613788187503815\n",
      "Time: 0.05s, CPU: 14.50%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 100, Train Loss: 66.27046179771423, Val Loss: 7.61378812789917\n",
      "Time: 0.05s, CPU: 14.35%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 101, Train Loss: 66.27045691013336, Val Loss: 7.613788187503815\n",
      "Time: 0.05s, CPU: 11.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 102, Train Loss: 66.27045691013336, Val Loss: 7.61378812789917\n",
      "Time: 0.05s, CPU: 13.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 103, Train Loss: 66.27045631408691, Val Loss: 7.613788068294525\n",
      "Time: 0.04s, CPU: 15.55%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 104, Train Loss: 66.27045810222626, Val Loss: 7.613788068294525\n",
      "Time: 0.04s, CPU: 9.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 105, Train Loss: 66.27045750617981, Val Loss: 7.61378800868988\n",
      "Time: 0.04s, CPU: 17.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 106, Train Loss: 66.27045691013336, Val Loss: 7.61378800868988\n",
      "Time: 0.04s, CPU: 32.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 107, Train Loss: 66.2704553604126, Val Loss: 7.613787949085236\n",
      "Time: 0.05s, CPU: 7.30%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 108, Train Loss: 66.2704541683197, Val Loss: 7.613788068294525\n",
      "Time: 0.05s, CPU: 11.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 109, Train Loss: 66.2704553604126, Val Loss: 7.61378800868988\n",
      "Time: 0.05s, CPU: 13.75%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 110, Train Loss: 66.27045595645905, Val Loss: 7.61378800868988\n",
      "Time: 0.04s, CPU: 6.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.50%\n",
      "Epoch 111, Train Loss: 66.2704565525055, Val Loss: 7.613787949085236\n",
      "Time: 0.05s, CPU: 25.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 112, Train Loss: 66.2704565525055, Val Loss: 7.613787889480591\n",
      "Time: 0.04s, CPU: 18.05%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 113, Train Loss: 66.27045714855194, Val Loss: 7.613788545131683\n",
      "Time: 0.04s, CPU: 6.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 114, Train Loss: 66.2704565525055, Val Loss: 7.613788545131683\n",
      "Time: 0.04s, CPU: 15.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 115, Train Loss: 66.27045774459839, Val Loss: 7.6137890219688416\n",
      "Time: 0.04s, CPU: 5.90%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 116, Train Loss: 66.2704565525055, Val Loss: 7.613788485527039\n",
      "Time: 0.05s, CPU: 14.60%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 117, Train Loss: 66.27045595645905, Val Loss: 7.613788485527039\n",
      "Time: 0.04s, CPU: 16.45%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 118, Train Loss: 66.27045595645905, Val Loss: 7.613788485527039\n",
      "Time: 0.05s, CPU: 19.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 119, Train Loss: 66.27045595645905, Val Loss: 7.613787829875946\n",
      "Time: 0.04s, CPU: 13.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 120, Train Loss: 66.2704565525055, Val Loss: 7.613787770271301\n",
      "Time: 0.05s, CPU: 27.80%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 121, Train Loss: 66.27045559883118, Val Loss: 7.613787770271301\n",
      "Time: 0.05s, CPU: 20.00%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 122, Train Loss: 66.27045619487762, Val Loss: 7.613787770271301\n",
      "Time: 0.04s, CPU: 14.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 123, Train Loss: 66.27045619487762, Val Loss: 7.613787770271301\n",
      "Time: 0.04s, CPU: 15.95%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 124, Train Loss: 66.27045488357544, Val Loss: 7.613788366317749\n",
      "Time: 0.04s, CPU: 6.25%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 125, Train Loss: 66.27045559883118, Val Loss: 7.613788366317749\n",
      "Time: 0.04s, CPU: 16.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 126, Train Loss: 66.27045679092407, Val Loss: 7.613787770271301\n",
      "Time: 0.04s, CPU: 14.40%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 127, Train Loss: 66.27045679092407, Val Loss: 7.613787829875946\n",
      "Time: 0.04s, CPU: 6.65%, Memory: 3.21GB, GPU: 0.02GB, GPU Util: 5.50%\n",
      "Early stopping at epoch 128\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABF80lEQVR4nO3dd3wVVf7G8eemN3IDAVIk9CgdERADApYoTaTZ2KhBURYMSJEVWaSqYIcFFGwLNmTFHyColICAivQmShEUCQIJK5iElkJyfn9gZrmEQAiBmwmf98t5kXvm3JnvHK7mcebMXIcxxggAAMCGPNxdAAAAQFERZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZAAAgG0RZIDLpEePHqpatWqR3jtq1Cg5HI7iLaiE+e233+RwODR9+vQrvm+Hw6FRo0ZZr6dPny6Hw6Hffvvtgu+tWrWqevToUaz1XMpnBbjaEWRw1XE4HIVali9f7u5Sr3pPPvmkHA6Hdu/eXWCfYcOGyeFw6IcffriClV28AwcOaNSoUdq8ebO7S7HkhclXX33V3aUARebl7gKAK+3DDz90ef3BBx8oMTExX3vt2rUvaT/vvPOOcnNzi/TeZ599Vs8888wl7b80iIuL06RJkzRjxgyNGDHinH0++eQT1a9fXw0aNCjyfh566CE98MAD8vX1LfI2LuTAgQMaPXq0qlatquuvv95l3aV8VoCrHUEGV50HH3zQ5fXq1auVmJiYr/1sJ06cUEBAQKH34+3tXaT6JMnLy0teXvzr2axZM9WsWVOffPLJOYPMqlWrtGfPHr344ouXtB9PT095enpe0jYuxaV8VoCrHZeWgHO45ZZbVK9ePW3YsEGtWrVSQECA/vnPf0qSPv/8c3Xo0EGRkZHy9fVVjRo19NxzzyknJ8dlG2fPezjzNP7bb7+tGjVqyNfXV02bNtW6detc3nuuOTIOh0N9+/bV3LlzVa9ePfn6+qpu3bpauHBhvvqXL1+uJk2ayM/PTzVq1NBbb71V6Hk33377re69915VrlxZvr6+ioqK0sCBA3Xy5Ml8xxcUFKT9+/erc+fOCgoKUoUKFTR48OB8Y5GamqoePXrI6XQqJCRE8fHxSk1NvWAt0umzMjt27NDGjRvzrZsxY4YcDoe6d++urKwsjRgxQo0bN5bT6VRgYKBatmypZcuWXXAf55ojY4zR888/r0qVKikgIEC33nqrfvrpp3zvPXLkiAYPHqz69esrKChIwcHBateunbZs2WL1Wb58uZo2bSpJeuSRR6zLl3nzg841R+b48eN66qmnFBUVJV9fX1133XV69dVXZYxx6Xcxn4uiOnTokHr27KmwsDD5+fmpYcOGev/99/P1mzlzpho3bqwyZcooODhY9evX17/+9S9rfXZ2tkaPHq3o6Gj5+fkpNDRUN998sxITE4utVlx9+F8+oACHDx9Wu3bt9MADD+jBBx9UWFiYpNO/9IKCgjRo0CAFBQXp66+/1ogRI5Senq5XXnnlgtudMWOGjh49qr///e9yOBx6+eWX1bVrV/36668X/D/z7777TrNnz9YTTzyhMmXKaOLEierWrZuSkpIUGhoqSdq0aZPatm2riIgIjR49Wjk5ORozZowqVKhQqOOeNWuWTpw4oT59+ig0NFRr167VpEmT9Pvvv2vWrFkufXNyctSmTRs1a9ZMr776qpYsWaLXXntNNWrUUJ8+fSSdDgSdOnXSd999p969e6t27dqaM2eO4uPjC1VPXFycRo8erRkzZuiGG25w2fenn36qli1bqnLlyvrjjz/07rvvqnv37nr88cd19OhRvffee2rTpo3Wrl2b73LOhYwYMULPP/+82rdvr/bt22vjxo268847lZWV5dLv119/1dy5c3XvvfeqWrVqSklJ0VtvvaXWrVtr27ZtioyMVO3atTVmzBiNGDFCvXr1UsuWLSVJzZs3P+e+jTG6++67tWzZMvXs2VPXX3+9Fi1apH/84x/av3+/xo8f79K/MJ+Lojp58qRuueUW7d69W3379lW1atU0a9Ys9ejRQ6mpqerfv78kKTExUd27d9ftt9+ul156SZK0fft2rVy50uozatQojRs3To899phuvPFGpaena/369dq4caPuuOOOS6oTVzEDXOUSEhLM2f8qtG7d2kgyU6dOzdf/xIkT+dr+/ve/m4CAAJORkWG1xcfHmypVqliv9+zZYySZ0NBQc+TIEav9888/N5LM/PnzrbaRI0fmq0mS8fHxMbt377batmzZYiSZSZMmWW0dO3Y0AQEBZv/+/Vbbrl27jJeXV75tnsu5jm/cuHHG4XCYvXv3uhyfJDNmzBiXvo0aNTKNGze2Xs+dO9dIMi+//LLVdurUKdOyZUsjyUybNu2CNTVt2tRUqlTJ5OTkWG0LFy40ksxbb71lbTMzM9PlfX/++acJCwszjz76qEu7JDNy5Ejr9bRp04wks2fPHmOMMYcOHTI+Pj6mQ4cOJjc31+r3z3/+00gy8fHxVltGRoZLXcac/rv29fV1GZt169YVeLxnf1byxuz555936XfPPfcYh8Ph8hko7OfiXPI+k6+88kqBfSZMmGAkmY8++shqy8rKMjExMSYoKMikp6cbY4zp37+/CQ4ONqdOnSpwWw0bNjQdOnQ4b03AxeLSElAAX19fPfLII/na/f39rZ+PHj2qP/74Qy1bttSJEye0Y8eOC273/vvvV9myZa3Xef93/uuvv17wvbGxsapRo4b1ukGDBgoODrbem5OToyVLlqhz586KjIy0+tWsWVPt2rW74PYl1+M7fvy4/vjjDzVv3lzGGG3atClf/969e7u8btmypcuxfPXVV/Ly8rLO0Ein56T069evUPVIp+c1/f777/rmm2+sthkzZsjHx0f33nuvtU0fHx9JUm5uro4cOaJTp06pSZMm57wsdT5LlixRVlaW+vXr53I5bsCAAfn6+vr6ysPj9H9Kc3JydPjwYQUFBem666676P3m+eqrr+Tp6aknn3zSpf2pp56SMUYLFixwab/Q5+JSfPXVVwoPD1f37t2tNm9vbz355JM6duyYVqxYIUkKCQnR8ePHz3uZKCQkRD/99JN27dp1yXUBeQgyQAGuueYa6xfjmX766Sd16dJFTqdTwcHBqlChgjVROC0t7YLbrVy5ssvrvFDz559/XvR7896f995Dhw7p5MmTqlmzZr5+52o7l6SkJPXo0UPlypWz5r20bt1aUv7j8/Pzy3fJ6sx6JGnv3r2KiIhQUFCQS7/rrruuUPVI0gMPPCBPT0/NmDFDkpSRkaE5c+aoXbt2LqHw/fffV4MGDaz5FxUqVNCXX35ZqL+XM+3du1eSFB0d7dJeoUIFl/1Jp0PT+PHjFR0dLV9fX5UvX14VKlTQDz/8cNH7PXP/kZGRKlOmjEt73p10efXludDn4lLs3btX0dHRVlgrqJYnnnhC1157rdq1a6dKlSrp0UcfzTdPZ8yYMUpNTdW1116r+vXr6x//+EeJv20eJR9BBijAmWcm8qSmpqp169basmWLxowZo/nz5ysxMdGaE1CYW2gLujvGnDWJs7jfWxg5OTm644479OWXX2rIkCGaO3euEhMTrUmpZx/flbrTp2LFirrjjjv0f//3f8rOztb8+fN19OhRxcXFWX0++ugj9ejRQzVq1NB7772nhQsXKjExUbfddttlvbV57NixGjRokFq1aqWPPvpIixYtUmJiourWrXvFbqm+3J+LwqhYsaI2b96sefPmWfN72rVr5zIXqlWrVvrll1/073//W/Xq1dO7776rG264Qe++++4VqxOlD5N9gYuwfPlyHT58WLNnz1arVq2s9j179rixqv+pWLGi/Pz8zvkAufM9VC7P1q1b9fPPP+v999/Xww8/bLVfyl0lVapU0dKlS3Xs2DGXszI7d+68qO3ExcVp4cKFWrBggWbMmKHg4GB17NjRWv/ZZ5+pevXqmj17tsvloJEjRxapZknatWuXqlevbrX/97//zXeW47PPPtOtt96q9957z6U9NTVV5cuXt15fzJOaq1SpoiVLlujo0aMuZ2XyLl3m1XclVKlSRT/88INyc3NdzsqcqxYfHx917NhRHTt2VG5urp544gm99dZbGj58uHVGsFy5cnrkkUf0yCOP6NixY2rVqpVGjRqlxx577IodE0oXzsgAFyHv/3zP/D/drKwsvfnmm+4qyYWnp6diY2M1d+5cHThwwGrfvXt3vnkVBb1fcj0+Y4zLLbQXq3379jp16pSmTJliteXk5GjSpEkXtZ3OnTsrICBAb775phYsWKCuXbvKz8/vvLWvWbNGq1atuuiaY2Nj5e3trUmTJrlsb8KECfn6enp65jvzMWvWLO3fv9+lLTAwUJIKddt5+/btlZOTo8mTJ7u0jx8/Xg6Ho9DznYpD+/btlZycrP/85z9W26lTpzRp0iQFBQVZlx0PHz7s8j4PDw/rIYWZmZnn7BMUFKSaNWta64Gi4IwMcBGaN2+usmXLKj4+3np8/ocffnhFT+FfyKhRo7R48WK1aNFCffr0sX4h1qtX74KPx69Vq5Zq1KihwYMHa//+/QoODtb//d//XdJci44dO6pFixZ65pln9Ntvv6lOnTqaPXv2Rc8fCQoKUufOna15MmdeVpKku+66S7Nnz1aXLl3UoUMH7dmzR1OnTlWdOnV07Nixi9pX3vNwxo0bp7vuukvt27fXpk2btGDBApezLHn7HTNmjB555BE1b95cW7du1ccff+xyJkeSatSooZCQEE2dOlVlypRRYGCgmjVrpmrVquXbf8eOHXXrrbdq2LBh+u2339SwYUMtXrxYn3/+uQYMGOAysbc4LF26VBkZGfnaO3furF69eumtt95Sjx49tGHDBlWtWlWfffaZVq5cqQkTJlhnjB577DEdOXJEt912mypVqqS9e/dq0qRJuv766635NHXq1NEtt9yixo0bq1y5clq/fr0+++wz9e3bt1iPB1cZ99wsBZQcBd1+Xbdu3XP2X7lypbnpppuMv7+/iYyMNE8//bRZtGiRkWSWLVtm9Svo9utz3eqqs24HLuj264SEhHzvrVKlisvtwMYYs3TpUtOoUSPj4+NjatSoYd59913z1FNPGT8/vwJG4X+2bdtmYmNjTVBQkClfvrx5/PHHrdt5z7x1OD4+3gQGBuZ7/7lqP3z4sHnooYdMcHCwcTqd5qGHHjKbNm0q9O3Xeb788ksjyUREROS75Tk3N9eMHTvWVKlSxfj6+ppGjRqZL774It/fgzEXvv3aGGNycnLM6NGjTUREhPH39ze33HKL+fHHH/ONd0ZGhnnqqaesfi1atDCrVq0yrVu3Nq1bt3bZ7+eff27q1Klj3Qqfd+znqvHo0aNm4MCBJjIy0nh7e5vo6GjzyiuvuNwOnncshf1cnC3vM1nQ8uGHHxpjjElJSTGPPPKIKV++vPHx8TH169fP9/f22WefmTvvvNNUrFjR+Pj4mMqVK5u///3v5uDBg1af559/3tx4440mJCTE+Pv7m1q1apkXXnjBZGVlnbdO4HwcxpSg/5UEcNl07tyZW18BlDrMkQFKobO/TmDXrl366quvdMstt7inIAC4TDgjA5RCERER6tGjh6pXr669e/dqypQpyszM1KZNm/I9GwUA7IzJvkAp1LZtW33yySdKTk6Wr6+vYmJiNHbsWEIMgFKHMzIAAMC2mCMDAABsiyADAABsq9TPkcnNzdWBAwdUpkyZi3pEOAAAcB9jjI4eParIyMh8X1p6plIfZA4cOKCoqCh3lwEAAIpg3759qlSpUoHrS32QyXt89r59+xQcHOzmagAAQGGkp6crKirK5YtTz6XUB5m8y0nBwcEEGQAAbOZC00KY7AsAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIFNEOblGS7enuLsMAACuagSZIjDGaPT8n9Tz/fV6ccEOGWPcXRIAAFclgkwRRTj9JUlTV/yiIf/3g07l5Lq5IgAArj5e7i7AjhwOh/rcUkPlAr01dPZWfbr+dx05nq1/PXC9DqZl6OeUo9p96Jj2/3lSB9JO6mBahk5m5cjP20MBPl7y9/aUn4+nArw95e/jKT9vTwX4eMr/r9deHg55ejjk4fjrTw+HPB0OeeX97CFrnafjf+vP7OvhoXxtZ24zbxteHh6n+561LQ+PM7f/v205HA53Dz8AABaHKeXXRdLT0+V0OpWWlqbg4OBi3/6in5LV75NNyjqVK4dDKt2jKXl5OOTt6SEfL4/Tf3o6rJ8DfDxVxs9bZfy8FOTrJQ+HQw6H9L/s89dr6a8/Hda6022nX+St+9/Pf/3p+KvVek/+7Z35Hv3V/3zbsypznH970v/2f3bNZ+7vf+35t5dXs/L6lyKlKeCWniM5rRT91UgqXcfjKEWftoZRIapWPrBYt1nY39+ckblEbeqG68NHb9TjH6xXesYp+Xt76tqwIEWHlVFU2QBFhPgp0umvID8vnczKUUZ2jk5k5ehk9l9L1imdzMrViexTysg6vS4n1yjHGOXkGuX+9WdOrs742Vh9cs/6MydX52g7a32+tv/tJ/cCQexUrtGp3NO1AwAgSWO71C/2IFNYBJli0Kx6qNYOi9V/j2bqmhB/eXjYN2UbczrMuISovFCUa3Qq1yjrVK6ycnKVdSpX2X/9mXUqVyeycnQ0M1tHM07pWOYpGXN6e3lnqYxOn7EyMn/9ae0037qz+//1jzWx2pj82zvzGM63PeXt32o3+bYnl/ecuT3XfejM4ylge3nvMmcdQ2lRmo6mlP3VlKq/G6n0/btTmoQ7fd22b4JMMfHz9lRUuQB3l3HJHA6HPB2n58wAAFDScdcSAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLYIMAACwLbcHmf379+vBBx9UaGio/P39Vb9+fa1fv95ab4zRiBEjFBERIX9/f8XGxmrXrl1urBgAAJQUbg0yf/75p1q0aCFvb28tWLBA27Zt02uvvaayZctafV5++WVNnDhRU6dO1Zo1axQYGKg2bdooIyPDjZUDAICSwGGMMe7a+TPPPKOVK1fq22+/Ped6Y4wiIyP11FNPafDgwZKktLQ0hYWFafr06XrggQcuuI/09HQ5nU6lpaUpODi4WOsHAACXR2F/f7v1jMy8efPUpEkT3XvvvapYsaIaNWqkd955x1q/Z88eJScnKzY21mpzOp1q1qyZVq1adc5tZmZmKj093WUBAAClk1uDzK+//qopU6YoOjpaixYtUp8+ffTkk0/q/ffflyQlJydLksLCwlzeFxYWZq0727hx4+R0Oq0lKirq8h4EAABwG7cGmdzcXN1www0aO3asGjVqpF69eunxxx/X1KlTi7zNoUOHKi0tzVr27dtXjBUDAICSxK1BJiIiQnXq1HFpq127tpKSkiRJ4eHhkqSUlBSXPikpKda6s/n6+io4ONhlAQAApZNbg0yLFi20c+dOl7aff/5ZVapUkSRVq1ZN4eHhWrp0qbU+PT1da9asUUxMzBWtFQAAlDxe7tz5wIED1bx5c40dO1b33Xef1q5dq7fffltvv/22JMnhcGjAgAF6/vnnFR0drWrVqmn48OGKjIxU586d3Vk6AAAoAdwaZJo2bao5c+Zo6NChGjNmjKpVq6YJEyYoLi7O6vP000/r+PHj6tWrl1JTU3XzzTdr4cKF8vPzc2PlAACgJHDrc2SuBJ4jAwCA/djiOTIAAACXgiADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsiyADAABsy61BZtSoUXI4HC5LrVq1rPUZGRlKSEhQaGiogoKC1K1bN6WkpLixYgAAUJK4/YxM3bp1dfDgQWv57rvvrHUDBw7U/PnzNWvWLK1YsUIHDhxQ165d3VgtAAAoSbzcXoCXl8LDw/O1p6Wl6b333tOMGTN02223SZKmTZum2rVra/Xq1brpppuudKkAAKCEcfsZmV27dikyMlLVq1dXXFyckpKSJEkbNmxQdna2YmNjrb61atVS5cqVtWrVqgK3l5mZqfT0dJcFAACUTm4NMs2aNdP06dO1cOFCTZkyRXv27FHLli119OhRJScny8fHRyEhIS7vCQsLU3JycoHbHDdunJxOp7VERUVd5qMAAADu4tZLS+3atbN+btCggZo1a6YqVaro008/lb+/f5G2OXToUA0aNMh6nZ6eTpgBAKCUcvulpTOFhITo2muv1e7duxUeHq6srCylpqa69ElJSTnnnJo8vr6+Cg4OdlkAAEDpVKKCzLFjx/TLL78oIiJCjRs3lre3t5YuXWqt37lzp5KSkhQTE+PGKgEAQEnh1ktLgwcPVseOHVWlShUdOHBAI0eOlKenp7p37y6n06mePXtq0KBBKleunIKDg9WvXz/FxMRwxxIAAJDk5iDz+++/q3v37jp8+LAqVKigm2++WatXr1aFChUkSePHj5eHh4e6deumzMxMtWnTRm+++aY7SwYAACWIwxhj3F3E5ZSeni6n06m0tDTmywAAYBOF/f1doubIAAAAXAyCDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC0vdxcAACjZcnJylJ2d7e4yUMp4e3vL09PzkrdDkAEAnJMxRsnJyUpNTXV3KSilQkJCFB4eLofDUeRtEGQAAOeUF2IqVqyogICAS/plA5zJGKMTJ07o0KFDkqSIiIgib4sgAwDIJycnxwoxoaGh7i4HpZC/v78k6dChQ6pYsWKRLzOVmMm+L774ohwOhwYMGGC1ZWRkKCEhQaGhoQoKClK3bt2UkpLiviIB4CqRNycmICDAzZWgNMv7fF3KHKwSEWTWrVunt956Sw0aNHBpHzhwoObPn69Zs2ZpxYoVOnDggLp27eqmKgHg6sPlJFxOxfH5cnuQOXbsmOLi4vTOO++obNmyVntaWpree+89vf7667rtttvUuHFjTZs2Td9//71Wr17txooBAEBJ4fYgk5CQoA4dOig2NtalfcOGDcrOznZpr1WrlipXrqxVq1YVuL3MzEylp6e7LAAAXIqqVatqwoQJhe6/fPlyORwO7vi6AtwaZGbOnKmNGzdq3Lhx+dYlJyfLx8dHISEhLu1hYWFKTk4ucJvjxo2T0+m0lqioqOIuGwBQQjkcjvMuo0aNKtJ2161bp169ehW6f/PmzXXw4EE5nc4i7a+wCExuvGtp37596t+/vxITE+Xn51ds2x06dKgGDRpkvU5PTyfMAMBV4uDBg9bP//nPfzRixAjt3LnTagsKCrJ+NsYoJydHXl4X/lVYoUKFi6rDx8dH4eHhF/UeFI3bzshs2LBBhw4d0g033CAvLy95eXlpxYoVmjhxory8vBQWFqasrKx8KTMlJeW8Hw5fX18FBwe7LACAq0N4eLi1OJ1OORwO6/WOHTtUpkwZLViwQI0bN5avr6++++47/fLLL+rUqZPCwsIUFBSkpk2basmSJS7bPfvSksPh0LvvvqsuXbooICBA0dHRmjdvnrX+7DMl06dPV0hIiBYtWqTatWsrKChIbdu2dQlep06d0pNPPqmQkBCFhoZqyJAhio+PV+fOnYs8Hn/++acefvhhlS1bVgEBAWrXrp127dplrd+7d686duyosmXLKjAwUHXr1tVXX31lvTcuLk4VKlSQv7+/oqOjNW3atCLXcrm4Lcjcfvvt2rp1qzZv3mwtTZo0UVxcnPWzt7e3li5dar1n586dSkpKUkxMjLvKBoCrljFGJ7JOuWUxxhTbcTzzzDN68cUXtX37djVo0EDHjh1T+/bttXTpUm3atElt27ZVx44dlZSUdN7tjB49Wvfdd59++OEHtW/fXnFxcTpy5EiB/U+cOKFXX31VH374ob755hslJSVp8ODB1vqXXnpJH3/8saZNm6aVK1cqPT1dc+fOvaRj7dGjh9avX6958+Zp1apVMsaoffv21u3OCQkJyszM1DfffKOtW7fqpZdess5aDR8+XNu2bdOCBQu0fft2TZkyReXLl7+kei6HIl1a2rdvnxwOhypVqiRJWrt2rWbMmKE6deoU+hpimTJlVK9ePZe2wMBAhYaGWu09e/bUoEGDVK5cOQUHB6tfv36KiYnRTTfdVJSyAQCX4GR2juqMWOSWfW8b00YBPsUzG2LMmDG64447rNflypVTw4YNrdfPPfec5syZo3nz5qlv374FbqdHjx7q3r27JGns2LGaOHGi1q5dq7Zt256zf3Z2tqZOnaoaNWpIkvr27asxY8ZY6ydNmqShQ4eqS5cukqTJkydbZ0eKYteuXZo3b55Wrlyp5s2bS5I+/vhjRUVFae7cubr33nuVlJSkbt26qX79+pKk6tWrW+9PSkpSo0aN1KRJE0mnz0qVREU6I/O3v/1Ny5Ytk3R6Uu4dd9yhtWvXatiwYS5/KZdq/Pjxuuuuu9StWze1atVK4eHhmj17drFtHwBw9cn7xZzn2LFjGjx4sGrXrq2QkBAFBQVp+/btFzwjc+azzwIDAxUcHGw9cv9cAgICrBAjnX4sf17/tLQ0paSk6MYbb7TWe3p6qnHjxhd1bGfavn27vLy81KxZM6stNDRU1113nbZv3y5JevLJJ/X888+rRYsWGjlypH744Qerb58+fTRz5kxdf/31evrpp/X9998XuZbLqUjx9scff7QG+9NPP1W9evW0cuVKLV68WL1799aIESOKVMzy5ctdXvv5+emNN97QG2+8UaTtAQCKj7+3p7aNaeO2fReXwMBAl9eDBw9WYmKiXn31VdWsWVP+/v665557lJWVdd7teHt7u7x2OBzKzc29qP7FecmsKB577DG1adNGX375pRYvXqxx48bptddeU79+/dSuXTvt3btXX331lRITE3X77bcrISFBr776qltrPluRzshkZ2fL19dXkrRkyRLdfffdkk4/5+XMiUsAgNLD4XAowMfLLcvlfMLwypUr1aNHD3Xp0kX169dXeHi4fvvtt8u2v3NxOp0KCwvTunXrrLacnBxt3LixyNusXbu2Tp06pTVr1lhthw8f1s6dO1WnTh2rLSoqSr1799bs2bP11FNP6Z133rHWVahQQfHx8froo480YcIEvf3220Wu53Ip0hmZunXraurUqerQoYMSExP13HPPSZIOHDjAl4sBAGwlOjpas2fPVseOHeVwODR8+PDznlm5XPr166dx48apZs2aqlWrliZNmqQ///yzUCFu69atKlOmjPXa4XCoYcOG6tSpkx5//HG99dZbKlOmjJ555hldc8016tSpkyRpwIABateuna699lr9+eefWrZsmWrXri1JGjFihBo3bqy6desqMzNTX3zxhbWuJClSkHnppZfUpUsXvfLKK4qPj7cmSc2bN8/l+h4AACXd66+/rkcffVTNmzdX+fLlNWTIELc8FX7IkCFKTk7Www8/LE9PT/Xq1Utt2rQp1LdCt2rVyuW1p6enTp06pWnTpql///666667lJWVpVatWumrr76yLnPl5OQoISFBv//+u4KDg9W2bVuNHz9e0uln4QwdOlS//fab/P391bJlS82cObP4D/wSOUwRL9Dl5OQoPT3d5fuRfvvtNwUEBKhixYrFVuClSk9Pl9PpVFpaGs+UAYBCysjI0J49e1StWrVifWgpCi83N1e1a9fWfffdZ135KG3O9zkr7O/vIp2ROXnypIwxVojZu3ev5syZo9q1a6tNG/dMBAMAwM727t2rxYsXq3Xr1srMzNTkyZO1Z88e/e1vf3N3aSVakSb7durUSR988IEkKTU1Vc2aNdNrr72mzp07a8qUKcVaIAAAVwMPDw9Nnz5dTZs2VYsWLbR161YtWbKkRM5LKUmKFGQ2btyoli1bSpI+++wzhYWFae/evfrggw80ceLEYi0QAICrQVRUlFauXKm0tDSlp6fr+++/zzf3BfkVKcicOHHCmh29ePFide3aVR4eHrrpppu0d+/eYi0QAACgIEUKMjVr1tTcuXO1b98+LVq0SHfeeack6dChQ0yoBQAAV0yRgsyIESM0ePBgVa1aVTfeeKP1JY6LFy9Wo0aNirVAAACAghTprqV77rlHN998sw4ePOjyRVu333679WVXAAAAl1uRv0o0PDxc4eHh+v333yVJlSpV4mF4AADgiirSpaXc3FyNGTNGTqdTVapUUZUqVRQSEqLnnnvOLY91BgAAV6ciBZlhw4Zp8uTJevHFF7Vp0yZt2rRJY8eO1aRJkzR8+PDirhEAgCvqlltu0YABA6zXVatW1YQJE877HofDoblz517yvotrO1eLIgWZ999/X++++6769OmjBg0aqEGDBnriiSf0zjvvaPr06cVcIgAAhdOxY0e1bdv2nOu+/fZbORwO/fDDDxe93XXr1qlXr16XWp6LUaNG6frrr8/XfvDgQbVr165Y93W26dOnKyQk5LLu40opUpA5cuSIatWqla+9Vq1aOnLkyCUXBQBAUfTs2VOJiYnW/M0zTZs2TU2aNFGDBg0uersVKlRQQEBAcZR4QeHh4fL19b0i+yoNihRkGjZsqMmTJ+drnzx5cpE+IAAAFIe77rpLFSpUyHd14NixY5o1a5Z69uypw4cPq3v37rrmmmsUEBCg+vXr65NPPjnvds++tLRr1y61atVKfn5+qlOnjhITE/O9Z8iQIbr22msVEBCg6tWra/jw4crOzpZ0+ozI6NGjtWXLFjkcDjkcDqvmsy8tbd26Vbfddpv8/f0VGhqqXr166dixY9b6Hj16qHPnznr11VcVERGh0NBQJSQkWPsqiqSkJHXq1ElBQUEKDg7Wfffdp5SUFGv9li1bdOutt6pMmTIKDg5W48aNtX79ekmnvzOqY8eOKlu2rAIDA1W3bl199dVXRa7lQop019LLL7+sDh06aMmSJdYzZFatWqV9+/Zd1mIBAG5kjJR9wj379g6QHI4LdvPy8tLDDz+s6dOna9iwYXL89Z5Zs2YpJydH3bt317Fjx9S4cWMNGTJEwcHB+vLLL/XQQw+pRo0ahbr7Njc3V127dlVYWJjWrFmjtLQ0l/k0ecqUKaPp06crMjJSW7du1eOPP64yZcro6aef1v33368ff/xRCxcu1JIlSyRJTqcz3zaOHz+uNm3aKCYmRuvWrdOhQ4f02GOPqW/fvi5hbdmyZYqIiNCyZcu0e/du3X///br++uv1+OOPX/B4znV8eSFmxYoVOnXqlBISEnT//fdr+fLlkqS4uDg1atRIU6ZMkaenpzZv3ixvb29JUkJCgrKysvTNN98oMDBQ27ZtU1BQ0EXXUVhFCjKtW7fWzz//rDfeeEM7duyQJHXt2lW9evXS888/b30PEwCgFMk+IY2NdM++/3lA8gksVNdHH31Ur7zyilasWKFbbrlF0unLSt26dZPT6ZTT6dTgwYOt/v369dOiRYv06aefFirILFmyRDt27NCiRYsUGXl6PMaOHZtvXsuzzz5r/Vy1alUNHjxYM2fO1NNPPy1/f38FBQXJy8tL4eHhBe5rxowZysjI0AcffKDAwNPHP3nyZHXs2FEvvfSSwsLCJElly5bV5MmT5enpqVq1aqlDhw5aunRpkYLM0qVLtXXrVu3Zs0dRUVGSpA8++EB169bVunXr1LRpUyUlJekf//iHNc0kOjraen9SUpK6deum+vXrS5KqV69+0TVcjCI/RyYyMlIvvPCCS9uWLVv03nvv6e23377kwgAAKIpatWqpefPm+ve//61bbrlFu3fv1rfffqsxY8ZIknJycjR27Fh9+umn2r9/v7KyspSZmVnoOTDbt29XVFSUFWIkWVcnzvSf//xHEydO1C+//KJjx47p1KlTF/01Ptu3b1fDhg2tECNJLVq0UG5urnbu3GkFmbp168rT09PqExERoa1bt17Uvs7cZ1RUlBViJKlOnToKCQnR9u3b1bRpUw0aNEiPPfaYPvzwQ8XGxuree+9VjRo1JElPPvmk+vTpo8WLFys2NlbdunW7rNNOihxkAABXGe+A02dG3LXvi9CzZ0/169dPb7zxhqZNm6YaNWqodevWkqRXXnlF//rXvzRhwgTVr19fgYGBGjBggLKysoqt3FWrVikuLk6jR49WmzZt5HQ6NXPmTL322mvFto8z5V3WyeNwOC7rc91GjRqlv/3tb/ryyy+1YMECjRw5UjNnzlSXLl302GOPqU2bNvryyy+1ePFijRs3Tq+99pr69et3WWop0mRfAMBVyOE4fXnHHUsh5sec6b777pOHh4dmzJihDz74QI8++qg1X2blypXq1KmTHnzwQTVs2FDVq1fXzz//XOht165dW/v27dPBgwetttWrV7v0+f7771WlShUNGzZMTZo0UXR0tPbu3evSx8fHRzk5ORfc15YtW3T8+HGrbeXKlfLw8NB1111X6JovRt7x7du3z2rbtm2bUlNTVadOHavt2muv1cCBA7V48WJ17dpV06ZNs9ZFRUWpd+/emj17tp566im98847l6VWiSADACiFgoKCdP/992vo0KE6ePCgevToYa2Ljo5WYmKivv/+e23fvl1///vfXe7IuZDY2Fhde+21io+P15YtW/Ttt99q2LBhLn2io6OVlJSkmTNn6pdfftHEiRM1Z84clz5Vq1bVnj17tHnzZv3xxx/KzMzMt6+4uDj5+fkpPj5eP/74o5YtW6Z+/frpoYcesi4rFVVOTo42b97ssmzfvl2xsbGqX7++4uLitHHjRq1du1YPP/ywWrdurSZNmujkyZPq27evli9frr1792rlypVat26dateuLUkaMGCAFi1apD179mjjxo1atmyZte5yuKhLS127dj3v+tTU1EupBQCAYtOzZ0+99957at++vct8lmeffVa//vqr2rRpo4CAAPXq1UudO3dWWlpaobbr4eGhOXPmqGfPnrrxxhtVtWpVTZw40eVBfHfffbcGDhyovn37KjMzUx06dNDw4cM1atQoq0+3bt00e/Zs3XrrrUpNTdW0adNcApckBQQEaNGiRerfv7+aNm2qgIAAdevWTa+//voljY10+pb0Ro0aubTVqFFDu3fv1ueff65+/fqpVatW8vDwUNu2bTVp0iRJkqenpw4fPqyHH35YKSkpKl++vLp27arRo0dLOh2QEhIS9Pvvvys4OFht27bV+PHjL7negjiMMaawnR955JFC9Tvz9JK7paeny+l0Ki0t7aInWQHA1SojI0N79uxRtWrV5Ofn5+5yUEqd73NW2N/fF3VGpiQFFAAAAObIAAAA2yLIAAAA2yLIAAAA2yLIAAAKdBH3gwAXrTg+XwQZAEA+eU+KPXHCTV8SiatC3ufr7CcTXwy+ogAAkI+np6dCQkJ06NAhSaefZ+K4yKfrAgUxxujEiRM6dOiQQkJCXL4n6mIRZAAA55T3rcx5YQYobiEhIef99u/CIMgAAM7J4XAoIiJCFStWVHZ2trvLQSnj7e19SWdi8hBkAADn5enpWSy/cIDLgcm+AADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAttwaZKZMmaIGDRooODhYwcHBiomJ0YIFC6z1GRkZSkhIUGhoqIKCgtStWzelpKS4sWIAAFCSuDXIVKpUSS+++KI2bNig9evX67bbblOnTp30008/SZIGDhyo+fPna9asWVqxYoUOHDigrl27urNkAABQgjiMMcbdRZypXLlyeuWVV3TPPfeoQoUKmjFjhu655x5J0o4dO1S7dm2tWrVKN910U6G2l56eLqfTqbS0NAUHB1/O0gEAQDEp7O/vEjNHJicnRzNnztTx48cVExOjDRs2KDs7W7GxsVafWrVqqXLlylq1alWB28nMzFR6errLAgAASie3B5mtW7cqKChIvr6+6t27t+bMmaM6deooOTlZPj4+CgkJcekfFham5OTkArc3btw4OZ1Oa4mKirrMRwAAANzF7UHmuuuu0+bNm7VmzRr16dNH8fHx2rZtW5G3N3ToUKWlpVnLvn37irFaAABQkni5uwAfHx/VrFlTktS4cWOtW7dO//rXv3T//fcrKytLqampLmdlUlJSFB4eXuD2fH195evre7nLBgAAJYDbz8icLTc3V5mZmWrcuLG8vb21dOlSa93OnTuVlJSkmJgYN1YIAABKCreekRk6dKjatWunypUr6+jRo5oxY4aWL1+uRYsWyel0qmfPnho0aJDKlSun4OBg9evXTzExMYW+YwkAAJRubg0yhw4d0sMPP6yDBw/K6XSqQYMGWrRoke644w5J0vjx4+Xh4aFu3bopMzNTbdq00ZtvvunOkgEAQAlS4p4jU9x4jgwAAPZju+fIAAAAXCyCDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC2CDAAAsC23Bplx48apadOmKlOmjCpWrKjOnTtr586dLn0yMjKUkJCg0NBQBQUFqVu3bkpJSXFTxQAAoCRxa5BZsWKFEhIStHr1aiUmJio7O1t33nmnjh8/bvUZOHCg5s+fr1mzZmnFihU6cOCAunbt6saqAQBASeEwxhh3F5Hnv//9rypWrKgVK1aoVatWSktLU4UKFTRjxgzdc889kqQdO3aodu3aWrVqlW666aYLbjM9PV1Op1NpaWkKDg6+3IcAAACKQWF/f5eoOTJpaWmSpHLlykmSNmzYoOzsbMXGxlp9atWqpcqVK2vVqlVuqREAAJQcXu4uIE9ubq4GDBigFi1aqF69epKk5ORk+fj4KCQkxKVvWFiYkpOTz7mdzMxMZWZmWq/T09MvW80AAMC9SswZmYSEBP3444+aOXPmJW1n3Lhxcjqd1hIVFVVMFQIAgJKmRASZvn376osvvtCyZctUqVIlqz08PFxZWVlKTU116Z+SkqLw8PBzbmvo0KFKS0uzln379l3O0gEAgBu5NcgYY9S3b1/NmTNHX3/9tapVq+ayvnHjxvL29tbSpUuttp07dyopKUkxMTHn3Kavr6+Cg4NdFgAAUDq5dY5MQkKCZsyYoc8//1xlypSx5r04nU75+/vL6XSqZ8+eGjRokMqVK6fg4GD169dPMTExhbpjCQAAlG5uvf3a4XCcs33atGnq0aOHpNMPxHvqqaf0ySefKDMzU23atNGbb75Z4KWls3H7NQAA9lPY398l6jkylwNBBgAA+7Hlc2QAAAAuBkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYFkEGAADYlluDzDfffKOOHTsqMjJSDodDc+fOdVlvjNGIESMUEREhf39/xcbGateuXe4pFgAAlDhuDTLHjx9Xw4YN9cYbb5xz/csvv6yJEydq6tSpWrNmjQIDA9WmTRtlZGRc4UoBAEBJ5OXOnbdr107t2rU75zpjjCZMmKBnn31WnTp1kiR98MEHCgsL09y5c/XAAw9cyVIBAEAJVGLnyOzZs0fJycmKjY212pxOp5o1a6ZVq1YV+L7MzEylp6e7LAAAoHQqsUEmOTlZkhQWFubSHhYWZq07l3HjxsnpdFpLVFTUZa0TAAC4T4kNMkU1dOhQpaWlWcu+ffvcXRIAALhMSmyQCQ8PlySlpKS4tKekpFjrzsXX11fBwcEuCwAAKJ1KbJCpVq2awsPDtXTpUqstPT1da9asUUxMjBsrAwAAJYVb71o6duyYdu/ebb3es2ePNm/erHLlyqly5coaMGCAnn/+eUVHR6tatWoaPny4IiMj1blzZ/cVDQAASgy3Bpn169fr1ltvtV4PGjRIkhQfH6/p06fr6aef1vHjx9WrVy+lpqbq5ptv1sKFC+Xn5+eukgEAQAniMMYYdxdxOaWnp8vpdCotLY35MgAA2ERhf3+X2DkyAAAAF0KQAQAAtkWQAQAAtkWQAQAAtkWQAQAAtuXW269t7WSqlJEmnco4vWRn/O/nUxnSqazT/RwOyeFxxp9/LTrrtUNnrDuDy01lpoD2s9cV1H6+9xSwnyLXUKpvhgMAnCmigVS2qlt2TZApqsXDpE0fubsKAADc764JUpNH3LJrgkxReQdKXv6St5/kdcaS99rT5/RZGJN7+uyEyT3Hz2csMqfX5eacfp/ljJ9d2s9eV1B7YbZViPaivgcAUPoFVXTbrnkgHgAAKHF4IB4AACj1CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2CDIAAMC2vNxdwOVmjJF0+uvAAQCAPeT93s77PV6QUh9kjh49KkmKiopycyUAAOBiHT16VE6ns8D1DnOhqGNzubm5OnDggMqUKSOHw1Fs201PT1dUVJT27dun4ODgYttuacDYFIyxKRhjUzDGpmCMzfnZeXyMMTp69KgiIyPl4VHwTJhSf0bGw8NDlSpVumzbDw4Ott2H40phbArG2BSMsSkYY1Mwxub87Do+5zsTk4fJvgAAwLYIMgAAwLYIMkXk6+urkSNHytfX192llDiMTcEYm4IxNgVjbArG2Jzf1TA+pX6yLwAAKL04IwMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIFNEb7zxhqpWrSo/Pz81a9ZMa9eudXdJV9y4cePUtGlTlSlTRhUrVlTnzp21c+dOlz4ZGRlKSEhQaGiogoKC1K1bN6WkpLipYvd48cUX5XA4NGDAAKvtah+X/fv368EHH1RoaKj8/f1Vv359rV+/3lpvjNGIESMUEREhf39/xcbGateuXW6s+MrIycnR8OHDVa1aNfn7+6tGjRp67rnnXL5r5moZm2+++UYdO3ZUZGSkHA6H5s6d67K+MONw5MgRxcXFKTg4WCEhIerZs6eOHTt2BY/i8jjf2GRnZ2vIkCGqX7++AgMDFRkZqYcfflgHDhxw2UZpGhuCTBH85z//0aBBgzRy5Eht3LhRDRs2VJs2bXTo0CF3l3ZFrVixQgkJCVq9erUSExOVnZ2tO++8U8ePH7f6DBw4UPPnz9esWbO0YsUKHThwQF27dnVj1VfWunXr9NZbb6lBgwYu7VfzuPz5559q0aKFvL29tWDBAm3btk2vvfaaypYta/V5+eWXNXHiRE2dOlVr1qxRYGCg2rRpo4yMDDdWfvm99NJLmjJliiZPnqzt27frpZde0ssvv6xJkyZZfa6WsTl+/LgaNmyoN95445zrCzMOcXFx+umnn5SYmKgvvvhC33zzjXr16nWlDuGyOd/YnDhxQhs3btTw4cO1ceNGzZ49Wzt37tTdd9/t0q9UjY3BRbvxxhtNQkKC9TonJ8dERkaacePGubEq9zt06JCRZFasWGGMMSY1NdV4e3ubWbNmWX22b99uJJlVq1a5q8wr5ujRoyY6OtokJiaa1q1bm/79+xtjGJchQ4aYm2++ucD1ubm5Jjw83LzyyitWW2pqqvH19TWffPLJlSjRbTp06GAeffRRl7auXbuauLg4Y8zVOzaSzJw5c6zXhRmHbdu2GUlm3bp1Vp8FCxYYh8Nh9u/ff8Vqv9zOHptzWbt2rZFk9u7da4wpfWPDGZmLlJWVpQ0bNig2NtZq8/DwUGxsrFatWuXGytwvLS1NklSuXDlJ0oYNG5Sdne0yVrVq1VLlypWvirFKSEhQhw4dXI5fYlzmzZunJk2a6N5771XFihXVqFEjvfPOO9b6PXv2KDk52WV8nE6nmjVrVurHp3nz5lq6dKl+/vlnSdKWLVv03XffqV27dpKu7rE5U2HGYdWqVQoJCVGTJk2sPrGxsfLw8NCaNWuueM3ulJaWJofDoZCQEEmlb2xK/ZdGFrc//vhDOTk5CgsLc2kPCwvTjh073FSV++Xm5mrAgAFq0aKF6tWrJ0lKTk6Wj4+P9S9PnrCwMCUnJ7uhyitn5syZ2rhxo9atW5dv3dU8LpL066+/asqUKRo0aJD++c9/at26dXryySfl4+Oj+Ph4awzO9e9YaR+fZ555Runp6apVq5Y8PT2Vk5OjF154QXFxcZJ0VY/NmQozDsnJyapYsaLLei8vL5UrV+6qGquMjAwNGTJE3bt3t740srSNDUEGxSIhIUE//vijvvvuO3eX4nb79u1T//79lZiYKD8/P3eXU+Lk5uaqSZMmGjt2rCSpUaNG+vHHHzV16lTFx8e7uTr3+vTTT/Xxxx9rxowZqlu3rjZv3qwBAwYoMjLyqh8bXLzs7Gzdd999MsZoypQp7i7nsuHS0kUqX768PD09891hkpKSovDwcDdV5V59+/bVF198oWXLlqlSpUpWe3h4uLKyspSamurSv7SP1YYNG3To0CHdcMMN8vLykpeXl1asWKGJEyfKy8tLYWFhV+W45ImIiFCdOnVc2mrXrq2kpCRJssbgavx37B//+IeeeeYZPfDAA6pfv74eeughDRw4UOPGjZN0dY/NmQozDuHh4fluwDh16pSOHDlyVYxVXojZu3evEhMTrbMxUukbG4LMRfLx8VHjxo21dOlSqy03N1dLly5VTEyMGyu78owx6tu3r+bMmaOvv/5a1apVc1nfuHFjeXt7u4zVzp07lZSUVKrH6vbbb9fWrVu1efNma2nSpIni4uKsn6/GccnTokWLfLfp//zzz6pSpYokqVq1agoPD3cZn/T0dK1Zs6bUj8+JEyfk4eH6n2VPT0/l5uZKurrH5kyFGYeYmBilpqZqw4YNVp+vv/5aubm5atas2RWv+UrKCzG7du3SkiVLFBoa6rK+1I2Nu2cb29HMmTONr6+vmT59utm2bZvp1auXCQkJMcnJye4u7Yrq06ePcTqdZvny5ebgwYPWcuLECatP7969TeXKlc3XX39t1q9fb2JiYkxMTIwbq3aPM+9aMubqHpe1a9caLy8v88ILL5hdu3aZjz/+2AQEBJiPPvrI6vPiiy+akJAQ8/nnn5sffvjBdOrUyVSrVs2cPHnSjZVffvHx8eaaa64xX3zxhdmzZ4+ZPXu2KV++vHn66aetPlfL2Bw9etRs2rTJbNq0yUgyr7/+utm0aZN1501hxqFt27amUaNGZs2aNea7774z0dHRpnv37u46pGJzvrHJysoyd999t6lUqZLZvHmzy3+bMzMzrW2UprEhyBTRpEmTTOXKlY2Pj4+58cYbzerVq91d0hUn6ZzLtGnTrD4nT540TzzxhClbtqwJCAgwXbp0MQcPHnRf0W5ydpC52sdl/vz5pl69esbX19fUqlXLvP322y7rc3NzzfDhw01YWJjx9fU1t99+u9m5c6ebqr1y0tPTTf/+/U3lypWNn5+fqV69uhk2bJjLL6CrZWyWLVt2zv++xMfHG2MKNw6HDx823bt3N0FBQSY4ONg88sgj5ujRo244muJ1vrHZs2dPgf9tXrZsmbWN0jQ2DmPOeGQkAACAjTBHBgAA2BZBBgAA2BZBBgAA2BZBBgAA2BZBBgAA2BZBBgAA2BZBBgAA2BZBBsBVx+FwaO7cue4uA0AxIMgAuKJ69Oghh8ORb2nbtq27SwNgQ17uLgDA1adt27aaNm2aS5uvr6+bqgFgZ5yRAXDF+fr6Kjw83GUpW7aspNOXfaZMmaJ27drJ399f1atX12effeby/q1bt+q2226Tv7+/QkND1atXLx07dsylz7///W/VrVtXvr6+ioiIUN++fV3W//HHH+rSpYsCAgIUHR2tefPmXd6DBnBZEGQAlDjDhw9Xt27dtGXLFsXFxemBBx7Q9u3bJUnHjx9XmzZtVLZsWa1bt06zZs3SkiVLXILKlClTlJCQoF69emnr1q2aN2+eatas6bKP0aNH67777tMPP/yg9u3bKy4uTkeOHLmixwmgGLj7WysBXF3i4+ONp6enCQwMdFleeOEFY8zpb1Xv3bu3y3uaNWtm+vTpY4wx5u233zZly5Y1x44ds9Z/+eWXxsPDwyQnJxtjjImMjDTDhg0rsAZJ5tlnn7VeHzt2zEgyCxYsKLbjBHBlMEcGwBV36623asqUKS5t5cqVs36OiYlxWRcTE6PNmzdLkrZv366GDRsqMDDQWt+iRQvl5uZq586dcjgcOnDggG6//fbz1tCgQQPr58DAQAUHB+vQoUNFPSQAbkKQAXDFBQYG5rvUU1z8/f0L1c/b29vltcPhUG5u7uUoCcBlxBwZACXO6tWr872uXbu2JKl27drasmWLjh8/bq1fuXKlPDw8dN1116lMmTKqWrWqli5dekVrBuAenJEBcMVlZmYqOTnZpc3Ly0vly5eXJM2aNUtNmjTRzTffrI8//lhr167Ve++9J0mKi4vTyJEjFR8fr1GjRum///2v+vXrp4ceekhhYWGSpFGjRql3796qWLGi2rVrp6NHj2rlypXq16/flT1QAJcdQQbAFbdw4UJFRES4tF133XXasWOHpNN3FM2cOVNPPPGEIiIi9Mknn6hOnTqSpICAAC1atEj9+/dX06ZNFRAQoG7duun111+3thUfH6+MjAyNHz9egwcPVvny5XXPPfdcuQMEcMU4jDHG3UUAQB6Hw6E5c+aoc+fO7i4FgA0wRwYAANgWQQYAANgWc2QAlChc7QZwMTgjAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbIsgAwAAbOv/AVn7XVVtHwsTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_1l_emci.pth\n",
      "Average Time per Epoch: 0.04s\n",
      "Average CPU Usage: 13.53%\n",
      "Average Memory Usage: 3.21GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 4.70%\n",
      "\n",
      "Total Training Time: 5.60s\n",
      "Max CPU Usage: 44.30%\n",
      "Max Memory Usage: 3.21GB\n",
      "Max GPU Usage: 0.02GB\n",
      "Max GPU Utilization: 6.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn1_emci = GCN1Layer(emci_num_features, 2*emci_num_features, emci_num_classes)\n",
    "print(gcn1_emci)\n",
    "print(f\"Total number of trainable parameters: {(gcn1_emci.count_parameters())*2}\\n\")\n",
    "single_train(gcn1_emci, emci_train_loader, emci_val_loader, \n",
    "            lr=0.01, num_epochs=500, step_size=20, patience=20, save_path='models/gcn_1l_emci.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4074\n",
      "Average Sensitivity (Recall): 1.0000\n",
      "Average Specificity: 0.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0006s\n",
      "Average CPU Usage: 30.63%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.13GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "gcn1_emci = GCN1Layer(emci_num_features, 2*emci_num_features, emci_num_classes)\n",
    "gcn1_emci.load_state_dict(torch.load('models/gcn_1l_emci.pth'))\n",
    "single_test(gcn1_emci.to(device), emci_test_loader)\n",
    "inference_performance(gcn1_emci.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN1Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 390\n",
      "\n",
      "Epoch 1, Train Loss: 260.04783940315247, Val Loss: 29.889315247535706\n",
      "Time: 0.13s, CPU: 38.50%, Memory: 3.33GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 2, Train Loss: 256.00520634651184, Val Loss: 29.789150834083557\n",
      "Time: 0.13s, CPU: 54.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 3, Train Loss: 255.32504272460938, Val Loss: 29.69202482700348\n",
      "Time: 0.17s, CPU: 55.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 4, Train Loss: 253.5604054927826, Val Loss: 29.77241063117981\n",
      "Time: 0.12s, CPU: 51.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 12.00%\n",
      "Epoch 5, Train Loss: 251.8092291355133, Val Loss: 29.8782559633255\n",
      "Time: 0.17s, CPU: 50.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 6, Train Loss: 250.1774344444275, Val Loss: 29.931592226028442\n",
      "Time: 0.13s, CPU: 46.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 7, Train Loss: 248.63074386119843, Val Loss: 29.958812355995178\n",
      "Time: 0.13s, CPU: 49.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 8, Train Loss: 246.93961703777313, Val Loss: 30.033871293067932\n",
      "Time: 0.15s, CPU: 55.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 9, Train Loss: 245.14063942432404, Val Loss: 30.15850067138672\n",
      "Time: 0.12s, CPU: 36.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 10, Train Loss: 243.36584293842316, Val Loss: 30.2810218334198\n",
      "Time: 0.12s, CPU: 45.70%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 11, Train Loss: 241.64310562610626, Val Loss: 30.400388717651367\n",
      "Time: 0.16s, CPU: 47.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 12, Train Loss: 239.95664620399475, Val Loss: 30.54614531993866\n",
      "Time: 0.14s, CPU: 51.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 13, Train Loss: 238.3549404144287, Val Loss: 30.70602858066559\n",
      "Time: 0.11s, CPU: 49.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 14, Train Loss: 236.84698569774628, Val Loss: 30.861856341362\n",
      "Time: 0.13s, CPU: 48.10%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 15, Train Loss: 235.44159722328186, Val Loss: 31.019299864768982\n",
      "Time: 0.11s, CPU: 52.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 16, Train Loss: 234.1414247751236, Val Loss: 31.166131496429443\n",
      "Time: 0.12s, CPU: 51.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 17, Train Loss: 232.94383871555328, Val Loss: 31.303107619285583\n",
      "Time: 0.12s, CPU: 55.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 18, Train Loss: 231.8284091949463, Val Loss: 31.431731343269348\n",
      "Time: 0.14s, CPU: 47.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 19, Train Loss: 230.78260469436646, Val Loss: 31.551478028297424\n",
      "Time: 0.13s, CPU: 56.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 20, Train Loss: 229.77600932121277, Val Loss: 31.65333867073059\n",
      "Time: 0.14s, CPU: 50.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 21, Train Loss: 228.81181943416595, Val Loss: 31.7436443567276\n",
      "Time: 0.13s, CPU: 48.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 22, Train Loss: 227.88539910316467, Val Loss: 31.830770015716553\n",
      "Time: 0.14s, CPU: 41.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 23, Train Loss: 227.00147533416748, Val Loss: 31.914525747299194\n",
      "Time: 0.12s, CPU: 39.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 24, Train Loss: 226.13738226890564, Val Loss: 31.991541624069214\n",
      "Time: 0.14s, CPU: 36.45%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 25, Train Loss: 225.2991372346878, Val Loss: 32.06133162975311\n",
      "Time: 0.12s, CPU: 41.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 26, Train Loss: 224.47901165485382, Val Loss: 32.12074148654938\n",
      "Time: 0.13s, CPU: 49.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 27, Train Loss: 223.66262924671173, Val Loss: 32.16928839683533\n",
      "Time: 0.14s, CPU: 44.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 28, Train Loss: 222.8625500202179, Val Loss: 32.21060621738434\n",
      "Time: 0.22s, CPU: 61.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 29, Train Loss: 222.0922521352768, Val Loss: 32.24633324146271\n",
      "Time: 0.30s, CPU: 72.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 7.00%\n",
      "Epoch 30, Train Loss: 221.34271001815796, Val Loss: 32.2734375\n",
      "Time: 0.41s, CPU: 71.55%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 6.50%\n",
      "Epoch 31, Train Loss: 220.62083649635315, Val Loss: 32.30013620853424\n",
      "Time: 0.36s, CPU: 62.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 7.00%\n",
      "Epoch 32, Train Loss: 219.9246209859848, Val Loss: 32.32321071624756\n",
      "Time: 0.11s, CPU: 53.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 33, Train Loss: 219.24647688865662, Val Loss: 32.340253472328186\n",
      "Time: 0.14s, CPU: 53.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 34, Train Loss: 218.58982586860657, Val Loss: 32.35989689826965\n",
      "Time: 0.11s, CPU: 50.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 35, Train Loss: 217.95175647735596, Val Loss: 32.37307405471802\n",
      "Time: 0.13s, CPU: 49.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 36, Train Loss: 217.328475356102, Val Loss: 32.38032245635986\n",
      "Time: 0.14s, CPU: 47.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 37, Train Loss: 216.718128323555, Val Loss: 32.37912833690643\n",
      "Time: 0.11s, CPU: 53.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 38, Train Loss: 216.12069737911224, Val Loss: 32.376009464263916\n",
      "Time: 0.15s, CPU: 52.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 39, Train Loss: 215.53910183906555, Val Loss: 32.37270712852478\n",
      "Time: 0.12s, CPU: 48.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 40, Train Loss: 214.96150135993958, Val Loss: 32.3632732629776\n",
      "Time: 0.13s, CPU: 66.55%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 41, Train Loss: 214.38907730579376, Val Loss: 32.353047609329224\n",
      "Time: 0.12s, CPU: 50.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 42, Train Loss: 213.8340458869934, Val Loss: 32.340903639793396\n",
      "Time: 0.12s, CPU: 43.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 43, Train Loss: 213.28738713264465, Val Loss: 32.32974135875702\n",
      "Time: 0.13s, CPU: 51.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 44, Train Loss: 212.7514398097992, Val Loss: 32.32256376743317\n",
      "Time: 0.13s, CPU: 50.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 45, Train Loss: 212.21631979942322, Val Loss: 32.31595265865326\n",
      "Time: 0.13s, CPU: 54.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 46, Train Loss: 211.69280409812927, Val Loss: 32.308279395103455\n",
      "Time: 0.13s, CPU: 51.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 47, Train Loss: 211.18432486057281, Val Loss: 32.30077350139618\n",
      "Time: 0.13s, CPU: 48.80%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 48, Train Loss: 210.6955507993698, Val Loss: 32.29564619064331\n",
      "Time: 0.12s, CPU: 40.35%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 49, Train Loss: 210.2168629169464, Val Loss: 32.28790211677551\n",
      "Time: 0.14s, CPU: 51.70%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 50, Train Loss: 209.7352752685547, Val Loss: 32.276286005973816\n",
      "Time: 0.13s, CPU: 42.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 51, Train Loss: 209.26588594913483, Val Loss: 32.26316034793854\n",
      "Time: 0.12s, CPU: 44.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 52, Train Loss: 208.81105709075928, Val Loss: 32.252596735954285\n",
      "Time: 0.12s, CPU: 48.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 53, Train Loss: 208.3683704137802, Val Loss: 32.24913668632507\n",
      "Time: 0.15s, CPU: 50.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 54, Train Loss: 207.93522000312805, Val Loss: 32.24603712558746\n",
      "Time: 0.12s, CPU: 45.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 55, Train Loss: 207.50379300117493, Val Loss: 32.241833567619324\n",
      "Time: 0.11s, CPU: 51.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 56, Train Loss: 207.0872766971588, Val Loss: 32.243343114852905\n",
      "Time: 0.13s, CPU: 50.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 57, Train Loss: 206.68638837337494, Val Loss: 32.24718940258026\n",
      "Time: 0.11s, CPU: 51.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 58, Train Loss: 206.28800988197327, Val Loss: 32.24902081489563\n",
      "Time: 0.13s, CPU: 44.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 59, Train Loss: 205.9142383337021, Val Loss: 32.25432515144348\n",
      "Time: 0.12s, CPU: 61.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 60, Train Loss: 205.5569453239441, Val Loss: 32.26313138008118\n",
      "Time: 0.12s, CPU: 51.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 61, Train Loss: 205.18989396095276, Val Loss: 32.26728022098541\n",
      "Time: 0.14s, CPU: 54.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 62, Train Loss: 204.84999239444733, Val Loss: 32.27650165557861\n",
      "Time: 0.16s, CPU: 62.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 63, Train Loss: 204.50451004505157, Val Loss: 32.28636682033539\n",
      "Time: 0.12s, CPU: 52.55%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 64, Train Loss: 204.17541444301605, Val Loss: 32.30172622203827\n",
      "Time: 0.12s, CPU: 42.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 65, Train Loss: 203.84245145320892, Val Loss: 32.315128684043884\n",
      "Time: 0.20s, CPU: 55.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 66, Train Loss: 203.52115988731384, Val Loss: 32.332364559173584\n",
      "Time: 0.36s, CPU: 62.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 67, Train Loss: 203.1861171722412, Val Loss: 32.34964871406555\n",
      "Time: 0.18s, CPU: 62.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 68, Train Loss: 202.86792016029358, Val Loss: 32.369601130485535\n",
      "Time: 0.12s, CPU: 47.65%, Memory: 3.32GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 69, Train Loss: 202.5458244085312, Val Loss: 32.38397562503815\n",
      "Time: 0.12s, CPU: 46.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 70, Train Loss: 202.24046349525452, Val Loss: 32.401697516441345\n",
      "Time: 0.13s, CPU: 49.50%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 71, Train Loss: 201.93502926826477, Val Loss: 32.41843128204346\n",
      "Time: 0.13s, CPU: 40.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 72, Train Loss: 201.63457262516022, Val Loss: 32.43998980522156\n",
      "Time: 0.77s, CPU: 73.40%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 9.50%\n",
      "Epoch 73, Train Loss: 201.34630870819092, Val Loss: 32.4601514339447\n",
      "Time: 1.28s, CPU: 84.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 74, Train Loss: 201.06006228923798, Val Loss: 32.48145246505737\n",
      "Time: 1.38s, CPU: 88.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 75, Train Loss: 200.78433072566986, Val Loss: 32.497838616371155\n",
      "Time: 1.65s, CPU: 84.00%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 76, Train Loss: 200.50890481472015, Val Loss: 32.51063919067383\n",
      "Time: 1.21s, CPU: 86.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 77, Train Loss: 200.2288407087326, Val Loss: 32.52557694911957\n",
      "Time: 1.27s, CPU: 87.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 78, Train Loss: 199.95353507995605, Val Loss: 32.541560769081116\n",
      "Time: 0.62s, CPU: 84.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 4.00%\n",
      "Epoch 79, Train Loss: 199.69417822360992, Val Loss: 32.56364071369171\n",
      "Time: 1.15s, CPU: 60.35%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 4.00%\n",
      "Epoch 80, Train Loss: 199.42211890220642, Val Loss: 32.5838280916214\n",
      "Time: 1.75s, CPU: 85.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 81, Train Loss: 199.17609357833862, Val Loss: 32.60455620288849\n",
      "Time: 1.41s, CPU: 86.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 82, Train Loss: 198.92313027381897, Val Loss: 32.62732815742493\n",
      "Time: 0.96s, CPU: 84.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 83, Train Loss: 198.67202985286713, Val Loss: 32.644309759140015\n",
      "Time: 0.85s, CPU: 69.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 84, Train Loss: 198.42679381370544, Val Loss: 32.65592586994171\n",
      "Time: 1.50s, CPU: 86.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 85, Train Loss: 198.1779705286026, Val Loss: 32.66127526760101\n",
      "Time: 1.50s, CPU: 82.90%, Memory: 3.32GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 86, Train Loss: 197.93832731246948, Val Loss: 32.6714848279953\n",
      "Time: 1.01s, CPU: 85.55%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 87, Train Loss: 197.69272649288177, Val Loss: 32.68270182609558\n",
      "Time: 0.87s, CPU: 57.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 88, Train Loss: 197.44733798503876, Val Loss: 32.69074201583862\n",
      "Time: 1.13s, CPU: 91.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 89, Train Loss: 197.21174275875092, Val Loss: 32.705103635787964\n",
      "Time: 0.12s, CPU: 48.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 5.50%\n",
      "Epoch 90, Train Loss: 196.9648416042328, Val Loss: 32.72147047519684\n",
      "Time: 0.12s, CPU: 46.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 91, Train Loss: 196.7368004322052, Val Loss: 32.74116539955139\n",
      "Time: 0.11s, CPU: 43.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 92, Train Loss: 196.4977114200592, Val Loss: 32.75528562068939\n",
      "Time: 0.34s, CPU: 59.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 11.00%\n",
      "Epoch 93, Train Loss: 196.28289592266083, Val Loss: 32.769615054130554\n",
      "Time: 0.28s, CPU: 78.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 6.00%\n",
      "Epoch 94, Train Loss: 196.04416692256927, Val Loss: 32.779650807380676\n",
      "Time: 0.12s, CPU: 64.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 7.00%\n",
      "Epoch 95, Train Loss: 195.8413931131363, Val Loss: 32.79288911819458\n",
      "Time: 0.12s, CPU: 42.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 11.50%\n",
      "Epoch 96, Train Loss: 195.6139885187149, Val Loss: 32.79991865158081\n",
      "Time: 0.13s, CPU: 53.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 97, Train Loss: 195.41575229167938, Val Loss: 32.8114800453186\n",
      "Time: 0.15s, CPU: 43.10%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 98, Train Loss: 195.1935693025589, Val Loss: 32.82378816604614\n",
      "Time: 0.14s, CPU: 51.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 99, Train Loss: 194.99140918254852, Val Loss: 32.8408180475235\n",
      "Time: 0.13s, CPU: 47.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 100, Train Loss: 194.77505505084991, Val Loss: 32.85478055477142\n",
      "Time: 0.12s, CPU: 52.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 101, Train Loss: 194.58586782217026, Val Loss: 32.871710658073425\n",
      "Time: 0.13s, CPU: 46.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 102, Train Loss: 194.36626762151718, Val Loss: 32.88984775543213\n",
      "Time: 0.12s, CPU: 58.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 103, Train Loss: 194.16666787862778, Val Loss: 32.91281604766846\n",
      "Time: 0.17s, CPU: 66.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.50%\n",
      "Epoch 104, Train Loss: 193.95029366016388, Val Loss: 32.931326508522034\n",
      "Time: 0.12s, CPU: 50.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 105, Train Loss: 193.7471684217453, Val Loss: 32.95717227458954\n",
      "Time: 0.11s, CPU: 51.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 106, Train Loss: 193.5399889945984, Val Loss: 32.98048496246338\n",
      "Time: 0.11s, CPU: 61.00%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 107, Train Loss: 193.33327955007553, Val Loss: 32.999903082847595\n",
      "Time: 0.12s, CPU: 42.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 108, Train Loss: 193.1303125023842, Val Loss: 33.016051054000854\n",
      "Time: 0.12s, CPU: 52.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 109, Train Loss: 192.92963933944702, Val Loss: 33.03273332118988\n",
      "Time: 0.13s, CPU: 45.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 110, Train Loss: 192.72045809030533, Val Loss: 33.05174267292023\n",
      "Time: 0.13s, CPU: 45.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 111, Train Loss: 192.51635020971298, Val Loss: 33.06716966629028\n",
      "Time: 0.13s, CPU: 47.05%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 112, Train Loss: 192.3277930021286, Val Loss: 33.08985471725464\n",
      "Time: 0.14s, CPU: 53.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 113, Train Loss: 192.1243457198143, Val Loss: 33.10584497451782\n",
      "Time: 0.22s, CPU: 64.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 114, Train Loss: 191.94172900915146, Val Loss: 33.128462433815\n",
      "Time: 0.14s, CPU: 37.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 115, Train Loss: 191.73822164535522, Val Loss: 33.14799964427948\n",
      "Time: 0.12s, CPU: 64.05%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 116, Train Loss: 191.5648342370987, Val Loss: 33.16469156742096\n",
      "Time: 0.12s, CPU: 50.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 117, Train Loss: 191.37956655025482, Val Loss: 33.18302500247955\n",
      "Time: 0.12s, CPU: 52.55%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 118, Train Loss: 191.1920247077942, Val Loss: 33.195117473602295\n",
      "Time: 0.13s, CPU: 50.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 119, Train Loss: 191.01107251644135, Val Loss: 33.20522081851959\n",
      "Time: 0.12s, CPU: 62.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 120, Train Loss: 190.81677573919296, Val Loss: 33.21078264713287\n",
      "Time: 0.12s, CPU: 42.85%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 121, Train Loss: 190.63791418075562, Val Loss: 33.222859025001526\n",
      "Time: 0.11s, CPU: 49.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 122, Train Loss: 190.45246970653534, Val Loss: 33.22956347465515\n",
      "Time: 0.10s, CPU: 51.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 123, Train Loss: 190.26193195581436, Val Loss: 33.237326860427856\n",
      "Time: 0.13s, CPU: 47.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 124, Train Loss: 190.07630014419556, Val Loss: 33.24405384063721\n",
      "Time: 0.12s, CPU: 61.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 125, Train Loss: 189.88252311944962, Val Loss: 33.25022077560425\n",
      "Time: 0.12s, CPU: 45.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 126, Train Loss: 189.69327729940414, Val Loss: 33.260159969329834\n",
      "Time: 0.21s, CPU: 63.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 127, Train Loss: 189.49155658483505, Val Loss: 33.27412569522858\n",
      "Time: 0.13s, CPU: 52.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 128, Train Loss: 189.310633122921, Val Loss: 33.294219732284546\n",
      "Time: 0.29s, CPU: 76.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 6.00%\n",
      "Epoch 129, Train Loss: 189.1202336549759, Val Loss: 33.313483357429504\n",
      "Time: 0.11s, CPU: 47.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 6.00%\n",
      "Epoch 130, Train Loss: 188.9484499692917, Val Loss: 33.3405876159668\n",
      "Time: 0.14s, CPU: 53.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 131, Train Loss: 188.76655197143555, Val Loss: 33.36439919471741\n",
      "Time: 0.13s, CPU: 44.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 132, Train Loss: 188.60307729244232, Val Loss: 33.39496994018555\n",
      "Time: 0.12s, CPU: 38.85%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 133, Train Loss: 188.41605389118195, Val Loss: 33.41484832763672\n",
      "Time: 0.13s, CPU: 55.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 134, Train Loss: 188.25948530435562, Val Loss: 33.43940019607544\n",
      "Time: 0.13s, CPU: 52.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 135, Train Loss: 188.08189916610718, Val Loss: 33.463160276412964\n",
      "Time: 0.11s, CPU: 50.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 136, Train Loss: 187.92033916711807, Val Loss: 33.488211035728455\n",
      "Time: 0.11s, CPU: 51.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 137, Train Loss: 187.75015407800674, Val Loss: 33.50330650806427\n",
      "Time: 0.13s, CPU: 56.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 138, Train Loss: 187.59980952739716, Val Loss: 33.524977684020996\n",
      "Time: 0.15s, CPU: 47.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 139, Train Loss: 187.42209821939468, Val Loss: 33.55147039890289\n",
      "Time: 1.02s, CPU: 85.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 8.00%\n",
      "Epoch 140, Train Loss: 187.27641785144806, Val Loss: 33.575809836387634\n",
      "Time: 0.42s, CPU: 73.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 6.50%\n",
      "Epoch 141, Train Loss: 187.10791397094727, Val Loss: 33.59411108493805\n",
      "Time: 1.00s, CPU: 81.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 5.50%\n",
      "Epoch 142, Train Loss: 186.96452009677887, Val Loss: 33.61136305332184\n",
      "Time: 1.53s, CPU: 82.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 143, Train Loss: 186.80875545740128, Val Loss: 33.6280871629715\n",
      "Time: 1.40s, CPU: 73.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 144, Train Loss: 186.67063772678375, Val Loss: 33.64781105518341\n",
      "Time: 1.04s, CPU: 88.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 4.00%\n",
      "Epoch 145, Train Loss: 186.509848177433, Val Loss: 33.66754138469696\n",
      "Time: 1.41s, CPU: 89.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 146, Train Loss: 186.37537586688995, Val Loss: 33.68332242965698\n",
      "Time: 1.45s, CPU: 82.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 147, Train Loss: 186.21662521362305, Val Loss: 33.703033447265625\n",
      "Time: 1.71s, CPU: 85.05%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 148, Train Loss: 186.07884061336517, Val Loss: 33.71481692790985\n",
      "Time: 0.97s, CPU: 85.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 149, Train Loss: 185.92166996002197, Val Loss: 33.72712826728821\n",
      "Time: 1.48s, CPU: 87.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 150, Train Loss: 185.77752500772476, Val Loss: 33.7363303899765\n",
      "Time: 1.14s, CPU: 80.05%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 151, Train Loss: 185.62790018320084, Val Loss: 33.74579644203186\n",
      "Time: 0.91s, CPU: 85.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 152, Train Loss: 185.4828673005104, Val Loss: 33.755661606788635\n",
      "Time: 1.72s, CPU: 84.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 153, Train Loss: 185.33026641607285, Val Loss: 33.76686251163483\n",
      "Time: 1.24s, CPU: 79.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 154, Train Loss: 185.17897951602936, Val Loss: 33.779022574424744\n",
      "Time: 1.50s, CPU: 89.45%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 155, Train Loss: 185.02494180202484, Val Loss: 33.7890100479126\n",
      "Time: 0.77s, CPU: 87.55%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 156, Train Loss: 184.87512171268463, Val Loss: 33.79750406742096\n",
      "Time: 1.56s, CPU: 87.00%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 157, Train Loss: 184.73232746124268, Val Loss: 33.80034935474396\n",
      "Time: 1.33s, CPU: 77.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 158, Train Loss: 184.58303880691528, Val Loss: 33.80867278575897\n",
      "Time: 0.67s, CPU: 86.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 159, Train Loss: 184.42304229736328, Val Loss: 33.81608211994171\n",
      "Time: 0.15s, CPU: 50.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 9.50%\n",
      "Epoch 160, Train Loss: 184.27474164962769, Val Loss: 33.83267104625702\n",
      "Time: 0.12s, CPU: 55.55%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 161, Train Loss: 184.1109561920166, Val Loss: 33.848130226135254\n",
      "Time: 0.11s, CPU: 41.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 162, Train Loss: 183.96043479442596, Val Loss: 33.86743891239166\n",
      "Time: 0.13s, CPU: 47.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 21.00%\n",
      "Epoch 163, Train Loss: 183.79950749874115, Val Loss: 33.891128182411194\n",
      "Time: 0.13s, CPU: 41.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 164, Train Loss: 183.65854442119598, Val Loss: 33.91209125518799\n",
      "Time: 0.11s, CPU: 42.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 165, Train Loss: 183.49243354797363, Val Loss: 33.93053090572357\n",
      "Time: 0.13s, CPU: 45.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 166, Train Loss: 183.33242046833038, Val Loss: 33.95263338088989\n",
      "Time: 0.13s, CPU: 52.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 167, Train Loss: 183.17125123739243, Val Loss: 33.975521206855774\n",
      "Time: 0.11s, CPU: 45.05%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 168, Train Loss: 183.01541149616241, Val Loss: 33.99033987522125\n",
      "Time: 0.12s, CPU: 53.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 169, Train Loss: 182.8683786392212, Val Loss: 33.99966752529144\n",
      "Time: 0.11s, CPU: 40.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 21.00%\n",
      "Epoch 170, Train Loss: 182.71997392177582, Val Loss: 34.017247796058655\n",
      "Time: 0.12s, CPU: 51.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 171, Train Loss: 182.56090253591537, Val Loss: 34.02967178821564\n",
      "Time: 0.11s, CPU: 40.30%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 172, Train Loss: 182.4196616411209, Val Loss: 34.04339933395386\n",
      "Time: 0.11s, CPU: 44.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 173, Train Loss: 182.26262629032135, Val Loss: 34.05525362491608\n",
      "Time: 0.12s, CPU: 45.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 174, Train Loss: 182.11558616161346, Val Loss: 34.06622922420502\n",
      "Time: 0.14s, CPU: 49.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 175, Train Loss: 181.9653281569481, Val Loss: 34.077191948890686\n",
      "Time: 0.12s, CPU: 50.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 176, Train Loss: 181.81689548492432, Val Loss: 34.092963337898254\n",
      "Time: 0.12s, CPU: 53.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 177, Train Loss: 181.6743711233139, Val Loss: 34.11322474479675\n",
      "Time: 0.11s, CPU: 44.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 178, Train Loss: 181.53119939565659, Val Loss: 34.13493454456329\n",
      "Time: 0.12s, CPU: 47.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 179, Train Loss: 181.37010568380356, Val Loss: 34.15715932846069\n",
      "Time: 0.15s, CPU: 56.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 180, Train Loss: 181.22621005773544, Val Loss: 34.17939376831055\n",
      "Time: 0.13s, CPU: 49.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 181, Train Loss: 181.08094364404678, Val Loss: 34.207003355026245\n",
      "Time: 0.19s, CPU: 72.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 12.00%\n",
      "Epoch 182, Train Loss: 180.9222046136856, Val Loss: 34.22948884963989\n",
      "Time: 0.12s, CPU: 59.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 183, Train Loss: 180.78075873851776, Val Loss: 34.25211274623871\n",
      "Time: 0.22s, CPU: 68.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 11.50%\n",
      "Epoch 184, Train Loss: 180.63660329580307, Val Loss: 34.26706337928772\n",
      "Time: 0.21s, CPU: 57.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 185, Train Loss: 180.46920305490494, Val Loss: 34.27546405792236\n",
      "Time: 0.17s, CPU: 53.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 186, Train Loss: 180.31387335062027, Val Loss: 34.278833985328674\n",
      "Time: 0.11s, CPU: 45.65%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 187, Train Loss: 180.16708761453629, Val Loss: 34.28770136833191\n",
      "Time: 0.12s, CPU: 48.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 188, Train Loss: 180.00315296649933, Val Loss: 34.29273855686188\n",
      "Time: 0.11s, CPU: 46.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 189, Train Loss: 179.86233180761337, Val Loss: 34.30443835258484\n",
      "Time: 0.14s, CPU: 48.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 190, Train Loss: 179.71388572454453, Val Loss: 34.31075656414032\n",
      "Time: 0.13s, CPU: 51.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 191, Train Loss: 179.56854635477066, Val Loss: 34.31685268878937\n",
      "Time: 0.13s, CPU: 63.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 192, Train Loss: 179.397563457489, Val Loss: 34.31977200508118\n",
      "Time: 0.14s, CPU: 53.55%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 193, Train Loss: 179.25531667470932, Val Loss: 34.328150153160095\n",
      "Time: 0.13s, CPU: 53.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 194, Train Loss: 179.0853971838951, Val Loss: 34.32796347141266\n",
      "Time: 0.13s, CPU: 54.80%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 195, Train Loss: 178.9325612783432, Val Loss: 34.334169030189514\n",
      "Time: 0.12s, CPU: 57.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 196, Train Loss: 178.77437818050385, Val Loss: 34.34538280963898\n",
      "Time: 0.12s, CPU: 54.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 197, Train Loss: 178.61103481054306, Val Loss: 34.35139203071594\n",
      "Time: 0.12s, CPU: 50.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 198, Train Loss: 178.44894933700562, Val Loss: 34.35682189464569\n",
      "Time: 0.13s, CPU: 49.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 199, Train Loss: 178.30361276865005, Val Loss: 34.37422835826874\n",
      "Time: 0.12s, CPU: 50.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 200, Train Loss: 178.1374238729477, Val Loss: 34.39281606674194\n",
      "Time: 0.13s, CPU: 44.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 201, Train Loss: 177.27304029464722, Val Loss: 34.49368214607239\n",
      "Time: 0.12s, CPU: 51.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 202, Train Loss: 176.96148240566254, Val Loss: 34.529705286026\n",
      "Time: 0.13s, CPU: 54.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 203, Train Loss: 176.67283099889755, Val Loss: 34.477328181266785\n",
      "Time: 0.11s, CPU: 44.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 204, Train Loss: 176.7894992828369, Val Loss: 34.50699770450592\n",
      "Time: 0.15s, CPU: 56.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 205, Train Loss: 176.6574615240097, Val Loss: 34.53206777572632\n",
      "Time: 0.12s, CPU: 48.80%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 206, Train Loss: 176.54370445013046, Val Loss: 34.51800870895386\n",
      "Time: 0.14s, CPU: 53.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 207, Train Loss: 176.48658949136734, Val Loss: 34.529956340789795\n",
      "Time: 0.11s, CPU: 46.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 208, Train Loss: 176.41018545627594, Val Loss: 34.544456362724304\n",
      "Time: 0.12s, CPU: 52.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 209, Train Loss: 176.3204327225685, Val Loss: 34.54946458339691\n",
      "Time: 0.11s, CPU: 46.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 210, Train Loss: 176.24050945043564, Val Loss: 34.5584671497345\n",
      "Time: 0.12s, CPU: 51.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 211, Train Loss: 176.1634727716446, Val Loss: 34.56604063510895\n",
      "Time: 0.11s, CPU: 41.00%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 212, Train Loss: 176.0799776315689, Val Loss: 34.57508826255798\n",
      "Time: 0.11s, CPU: 51.70%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 213, Train Loss: 175.9946804046631, Val Loss: 34.585442662239075\n",
      "Time: 0.14s, CPU: 45.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 214, Train Loss: 175.9081107378006, Val Loss: 34.595253109931946\n",
      "Time: 0.12s, CPU: 42.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 215, Train Loss: 175.8226798772812, Val Loss: 34.606669664382935\n",
      "Time: 0.21s, CPU: 54.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 216, Train Loss: 175.74144542217255, Val Loss: 34.61786413192749\n",
      "Time: 0.12s, CPU: 48.80%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 217, Train Loss: 175.65339386463165, Val Loss: 34.62881076335907\n",
      "Time: 0.17s, CPU: 52.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 218, Train Loss: 175.56866317987442, Val Loss: 34.63765239715576\n",
      "Time: 0.42s, CPU: 70.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 11.50%\n",
      "Epoch 219, Train Loss: 175.4803694486618, Val Loss: 34.64522588253021\n",
      "Time: 0.16s, CPU: 76.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 8.00%\n",
      "Epoch 220, Train Loss: 175.3978332877159, Val Loss: 34.65078127384186\n",
      "Time: 0.11s, CPU: 38.05%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 221, Train Loss: 175.3146995306015, Val Loss: 34.655171513557434\n",
      "Time: 0.13s, CPU: 53.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 222, Train Loss: 175.22871536016464, Val Loss: 34.65593111515045\n",
      "Time: 0.11s, CPU: 46.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 223, Train Loss: 175.14727127552032, Val Loss: 34.66035354137421\n",
      "Time: 0.12s, CPU: 62.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 224, Train Loss: 175.0629185438156, Val Loss: 34.66205942630768\n",
      "Time: 0.14s, CPU: 59.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 225, Train Loss: 174.9779042005539, Val Loss: 34.66361081600189\n",
      "Time: 0.12s, CPU: 54.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 226, Train Loss: 174.90061700344086, Val Loss: 34.665661096572876\n",
      "Time: 0.13s, CPU: 47.80%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 227, Train Loss: 174.82316809892654, Val Loss: 34.66633701324463\n",
      "Time: 0.13s, CPU: 49.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 228, Train Loss: 174.74378907680511, Val Loss: 34.66574800014496\n",
      "Time: 0.14s, CPU: 50.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 229, Train Loss: 174.66783660650253, Val Loss: 34.66556453704834\n",
      "Time: 0.11s, CPU: 46.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 230, Train Loss: 174.59510266780853, Val Loss: 34.66801071166992\n",
      "Time: 0.13s, CPU: 50.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 231, Train Loss: 174.51971912384033, Val Loss: 34.67020905017853\n",
      "Time: 0.12s, CPU: 53.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 232, Train Loss: 174.44617158174515, Val Loss: 34.670048117637634\n",
      "Time: 0.11s, CPU: 43.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 233, Train Loss: 174.3702393770218, Val Loss: 34.67070150375366\n",
      "Time: 0.12s, CPU: 48.35%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 234, Train Loss: 174.29737842082977, Val Loss: 34.6721305847168\n",
      "Time: 0.12s, CPU: 45.10%, Memory: 3.32GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 235, Train Loss: 174.22494554519653, Val Loss: 34.673904061317444\n",
      "Time: 0.17s, CPU: 54.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 236, Train Loss: 174.1517550945282, Val Loss: 34.674007058143616\n",
      "Time: 0.14s, CPU: 47.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 237, Train Loss: 174.07464450597763, Val Loss: 34.67538142204285\n",
      "Time: 0.14s, CPU: 50.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 238, Train Loss: 174.00446915626526, Val Loss: 34.679192304611206\n",
      "Time: 0.12s, CPU: 46.35%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 239, Train Loss: 173.93252336978912, Val Loss: 34.681416392326355\n",
      "Time: 0.12s, CPU: 41.65%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 240, Train Loss: 173.86063730716705, Val Loss: 34.68649220466614\n",
      "Time: 0.13s, CPU: 46.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 241, Train Loss: 173.79564344882965, Val Loss: 34.69217312335968\n",
      "Time: 0.70s, CPU: 62.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 9.00%\n",
      "Epoch 242, Train Loss: 173.7214656472206, Val Loss: 34.69497334957123\n",
      "Time: 0.87s, CPU: 77.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 243, Train Loss: 173.65419232845306, Val Loss: 34.7010372877121\n",
      "Time: 0.83s, CPU: 63.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 4.00%\n",
      "Epoch 244, Train Loss: 173.5876836180687, Val Loss: 34.70558845996857\n",
      "Time: 1.20s, CPU: 85.70%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 245, Train Loss: 173.5173916220665, Val Loss: 34.709251284599304\n",
      "Time: 0.65s, CPU: 88.15%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 246, Train Loss: 173.4534792304039, Val Loss: 34.7118262052536\n",
      "Time: 0.12s, CPU: 47.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 9.00%\n",
      "Epoch 247, Train Loss: 173.38224333524704, Val Loss: 34.71519935131073\n",
      "Time: 0.11s, CPU: 42.85%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 248, Train Loss: 173.31371492147446, Val Loss: 34.71972155570984\n",
      "Time: 0.11s, CPU: 45.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 249, Train Loss: 173.24761164188385, Val Loss: 34.72211301326752\n",
      "Time: 0.11s, CPU: 43.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.50%\n",
      "Epoch 250, Train Loss: 173.17679983377457, Val Loss: 34.72519326210022\n",
      "Time: 0.11s, CPU: 44.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 251, Train Loss: 173.10945850610733, Val Loss: 34.72894620895386\n",
      "Time: 0.11s, CPU: 49.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 252, Train Loss: 173.039764046669, Val Loss: 34.73186230659485\n",
      "Time: 0.15s, CPU: 43.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 253, Train Loss: 172.96840113401413, Val Loss: 34.734981179237366\n",
      "Time: 0.11s, CPU: 46.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 254, Train Loss: 172.90388756990433, Val Loss: 34.74258041381836\n",
      "Time: 0.12s, CPU: 42.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 255, Train Loss: 172.83385682106018, Val Loss: 34.74647498130798\n",
      "Time: 0.14s, CPU: 33.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 256, Train Loss: 172.76601713895798, Val Loss: 34.748467326164246\n",
      "Time: 0.17s, CPU: 41.55%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 257, Train Loss: 172.7023652791977, Val Loss: 34.75236511230469\n",
      "Time: 0.12s, CPU: 51.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 258, Train Loss: 172.63216590881348, Val Loss: 34.75473082065582\n",
      "Time: 0.13s, CPU: 54.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 259, Train Loss: 172.56311213970184, Val Loss: 34.75859320163727\n",
      "Time: 0.13s, CPU: 51.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 260, Train Loss: 172.50000059604645, Val Loss: 34.76510775089264\n",
      "Time: 0.12s, CPU: 55.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 261, Train Loss: 172.43282079696655, Val Loss: 34.76976835727692\n",
      "Time: 0.13s, CPU: 52.00%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 262, Train Loss: 172.36711531877518, Val Loss: 34.77578401565552\n",
      "Time: 0.12s, CPU: 49.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 263, Train Loss: 172.3047005534172, Val Loss: 34.78373408317566\n",
      "Time: 0.12s, CPU: 41.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 264, Train Loss: 172.24025136232376, Val Loss: 34.79076683521271\n",
      "Time: 0.14s, CPU: 73.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 265, Train Loss: 172.17419803142548, Val Loss: 34.79804742336273\n",
      "Time: 0.16s, CPU: 49.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 266, Train Loss: 172.11282444000244, Val Loss: 34.806705594062805\n",
      "Time: 0.12s, CPU: 58.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 267, Train Loss: 172.0507614016533, Val Loss: 34.81295299530029\n",
      "Time: 0.19s, CPU: 63.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 268, Train Loss: 171.98685842752457, Val Loss: 34.822097182273865\n",
      "Time: 0.20s, CPU: 66.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 10.00%\n",
      "Epoch 269, Train Loss: 171.92826282978058, Val Loss: 34.83069097995758\n",
      "Time: 0.21s, CPU: 74.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 270, Train Loss: 171.86184841394424, Val Loss: 34.839590549468994\n",
      "Time: 0.16s, CPU: 52.50%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 11.00%\n",
      "Epoch 271, Train Loss: 171.80270594358444, Val Loss: 34.84976148605347\n",
      "Time: 0.11s, CPU: 48.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 272, Train Loss: 171.74175733327866, Val Loss: 34.85873186588287\n",
      "Time: 0.12s, CPU: 48.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 273, Train Loss: 171.6814462542534, Val Loss: 34.86658537387848\n",
      "Time: 0.12s, CPU: 43.00%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 274, Train Loss: 171.6248888373375, Val Loss: 34.875253200531006\n",
      "Time: 0.16s, CPU: 40.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 275, Train Loss: 171.56375175714493, Val Loss: 34.88280415534973\n",
      "Time: 0.13s, CPU: 51.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 276, Train Loss: 171.50095689296722, Val Loss: 34.891764879226685\n",
      "Time: 0.13s, CPU: 51.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 277, Train Loss: 171.44216406345367, Val Loss: 34.90262460708618\n",
      "Time: 0.12s, CPU: 47.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 278, Train Loss: 171.37864232063293, Val Loss: 34.91265070438385\n",
      "Time: 0.11s, CPU: 43.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 279, Train Loss: 171.31763809919357, Val Loss: 34.92448568344116\n",
      "Time: 0.10s, CPU: 43.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 280, Train Loss: 171.25898492336273, Val Loss: 34.93714141845703\n",
      "Time: 0.13s, CPU: 52.80%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 21.00%\n",
      "Epoch 281, Train Loss: 171.1986762881279, Val Loss: 34.94663321971893\n",
      "Time: 0.13s, CPU: 39.10%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 282, Train Loss: 171.13992136716843, Val Loss: 34.95663356781006\n",
      "Time: 0.11s, CPU: 44.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 283, Train Loss: 171.08512961864471, Val Loss: 34.96848785877228\n",
      "Time: 0.12s, CPU: 48.30%, Memory: 3.32GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 284, Train Loss: 171.02742737531662, Val Loss: 34.97885513305664\n",
      "Time: 0.13s, CPU: 67.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 285, Train Loss: 170.9729766845703, Val Loss: 34.98833727836609\n",
      "Time: 0.12s, CPU: 51.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 286, Train Loss: 170.91537809371948, Val Loss: 34.99763596057892\n",
      "Time: 0.20s, CPU: 53.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 287, Train Loss: 170.8602305650711, Val Loss: 35.007015109062195\n",
      "Time: 0.14s, CPU: 43.35%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.50%\n",
      "Epoch 288, Train Loss: 170.80009412765503, Val Loss: 35.01556384563446\n",
      "Time: 0.11s, CPU: 43.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 289, Train Loss: 170.7423014640808, Val Loss: 35.023478507995605\n",
      "Time: 0.12s, CPU: 50.45%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 290, Train Loss: 170.68475955724716, Val Loss: 35.03193390369415\n",
      "Time: 0.12s, CPU: 51.05%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.50%\n",
      "Epoch 291, Train Loss: 170.6221284866333, Val Loss: 35.03577375411987\n",
      "Time: 0.11s, CPU: 48.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 292, Train Loss: 170.56386250257492, Val Loss: 35.03961682319641\n",
      "Time: 0.11s, CPU: 42.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 293, Train Loss: 170.5056699514389, Val Loss: 35.04318308830261\n",
      "Time: 0.15s, CPU: 53.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 294, Train Loss: 170.44351810216904, Val Loss: 35.04341161251068\n",
      "Time: 0.12s, CPU: 35.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 295, Train Loss: 170.38298296928406, Val Loss: 35.04620862007141\n",
      "Time: 0.11s, CPU: 44.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 296, Train Loss: 170.32928907871246, Val Loss: 35.050376772880554\n",
      "Time: 0.11s, CPU: 47.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 297, Train Loss: 170.26734268665314, Val Loss: 35.050270557403564\n",
      "Time: 0.13s, CPU: 54.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 298, Train Loss: 170.2101594209671, Val Loss: 35.05130052566528\n",
      "Time: 0.13s, CPU: 47.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 299, Train Loss: 170.15590870380402, Val Loss: 35.05452883243561\n",
      "Time: 0.11s, CPU: 43.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 300, Train Loss: 170.09651958942413, Val Loss: 35.0559868812561\n",
      "Time: 0.12s, CPU: 54.00%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 301, Train Loss: 170.04041528701782, Val Loss: 35.059913635253906\n",
      "Time: 0.12s, CPU: 42.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 302, Train Loss: 169.98369455337524, Val Loss: 35.062820076942444\n",
      "Time: 0.12s, CPU: 50.25%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 303, Train Loss: 169.9260568022728, Val Loss: 35.06754183769226\n",
      "Time: 0.12s, CPU: 49.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 304, Train Loss: 169.86871564388275, Val Loss: 35.07152330875397\n",
      "Time: 0.42s, CPU: 54.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 11.50%\n",
      "Epoch 305, Train Loss: 169.80858194828033, Val Loss: 35.073554277420044\n",
      "Time: 0.40s, CPU: 60.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 306, Train Loss: 169.75219243764877, Val Loss: 35.07627081871033\n",
      "Time: 0.14s, CPU: 56.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 307, Train Loss: 169.69668090343475, Val Loss: 35.07818591594696\n",
      "Time: 0.13s, CPU: 43.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 308, Train Loss: 169.64104974269867, Val Loss: 35.08059024810791\n",
      "Time: 0.12s, CPU: 45.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 309, Train Loss: 169.58812582492828, Val Loss: 35.08125650882721\n",
      "Time: 0.14s, CPU: 47.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 310, Train Loss: 169.53164911270142, Val Loss: 35.083351850509644\n",
      "Time: 0.12s, CPU: 48.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 311, Train Loss: 169.4798790216446, Val Loss: 35.088459849357605\n",
      "Time: 0.12s, CPU: 43.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 312, Train Loss: 169.4275485277176, Val Loss: 35.09082555770874\n",
      "Time: 0.12s, CPU: 49.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 313, Train Loss: 169.37202787399292, Val Loss: 35.09405064582825\n",
      "Time: 0.12s, CPU: 51.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 314, Train Loss: 169.32553851604462, Val Loss: 35.09695065021515\n",
      "Time: 0.15s, CPU: 59.55%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 315, Train Loss: 169.27172726392746, Val Loss: 35.100571632385254\n",
      "Time: 0.12s, CPU: 54.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 316, Train Loss: 169.22108852863312, Val Loss: 35.10487496852875\n",
      "Time: 0.11s, CPU: 50.00%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 317, Train Loss: 169.16884952783585, Val Loss: 35.10897874832153\n",
      "Time: 0.13s, CPU: 56.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 318, Train Loss: 169.11785501241684, Val Loss: 35.11228108406067\n",
      "Time: 0.14s, CPU: 49.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 319, Train Loss: 169.066326379776, Val Loss: 35.114115715026855\n",
      "Time: 0.12s, CPU: 51.15%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 320, Train Loss: 169.01686823368073, Val Loss: 35.114827036857605\n",
      "Time: 0.12s, CPU: 53.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 321, Train Loss: 168.96464264392853, Val Loss: 35.116896629333496\n",
      "Time: 0.12s, CPU: 44.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 322, Train Loss: 168.91283535957336, Val Loss: 35.11933958530426\n",
      "Time: 0.12s, CPU: 47.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 323, Train Loss: 168.8622796535492, Val Loss: 35.12269341945648\n",
      "Time: 0.11s, CPU: 55.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 324, Train Loss: 168.80847716331482, Val Loss: 35.125500082969666\n",
      "Time: 0.20s, CPU: 49.70%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 325, Train Loss: 168.75954627990723, Val Loss: 35.12953305244446\n",
      "Time: 0.13s, CPU: 55.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 326, Train Loss: 168.71119886636734, Val Loss: 35.134422183036804\n",
      "Time: 0.11s, CPU: 45.65%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.50%\n",
      "Epoch 327, Train Loss: 168.65868210792542, Val Loss: 35.138516306877136\n",
      "Time: 0.14s, CPU: 46.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 328, Train Loss: 168.61062037944794, Val Loss: 35.14468002319336\n",
      "Time: 0.11s, CPU: 41.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 329, Train Loss: 168.56270849704742, Val Loss: 35.1526654958725\n",
      "Time: 0.12s, CPU: 55.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 330, Train Loss: 168.51290225982666, Val Loss: 35.15926373004913\n",
      "Time: 0.14s, CPU: 47.05%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 331, Train Loss: 168.46631580591202, Val Loss: 35.165295481681824\n",
      "Time: 0.11s, CPU: 50.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 332, Train Loss: 168.41985833644867, Val Loss: 35.17284321784973\n",
      "Time: 0.14s, CPU: 50.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 333, Train Loss: 168.37166637182236, Val Loss: 35.177220582962036\n",
      "Time: 0.13s, CPU: 50.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 334, Train Loss: 168.32152128219604, Val Loss: 35.18194556236267\n",
      "Time: 0.13s, CPU: 41.55%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 335, Train Loss: 168.27847588062286, Val Loss: 35.188305616378784\n",
      "Time: 0.12s, CPU: 45.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 336, Train Loss: 168.23114156723022, Val Loss: 35.19363248348236\n",
      "Time: 0.11s, CPU: 48.60%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 337, Train Loss: 168.18516063690186, Val Loss: 35.20101284980774\n",
      "Time: 0.11s, CPU: 48.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 338, Train Loss: 168.14165765047073, Val Loss: 35.2075434923172\n",
      "Time: 0.21s, CPU: 61.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 339, Train Loss: 168.0941540002823, Val Loss: 35.21644949913025\n",
      "Time: 0.28s, CPU: 62.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 340, Train Loss: 168.05019009113312, Val Loss: 35.22535228729248\n",
      "Time: 0.23s, CPU: 62.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 9.50%\n",
      "Epoch 341, Train Loss: 168.00323581695557, Val Loss: 35.23057293891907\n",
      "Time: 0.15s, CPU: 48.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 12.00%\n",
      "Epoch 342, Train Loss: 167.95708256959915, Val Loss: 35.23896396160126\n",
      "Time: 0.12s, CPU: 38.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 12.00%\n",
      "Epoch 343, Train Loss: 167.91556704044342, Val Loss: 35.24824655056\n",
      "Time: 0.12s, CPU: 50.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 344, Train Loss: 167.86612612009048, Val Loss: 35.25522458553314\n",
      "Time: 0.12s, CPU: 46.55%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 345, Train Loss: 167.820445895195, Val Loss: 35.263081312179565\n",
      "Time: 0.12s, CPU: 50.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 346, Train Loss: 167.7747632265091, Val Loss: 35.270307183265686\n",
      "Time: 0.13s, CPU: 51.50%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 347, Train Loss: 167.72638857364655, Val Loss: 35.27556002140045\n",
      "Time: 0.11s, CPU: 45.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 348, Train Loss: 167.68103939294815, Val Loss: 35.283506870269775\n",
      "Time: 0.12s, CPU: 46.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 349, Train Loss: 167.63797092437744, Val Loss: 35.28900110721588\n",
      "Time: 0.15s, CPU: 42.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 350, Train Loss: 167.585668861866, Val Loss: 35.29165005683899\n",
      "Time: 0.13s, CPU: 49.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 351, Train Loss: 167.54003822803497, Val Loss: 35.295042514801025\n",
      "Time: 0.12s, CPU: 39.80%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 352, Train Loss: 167.49344503879547, Val Loss: 35.29771399497986\n",
      "Time: 0.12s, CPU: 40.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 353, Train Loss: 167.4432560801506, Val Loss: 35.300713777542114\n",
      "Time: 0.14s, CPU: 50.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 354, Train Loss: 167.39600962400436, Val Loss: 35.304733872413635\n",
      "Time: 0.15s, CPU: 53.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 355, Train Loss: 167.34490299224854, Val Loss: 35.30846428871155\n",
      "Time: 0.12s, CPU: 47.15%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 356, Train Loss: 167.30140417814255, Val Loss: 35.31352722644806\n",
      "Time: 0.13s, CPU: 50.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 357, Train Loss: 167.25373709201813, Val Loss: 35.315348982810974\n",
      "Time: 0.37s, CPU: 70.00%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 10.00%\n",
      "Epoch 358, Train Loss: 167.20755088329315, Val Loss: 35.320244550704956\n",
      "Time: 0.15s, CPU: 54.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 359, Train Loss: 167.16396516561508, Val Loss: 35.32435476779938\n",
      "Time: 0.11s, CPU: 46.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 360, Train Loss: 167.11561566591263, Val Loss: 35.32923102378845\n",
      "Time: 0.11s, CPU: 46.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.50%\n",
      "Epoch 361, Train Loss: 167.0703320503235, Val Loss: 35.3351469039917\n",
      "Time: 0.12s, CPU: 46.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 362, Train Loss: 167.02614283561707, Val Loss: 35.34219253063202\n",
      "Time: 0.12s, CPU: 43.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 363, Train Loss: 166.98317497968674, Val Loss: 35.34804081916809\n",
      "Time: 0.12s, CPU: 49.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 364, Train Loss: 166.93636107444763, Val Loss: 35.35541474819183\n",
      "Time: 0.12s, CPU: 42.45%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.50%\n",
      "Epoch 365, Train Loss: 166.89378082752228, Val Loss: 35.36290454864502\n",
      "Time: 0.13s, CPU: 47.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 366, Train Loss: 166.85129868984222, Val Loss: 35.36836016178131\n",
      "Time: 0.13s, CPU: 52.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 367, Train Loss: 166.80808293819427, Val Loss: 35.37781012058258\n",
      "Time: 0.13s, CPU: 45.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 368, Train Loss: 166.76686656475067, Val Loss: 35.38590180873871\n",
      "Time: 0.14s, CPU: 54.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 369, Train Loss: 166.7222043275833, Val Loss: 35.39224898815155\n",
      "Time: 0.11s, CPU: 43.95%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 370, Train Loss: 166.68013310432434, Val Loss: 35.40193068981171\n",
      "Time: 0.13s, CPU: 54.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 371, Train Loss: 166.6350976228714, Val Loss: 35.410231590270996\n",
      "Time: 0.15s, CPU: 49.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 372, Train Loss: 166.59081614017487, Val Loss: 35.41765058040619\n",
      "Time: 0.18s, CPU: 55.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 373, Train Loss: 166.54460990428925, Val Loss: 35.42203760147095\n",
      "Time: 0.31s, CPU: 71.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 9.50%\n",
      "Epoch 374, Train Loss: 166.49923020601273, Val Loss: 35.43029344081879\n",
      "Time: 0.20s, CPU: 64.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 7.00%\n",
      "Epoch 375, Train Loss: 166.45682376623154, Val Loss: 35.43689811229706\n",
      "Time: 0.12s, CPU: 45.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 376, Train Loss: 166.4092374444008, Val Loss: 35.441285133361816\n",
      "Time: 0.13s, CPU: 49.55%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 377, Train Loss: 166.3673364520073, Val Loss: 35.44626760482788\n",
      "Time: 0.15s, CPU: 50.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 378, Train Loss: 166.32440423965454, Val Loss: 35.4484144449234\n",
      "Time: 0.13s, CPU: 52.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 379, Train Loss: 166.27696251869202, Val Loss: 35.453834652900696\n",
      "Time: 0.12s, CPU: 53.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 380, Train Loss: 166.23554039001465, Val Loss: 35.460410356521606\n",
      "Time: 0.13s, CPU: 52.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 381, Train Loss: 166.19074094295502, Val Loss: 35.46466863155365\n",
      "Time: 0.12s, CPU: 46.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 382, Train Loss: 166.14276957511902, Val Loss: 35.47011458873749\n",
      "Time: 0.11s, CPU: 48.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 383, Train Loss: 166.09884989261627, Val Loss: 35.47523868083954\n",
      "Time: 0.13s, CPU: 47.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 384, Train Loss: 166.05149495601654, Val Loss: 35.47806465625763\n",
      "Time: 0.11s, CPU: 52.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 385, Train Loss: 166.00586068630219, Val Loss: 35.48184657096863\n",
      "Time: 0.11s, CPU: 50.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 386, Train Loss: 165.96304714679718, Val Loss: 35.484588861465454\n",
      "Time: 0.13s, CPU: 60.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 387, Train Loss: 165.91772282123566, Val Loss: 35.49028265476227\n",
      "Time: 0.12s, CPU: 56.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 388, Train Loss: 165.8748013973236, Val Loss: 35.49679720401764\n",
      "Time: 0.13s, CPU: 49.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 389, Train Loss: 165.82883006334305, Val Loss: 35.502217411994934\n",
      "Time: 0.12s, CPU: 45.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 390, Train Loss: 165.78217661380768, Val Loss: 35.50888967514038\n",
      "Time: 0.12s, CPU: 52.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 391, Train Loss: 165.74132573604584, Val Loss: 35.51523685455322\n",
      "Time: 0.13s, CPU: 53.85%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 20.00%\n",
      "Epoch 392, Train Loss: 165.69706410169601, Val Loss: 35.520782589912415\n",
      "Time: 0.15s, CPU: 63.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 393, Train Loss: 165.65302658081055, Val Loss: 35.52552688121796\n",
      "Time: 0.12s, CPU: 43.70%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 394, Train Loss: 165.61287820339203, Val Loss: 35.533316016197205\n",
      "Time: 0.12s, CPU: 53.00%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 395, Train Loss: 165.5729616880417, Val Loss: 35.54127252101898\n",
      "Time: 0.12s, CPU: 44.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 396, Train Loss: 165.529414832592, Val Loss: 35.54685366153717\n",
      "Time: 0.76s, CPU: 65.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 10.00%\n",
      "Epoch 397, Train Loss: 165.48937660455704, Val Loss: 35.55600428581238\n",
      "Time: 1.41s, CPU: 85.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 398, Train Loss: 165.448428273201, Val Loss: 35.563297748565674\n",
      "Time: 1.56s, CPU: 89.20%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 399, Train Loss: 165.40287494659424, Val Loss: 35.567996978759766\n",
      "Time: 0.99s, CPU: 86.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 400, Train Loss: 165.36297082901, Val Loss: 35.576117634773254\n",
      "Time: 1.58s, CPU: 87.50%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 401, Train Loss: 164.91565322875977, Val Loss: 35.60563266277313\n",
      "Time: 1.55s, CPU: 89.05%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 402, Train Loss: 164.88106948137283, Val Loss: 35.64137899875641\n",
      "Time: 1.54s, CPU: 88.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 403, Train Loss: 164.7716253399849, Val Loss: 35.637104630470276\n",
      "Time: 1.45s, CPU: 87.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 404, Train Loss: 164.7473782300949, Val Loss: 35.62053179740906\n",
      "Time: 1.45s, CPU: 88.55%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 405, Train Loss: 164.75585854053497, Val Loss: 35.62400150299072\n",
      "Time: 0.11s, CPU: 49.65%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 406, Train Loss: 164.7374785542488, Val Loss: 35.6359201669693\n",
      "Time: 0.12s, CPU: 57.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 407, Train Loss: 164.7076289653778, Val Loss: 35.63856589794159\n",
      "Time: 0.12s, CPU: 42.15%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 408, Train Loss: 164.68273413181305, Val Loss: 35.63744258880615\n",
      "Time: 0.12s, CPU: 46.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 409, Train Loss: 164.66230070590973, Val Loss: 35.63883304595947\n",
      "Time: 0.11s, CPU: 42.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 410, Train Loss: 164.6427719593048, Val Loss: 35.64060974121094\n",
      "Time: 0.19s, CPU: 63.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 411, Train Loss: 164.62073123455048, Val Loss: 35.6429979801178\n",
      "Time: 0.34s, CPU: 66.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 412, Train Loss: 164.5989158153534, Val Loss: 35.64425003528595\n",
      "Time: 0.13s, CPU: 70.10%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 413, Train Loss: 164.5756351351738, Val Loss: 35.64470708370209\n",
      "Time: 0.13s, CPU: 54.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 414, Train Loss: 164.55575877428055, Val Loss: 35.64547634124756\n",
      "Time: 1.01s, CPU: 62.35%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 9.50%\n",
      "Epoch 415, Train Loss: 164.53486424684525, Val Loss: 35.64490020275116\n",
      "Time: 0.68s, CPU: 84.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 416, Train Loss: 164.51119548082352, Val Loss: 35.64425003528595\n",
      "Time: 1.28s, CPU: 85.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 417, Train Loss: 164.49123388528824, Val Loss: 35.64524781703949\n",
      "Time: 1.05s, CPU: 84.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 418, Train Loss: 164.4698105454445, Val Loss: 35.64575958251953\n",
      "Time: 1.57s, CPU: 64.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 419, Train Loss: 164.44657891988754, Val Loss: 35.64458477497101\n",
      "Time: 1.49s, CPU: 87.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 420, Train Loss: 164.4256362915039, Val Loss: 35.64570486545563\n",
      "Time: 0.96s, CPU: 87.30%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 421, Train Loss: 164.40671300888062, Val Loss: 35.648476123809814\n",
      "Time: 0.14s, CPU: 46.60%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 8.50%\n",
      "Epoch 422, Train Loss: 164.38566142320633, Val Loss: 35.64995992183685\n",
      "Time: 0.14s, CPU: 53.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 423, Train Loss: 164.3628602027893, Val Loss: 35.650639057159424\n",
      "Time: 0.12s, CPU: 43.25%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 424, Train Loss: 164.34219372272491, Val Loss: 35.65267324447632\n",
      "Time: 0.14s, CPU: 43.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 425, Train Loss: 164.32273316383362, Val Loss: 35.65492630004883\n",
      "Time: 0.11s, CPU: 49.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 426, Train Loss: 164.301071703434, Val Loss: 35.65656137466431\n",
      "Time: 0.12s, CPU: 49.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 427, Train Loss: 164.27828377485275, Val Loss: 35.65826082229614\n",
      "Time: 0.11s, CPU: 50.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 428, Train Loss: 164.25708097219467, Val Loss: 35.65927469730377\n",
      "Time: 0.14s, CPU: 51.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 429, Train Loss: 164.23623096942902, Val Loss: 35.660964488983154\n",
      "Time: 0.13s, CPU: 49.35%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 430, Train Loss: 164.21451997756958, Val Loss: 35.66263818740845\n",
      "Time: 0.12s, CPU: 53.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 431, Train Loss: 164.1916350722313, Val Loss: 35.66435694694519\n",
      "Time: 0.13s, CPU: 45.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 432, Train Loss: 164.17157804965973, Val Loss: 35.6668803691864\n",
      "Time: 0.12s, CPU: 53.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 433, Train Loss: 164.15004909038544, Val Loss: 35.66904973983765\n",
      "Time: 0.21s, CPU: 54.70%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 434, Train Loss: 164.1277025938034, Val Loss: 35.66984796524048\n",
      "Time: 0.20s, CPU: 55.05%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 435, Train Loss: 164.10417467355728, Val Loss: 35.67064297199249\n",
      "Time: 0.16s, CPU: 70.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 436, Train Loss: 164.0836170911789, Val Loss: 35.67239713668823\n",
      "Time: 0.13s, CPU: 46.75%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 11.00%\n",
      "Epoch 437, Train Loss: 164.06222140789032, Val Loss: 35.67315673828125\n",
      "Time: 0.12s, CPU: 48.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 438, Train Loss: 164.0401070713997, Val Loss: 35.672757625579834\n",
      "Time: 0.18s, CPU: 54.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 439, Train Loss: 164.01767337322235, Val Loss: 35.67297649383545\n",
      "Time: 0.13s, CPU: 44.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 440, Train Loss: 163.9980765581131, Val Loss: 35.67418348789215\n",
      "Time: 0.15s, CPU: 47.40%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 441, Train Loss: 163.97687137126923, Val Loss: 35.6755610704422\n",
      "Time: 0.11s, CPU: 50.15%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 442, Train Loss: 163.954863011837, Val Loss: 35.676098585128784\n",
      "Time: 0.13s, CPU: 42.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 443, Train Loss: 163.93320405483246, Val Loss: 35.67595374584198\n",
      "Time: 0.14s, CPU: 52.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 444, Train Loss: 163.9126261472702, Val Loss: 35.67660391330719\n",
      "Time: 0.25s, CPU: 50.85%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 445, Train Loss: 163.8926722407341, Val Loss: 35.67805230617523\n",
      "Time: 1.28s, CPU: 87.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 5.00%\n",
      "Epoch 446, Train Loss: 163.87156331539154, Val Loss: 35.67933011054993\n",
      "Time: 1.48s, CPU: 86.40%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 447, Train Loss: 163.85029941797256, Val Loss: 35.68016052246094\n",
      "Time: 1.50s, CPU: 83.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 0.50%\n",
      "Epoch 448, Train Loss: 163.8290193080902, Val Loss: 35.6808203458786\n",
      "Time: 0.89s, CPU: 84.20%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 449, Train Loss: 163.80882441997528, Val Loss: 35.68232023715973\n",
      "Time: 1.53s, CPU: 82.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 450, Train Loss: 163.78891336917877, Val Loss: 35.684895157814026\n",
      "Time: 1.00s, CPU: 87.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 451, Train Loss: 163.76875519752502, Val Loss: 35.68736708164215\n",
      "Time: 0.12s, CPU: 36.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 6.00%\n",
      "Epoch 452, Train Loss: 163.74697715044022, Val Loss: 35.69033467769623\n",
      "Time: 0.14s, CPU: 49.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 453, Train Loss: 163.7278469800949, Val Loss: 35.69376254081726\n",
      "Time: 0.12s, CPU: 41.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.00%\n",
      "Epoch 454, Train Loss: 163.70709443092346, Val Loss: 35.696553111076355\n",
      "Time: 0.15s, CPU: 52.75%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 455, Train Loss: 163.6860870718956, Val Loss: 35.69834268093109\n",
      "Time: 0.12s, CPU: 46.20%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 15.50%\n",
      "Epoch 456, Train Loss: 163.6645936369896, Val Loss: 35.70043158531189\n",
      "Time: 0.14s, CPU: 53.30%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 457, Train Loss: 163.64579916000366, Val Loss: 35.702842354774475\n",
      "Time: 0.14s, CPU: 44.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 458, Train Loss: 163.62564033269882, Val Loss: 35.70500206947327\n",
      "Time: 0.12s, CPU: 51.80%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 16.00%\n",
      "Epoch 459, Train Loss: 163.6045743227005, Val Loss: 35.70698797702789\n",
      "Time: 0.12s, CPU: 53.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 460, Train Loss: 163.58338671922684, Val Loss: 35.70871961116791\n",
      "Time: 0.12s, CPU: 45.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 461, Train Loss: 163.5638426542282, Val Loss: 35.71107566356659\n",
      "Time: 0.19s, CPU: 55.60%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 462, Train Loss: 163.543807387352, Val Loss: 35.71406900882721\n",
      "Time: 0.16s, CPU: 58.60%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 463, Train Loss: 163.52360928058624, Val Loss: 35.71663749217987\n",
      "Time: 0.24s, CPU: 65.30%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 7.50%\n",
      "Epoch 464, Train Loss: 163.50127166509628, Val Loss: 35.718201756477356\n",
      "Time: 0.35s, CPU: 62.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 5.00%\n",
      "Epoch 465, Train Loss: 163.4809666275978, Val Loss: 35.71911263465881\n",
      "Time: 0.27s, CPU: 66.25%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 5.00%\n",
      "Epoch 466, Train Loss: 163.4611298441887, Val Loss: 35.720892548561096\n",
      "Time: 0.19s, CPU: 56.75%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 11.50%\n",
      "Epoch 467, Train Loss: 163.44173324108124, Val Loss: 35.722466468811035\n",
      "Time: 0.12s, CPU: 44.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.00%\n",
      "Epoch 468, Train Loss: 163.4206587076187, Val Loss: 35.7230007648468\n",
      "Time: 0.12s, CPU: 42.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 14.50%\n",
      "Epoch 469, Train Loss: 163.40123784542084, Val Loss: 35.72394061088562\n",
      "Time: 0.12s, CPU: 43.70%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 470, Train Loss: 163.38226056098938, Val Loss: 35.72528922557831\n",
      "Time: 0.12s, CPU: 48.95%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 19.00%\n",
      "Epoch 471, Train Loss: 163.36319679021835, Val Loss: 35.726534843444824\n",
      "Time: 0.13s, CPU: 45.70%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 472, Train Loss: 163.34354639053345, Val Loss: 35.72817313671112\n",
      "Time: 0.12s, CPU: 51.05%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 473, Train Loss: 163.3248974084854, Val Loss: 35.73044228553772\n",
      "Time: 0.13s, CPU: 40.45%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 474, Train Loss: 163.30642068386078, Val Loss: 35.73367702960968\n",
      "Time: 0.13s, CPU: 41.80%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 475, Train Loss: 163.28778564929962, Val Loss: 35.73677337169647\n",
      "Time: 0.11s, CPU: 42.40%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 16.50%\n",
      "Epoch 476, Train Loss: 163.26838338375092, Val Loss: 35.73894274234772\n",
      "Time: 0.14s, CPU: 48.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 477, Train Loss: 163.24969112873077, Val Loss: 35.741833090782166\n",
      "Time: 0.12s, CPU: 47.95%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.50%\n",
      "Epoch 478, Train Loss: 163.23098278045654, Val Loss: 35.745067834854126\n",
      "Time: 0.12s, CPU: 51.10%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 479, Train Loss: 163.21065574884415, Val Loss: 35.74751400947571\n",
      "Time: 0.13s, CPU: 40.90%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 17.00%\n",
      "Epoch 480, Train Loss: 163.19084250926971, Val Loss: 35.750072836875916\n",
      "Time: 0.30s, CPU: 55.80%, Memory: 3.32GB, GPU: 0.38GB, GPU Util: 12.50%\n",
      "Epoch 481, Train Loss: 163.17280626296997, Val Loss: 35.75353288650513\n",
      "Time: 1.43s, CPU: 87.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 482, Train Loss: 163.15410882234573, Val Loss: 35.756333112716675\n",
      "Time: 1.35s, CPU: 85.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 483, Train Loss: 163.13419938087463, Val Loss: 35.759007811546326\n",
      "Time: 1.39s, CPU: 82.50%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 484, Train Loss: 163.11570519208908, Val Loss: 35.76185631752014\n",
      "Time: 0.90s, CPU: 68.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 485, Train Loss: 163.0974760055542, Val Loss: 35.76541292667389\n",
      "Time: 1.06s, CPU: 86.45%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 486, Train Loss: 163.07907354831696, Val Loss: 35.76817452907562\n",
      "Time: 0.22s, CPU: 78.10%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 6.50%\n",
      "Epoch 487, Train Loss: 163.0597199201584, Val Loss: 35.76980638504028\n",
      "Time: 1.13s, CPU: 88.15%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 6.50%\n",
      "Epoch 488, Train Loss: 163.04154413938522, Val Loss: 35.77222681045532\n",
      "Time: 0.51s, CPU: 54.10%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 3.00%\n",
      "Epoch 489, Train Loss: 163.02399742603302, Val Loss: 35.77569651603699\n",
      "Time: 1.25s, CPU: 82.65%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 3.50%\n",
      "Epoch 490, Train Loss: 163.0040866136551, Val Loss: 35.77862226963043\n",
      "Time: 1.65s, CPU: 84.35%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 1.00%\n",
      "Epoch 491, Train Loss: 162.9847658276558, Val Loss: 35.781792640686035\n",
      "Time: 1.24s, CPU: 86.85%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 492, Train Loss: 162.96794658899307, Val Loss: 35.785828828811646\n",
      "Time: 0.78s, CPU: 56.90%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 2.00%\n",
      "Epoch 493, Train Loss: 162.94959795475006, Val Loss: 35.78929531574249\n",
      "Time: 1.23s, CPU: 77.65%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 4.50%\n",
      "Epoch 494, Train Loss: 162.93030059337616, Val Loss: 35.791802644729614\n",
      "Time: 1.52s, CPU: 84.55%, Memory: 3.33GB, GPU: 0.38GB, GPU Util: 0.00%\n",
      "Epoch 495, Train Loss: 162.9121631383896, Val Loss: 35.7950599193573\n",
      "Time: 0.22s, CPU: 82.00%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 1.50%\n",
      "Epoch 496, Train Loss: 162.89570915699005, Val Loss: 35.79883861541748\n",
      "Time: 0.11s, CPU: 28.90%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 13.00%\n",
      "Epoch 497, Train Loss: 162.87600553035736, Val Loss: 35.801188230514526\n",
      "Time: 0.11s, CPU: 43.70%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 11.00%\n",
      "Epoch 498, Train Loss: 162.8577532172203, Val Loss: 35.80395948886871\n",
      "Time: 0.11s, CPU: 58.50%, Memory: 3.34GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 499, Train Loss: 162.84030783176422, Val Loss: 35.80677902698517\n",
      "Time: 0.15s, CPU: 56.10%, Memory: 3.35GB, GPU: 0.38GB, GPU Util: 18.00%\n",
      "Epoch 500, Train Loss: 162.82169449329376, Val Loss: 35.808961272239685\n",
      "Time: 0.13s, CPU: 47.85%, Memory: 3.36GB, GPU: 0.38GB, GPU Util: 15.50%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_1l_slim160.pth\n",
      "Average Time per Epoch: 0.30s\n",
      "Average CPU Usage: 55.50%\n",
      "Average Memory Usage: 3.34GB\n",
      "Average GPU Usage: 0.38GB\n",
      "Average GPU Utilization: 13.72%\n",
      "\n",
      "Total Training Time: 151.67s\n",
      "Max CPU Usage: 91.35%\n",
      "Max Memory Usage: 3.36GB\n",
      "Max GPU Usage: 0.38GB\n",
      "Max GPU Utilization: 21.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn1_slim160 = GCN1Layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes)\n",
    "print(gcn1_slim160)\n",
    "print(f\"Total number of trainable parameters: {(gcn1_slim160.count_parameters())*2}\\n\")\n",
    "single_train(gcn1_slim160, slim160_train_loader, slim160_val_loader, \n",
    "            lr=0.01, num_epochs=500, step_size=200, gamma=0.5,\n",
    "            save_path='models/gcn_1l_slim160.pth', binary_classification=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3788\n",
      "Average Sensitivity (Recall): 0.3981\n",
      "Average Specificity: 0.6925\n",
      "\n",
      "Average Inference Time per Batch: 0.0009s\n",
      "Average CPU Usage: 35.02%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.20GB\n",
      "Average GPU Utilization: 1.00%\n"
     ]
    }
   ],
   "source": [
    "gcn1_slim160 = GCN1Layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes)\n",
    "gcn1_slim160.load_state_dict(torch.load('models/gcn_1l_slim160.pth'))\n",
    "single_test(gcn1_slim160.to(device), slim160_test_loader, binary_classification=False)\n",
    "inference_performance(gcn1_slim160.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN 2-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (gcn2): GCN (14 -> 28)\n",
      "  (fc): Linear(in_features=28, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1180\n",
      "\n",
      "Epoch 1, Train Loss: 86.46721732616425, Val Loss: 11.68528139591217\n",
      "Time: 0.03s, CPU: 18.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 0.50%\n",
      "Epoch 2, Train Loss: 80.85846072435379, Val Loss: 11.765201389789581\n",
      "Time: 0.04s, CPU: 24.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.00%\n",
      "Epoch 3, Train Loss: 79.94231593608856, Val Loss: 11.294539868831635\n",
      "Time: 0.03s, CPU: 28.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.00%\n",
      "Epoch 4, Train Loss: 79.69838243722916, Val Loss: 10.979202389717102\n",
      "Time: 0.03s, CPU: 21.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.00%\n",
      "Epoch 5, Train Loss: 78.92959696054459, Val Loss: 10.866539776325226\n",
      "Time: 0.03s, CPU: 19.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 6, Train Loss: 77.34769850969315, Val Loss: 10.953080356121063\n",
      "Time: 0.03s, CPU: 28.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 7, Train Loss: 75.19219267368317, Val Loss: 10.701701045036316\n",
      "Time: 0.04s, CPU: 25.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 8, Train Loss: 73.11086755990982, Val Loss: 10.182015001773834\n",
      "Time: 0.04s, CPU: 23.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 9, Train Loss: 71.41097939014435, Val Loss: 10.199012160301208\n",
      "Time: 0.03s, CPU: 22.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 10, Train Loss: 68.82053601741791, Val Loss: 9.913854897022247\n",
      "Time: 0.03s, CPU: 19.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 11, Train Loss: 67.1939280629158, Val Loss: 9.694613814353943\n",
      "Time: 0.04s, CPU: 22.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 12, Train Loss: 66.07376629114151, Val Loss: 9.734603762626648\n",
      "Time: 0.03s, CPU: 23.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 13, Train Loss: 65.23303931951523, Val Loss: 9.590409994125366\n",
      "Time: 0.04s, CPU: 20.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 14, Train Loss: 65.05367577075958, Val Loss: 9.682471454143524\n",
      "Time: 0.04s, CPU: 25.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 15, Train Loss: 64.67376112937927, Val Loss: 9.622151255607605\n",
      "Time: 0.04s, CPU: 31.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 16, Train Loss: 64.4986070394516, Val Loss: 9.601137042045593\n",
      "Time: 0.03s, CPU: 26.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 17, Train Loss: 64.21160227060318, Val Loss: 9.574739634990692\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 18, Train Loss: 63.90659326314926, Val Loss: 9.484460949897766\n",
      "Time: 0.04s, CPU: 30.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 19, Train Loss: 63.64144566655159, Val Loss: 9.4667387008667\n",
      "Time: 0.03s, CPU: 39.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 20, Train Loss: 63.29860711097717, Val Loss: 9.38691258430481\n",
      "Time: 0.03s, CPU: 27.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 21, Train Loss: 63.05599036812782, Val Loss: 9.36363011598587\n",
      "Time: 0.03s, CPU: 20.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 22, Train Loss: 62.75931531190872, Val Loss: 9.304143190383911\n",
      "Time: 0.04s, CPU: 29.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 23, Train Loss: 62.48479479551315, Val Loss: 9.236418306827545\n",
      "Time: 0.03s, CPU: 38.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 0.00%\n",
      "Epoch 24, Train Loss: 62.211638540029526, Val Loss: 9.192308485507965\n",
      "Time: 0.03s, CPU: 39.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 0.00%\n",
      "Epoch 25, Train Loss: 61.885159492492676, Val Loss: 9.11243051290512\n",
      "Time: 0.03s, CPU: 36.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 0.00%\n",
      "Epoch 26, Train Loss: 61.605986177921295, Val Loss: 9.037686288356781\n",
      "Time: 0.03s, CPU: 44.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 27, Train Loss: 61.35011750459671, Val Loss: 9.007077813148499\n",
      "Time: 0.04s, CPU: 38.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 28, Train Loss: 60.98520013689995, Val Loss: 8.875156939029694\n",
      "Time: 0.03s, CPU: 42.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 29, Train Loss: 60.756968170404434, Val Loss: 8.853980004787445\n",
      "Time: 0.04s, CPU: 36.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 30, Train Loss: 60.37129122018814, Val Loss: 8.730494678020477\n",
      "Time: 0.04s, CPU: 45.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 31, Train Loss: 60.10593894124031, Val Loss: 8.661072850227356\n",
      "Time: 0.05s, CPU: 40.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 32, Train Loss: 59.79510882496834, Val Loss: 8.571540713310242\n",
      "Time: 0.04s, CPU: 38.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 33, Train Loss: 59.472878605127335, Val Loss: 8.463415503501892\n",
      "Time: 0.03s, CPU: 41.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 34, Train Loss: 59.24041107296944, Val Loss: 8.425770699977875\n",
      "Time: 0.03s, CPU: 33.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 35, Train Loss: 58.94946736097336, Val Loss: 8.31486314535141\n",
      "Time: 0.04s, CPU: 48.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 36, Train Loss: 58.76343289017677, Val Loss: 8.288191258907318\n",
      "Time: 0.04s, CPU: 34.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 37, Train Loss: 58.456964910030365, Val Loss: 8.18140983581543\n",
      "Time: 0.03s, CPU: 41.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 38, Train Loss: 58.29679888486862, Val Loss: 8.120956420898438\n",
      "Time: 0.03s, CPU: 36.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 39, Train Loss: 58.10964983701706, Val Loss: 8.085702359676361\n",
      "Time: 0.04s, CPU: 33.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 40, Train Loss: 58.0067880153656, Val Loss: 8.029155135154724\n",
      "Time: 0.04s, CPU: 22.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 41, Train Loss: 57.92739483714104, Val Loss: 7.9927003383636475\n",
      "Time: 0.03s, CPU: 22.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 42, Train Loss: 57.78824618458748, Val Loss: 7.934555411338806\n",
      "Time: 0.04s, CPU: 23.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 43, Train Loss: 57.630381762981415, Val Loss: 7.8276801109313965\n",
      "Time: 0.03s, CPU: 33.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 44, Train Loss: 57.70956629514694, Val Loss: 7.845371961593628\n",
      "Time: 0.04s, CPU: 25.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 45, Train Loss: 57.3852596282959, Val Loss: 7.778114676475525\n",
      "Time: 0.03s, CPU: 14.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 46, Train Loss: 57.15857866406441, Val Loss: 7.7178409695625305\n",
      "Time: 0.03s, CPU: 31.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 47, Train Loss: 57.066317081451416, Val Loss: 7.713670134544373\n",
      "Time: 0.03s, CPU: 25.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 48, Train Loss: 56.86142462491989, Val Loss: 7.626273036003113\n",
      "Time: 0.03s, CPU: 19.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 49, Train Loss: 56.783133536577225, Val Loss: 7.628555595874786\n",
      "Time: 0.04s, CPU: 27.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 50, Train Loss: 56.57537332177162, Val Loss: 7.551385760307312\n",
      "Time: 0.03s, CPU: 28.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 51, Train Loss: 56.485574930906296, Val Loss: 7.542572915554047\n",
      "Time: 0.03s, CPU: 23.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 52, Train Loss: 56.31242626905441, Val Loss: 7.421898990869522\n",
      "Time: 0.03s, CPU: 20.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 53, Train Loss: 56.3501013815403, Val Loss: 7.466223388910294\n",
      "Time: 0.04s, CPU: 20.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 54, Train Loss: 56.01695042848587, Val Loss: 7.3223210871219635\n",
      "Time: 0.02s, CPU: 25.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 55, Train Loss: 55.940154641866684, Val Loss: 7.3315054178237915\n",
      "Time: 0.03s, CPU: 22.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 56, Train Loss: 55.67951852083206, Val Loss: 7.338292747735977\n",
      "Time: 0.03s, CPU: 21.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 57, Train Loss: 55.43118393421173, Val Loss: 7.253133952617645\n",
      "Time: 0.02s, CPU: 18.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 58, Train Loss: 55.38995689153671, Val Loss: 7.313492149114609\n",
      "Time: 0.04s, CPU: 33.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 59, Train Loss: 55.13074907660484, Val Loss: 7.262838184833527\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 60, Train Loss: 55.01733222603798, Val Loss: 7.256030738353729\n",
      "Time: 0.03s, CPU: 25.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 61, Train Loss: 54.85415756702423, Val Loss: 7.202811688184738\n",
      "Time: 0.03s, CPU: 31.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 62, Train Loss: 54.77377735078335, Val Loss: 7.241962105035782\n",
      "Time: 0.04s, CPU: 25.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 63, Train Loss: 54.575669422745705, Val Loss: 7.099915444850922\n",
      "Time: 0.04s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 64, Train Loss: 54.675020679831505, Val Loss: 7.171419560909271\n",
      "Time: 0.04s, CPU: 25.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 65, Train Loss: 54.4381939470768, Val Loss: 7.019955664873123\n",
      "Time: 0.03s, CPU: 26.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 66, Train Loss: 54.537420600652695, Val Loss: 7.177768796682358\n",
      "Time: 0.03s, CPU: 20.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 67, Train Loss: 54.147449880838394, Val Loss: 7.012434750795364\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 68, Train Loss: 54.28727938234806, Val Loss: 7.102067470550537\n",
      "Time: 0.03s, CPU: 19.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 69, Train Loss: 54.068757742643356, Val Loss: 7.070769667625427\n",
      "Time: 0.02s, CPU: 25.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 70, Train Loss: 54.002360343933105, Val Loss: 6.997132748365402\n",
      "Time: 0.03s, CPU: 21.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 71, Train Loss: 54.05959162116051, Val Loss: 7.075098305940628\n",
      "Time: 0.03s, CPU: 27.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 72, Train Loss: 53.81936205923557, Val Loss: 6.8995799124240875\n",
      "Time: 0.03s, CPU: 15.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 73, Train Loss: 53.990938156843185, Val Loss: 7.108370214700699\n",
      "Time: 0.04s, CPU: 44.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 74, Train Loss: 53.53627346456051, Val Loss: 6.91708043217659\n",
      "Time: 0.03s, CPU: 22.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 75, Train Loss: 53.675454661250114, Val Loss: 6.99553906917572\n",
      "Time: 0.04s, CPU: 23.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 76, Train Loss: 53.466388911008835, Val Loss: 6.9663262367248535\n",
      "Time: 0.03s, CPU: 28.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 77, Train Loss: 53.37298287451267, Val Loss: 6.921427845954895\n",
      "Time: 0.03s, CPU: 21.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 78, Train Loss: 53.34106872975826, Val Loss: 7.038139700889587\n",
      "Time: 0.04s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 79, Train Loss: 53.012524873018265, Val Loss: 6.906885355710983\n",
      "Time: 0.03s, CPU: 24.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 80, Train Loss: 53.095823124051094, Val Loss: 7.008081525564194\n",
      "Time: 0.04s, CPU: 23.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 81, Train Loss: 52.86898493766785, Val Loss: 6.905650198459625\n",
      "Time: 0.04s, CPU: 30.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 82, Train Loss: 52.90328897535801, Val Loss: 6.955830305814743\n",
      "Time: 0.04s, CPU: 26.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 83, Train Loss: 52.654893174767494, Val Loss: 6.9625720381736755\n",
      "Time: 0.04s, CPU: 34.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 84, Train Loss: 52.58767466247082, Val Loss: 6.935595721006393\n",
      "Time: 0.04s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 85, Train Loss: 52.555632665753365, Val Loss: 6.943780481815338\n",
      "Time: 0.04s, CPU: 22.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 86, Train Loss: 52.439327254891396, Val Loss: 6.897485554218292\n",
      "Time: 0.03s, CPU: 20.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 87, Train Loss: 52.40767416357994, Val Loss: 6.947689801454544\n",
      "Time: 0.04s, CPU: 17.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 88, Train Loss: 52.24426682293415, Val Loss: 6.859297156333923\n",
      "Time: 0.04s, CPU: 24.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 89, Train Loss: 52.28644445538521, Val Loss: 6.948162764310837\n",
      "Time: 0.04s, CPU: 20.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 90, Train Loss: 52.10773532092571, Val Loss: 6.880515664815903\n",
      "Time: 0.03s, CPU: 36.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 91, Train Loss: 52.072578594088554, Val Loss: 6.887576580047607\n",
      "Time: 0.04s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 92, Train Loss: 51.92413632571697, Val Loss: 6.904897838830948\n",
      "Time: 0.04s, CPU: 24.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 93, Train Loss: 51.792201548814774, Val Loss: 6.8859900534152985\n",
      "Time: 0.03s, CPU: 19.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 94, Train Loss: 51.69011136889458, Val Loss: 6.849118173122406\n",
      "Time: 0.03s, CPU: 24.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 95, Train Loss: 51.664452865719795, Val Loss: 6.912108957767487\n",
      "Time: 0.03s, CPU: 24.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 96, Train Loss: 51.48285545408726, Val Loss: 6.8756403028965\n",
      "Time: 0.04s, CPU: 18.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 97, Train Loss: 51.42009936273098, Val Loss: 6.823980510234833\n",
      "Time: 0.04s, CPU: 32.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 98, Train Loss: 51.36090396344662, Val Loss: 6.875014901161194\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 99, Train Loss: 51.19117288291454, Val Loss: 6.8642619252204895\n",
      "Time: 0.03s, CPU: 29.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 100, Train Loss: 51.07586979866028, Val Loss: 6.8418364226818085\n",
      "Time: 0.03s, CPU: 23.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 101, Train Loss: 51.0281807333231, Val Loss: 6.838172525167465\n",
      "Time: 0.03s, CPU: 23.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 102, Train Loss: 50.93635305762291, Val Loss: 6.87806636095047\n",
      "Time: 0.03s, CPU: 30.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 103, Train Loss: 50.771657437086105, Val Loss: 6.7980073392391205\n",
      "Time: 0.04s, CPU: 28.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 104, Train Loss: 50.78393404185772, Val Loss: 6.823505312204361\n",
      "Time: 0.04s, CPU: 30.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 105, Train Loss: 50.67511370778084, Val Loss: 6.88593864440918\n",
      "Time: 0.04s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 106, Train Loss: 50.485897570848465, Val Loss: 6.815501153469086\n",
      "Time: 0.03s, CPU: 25.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 107, Train Loss: 50.4700483083725, Val Loss: 6.8639905750751495\n",
      "Time: 0.04s, CPU: 29.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 108, Train Loss: 50.30032421648502, Val Loss: 6.766389012336731\n",
      "Time: 0.03s, CPU: 23.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 109, Train Loss: 50.396812841296196, Val Loss: 6.934472769498825\n",
      "Time: 0.04s, CPU: 22.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 110, Train Loss: 50.0122107565403, Val Loss: 6.760335713624954\n",
      "Time: 0.04s, CPU: 24.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 111, Train Loss: 50.12337490916252, Val Loss: 6.864826083183289\n",
      "Time: 0.04s, CPU: 25.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 112, Train Loss: 49.82306472957134, Val Loss: 6.744763255119324\n",
      "Time: 0.03s, CPU: 29.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 113, Train Loss: 49.943102926015854, Val Loss: 6.94573849439621\n",
      "Time: 0.04s, CPU: 20.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 114, Train Loss: 49.48473213613033, Val Loss: 6.790738552808762\n",
      "Time: 0.04s, CPU: 29.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 115, Train Loss: 49.52137492597103, Val Loss: 6.7653509974479675\n",
      "Time: 0.03s, CPU: 21.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 116, Train Loss: 49.40624679625034, Val Loss: 6.5848566591739655\n",
      "Time: 0.03s, CPU: 26.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 117, Train Loss: 49.72860252857208, Val Loss: 6.811350882053375\n",
      "Time: 0.04s, CPU: 20.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 118, Train Loss: 49.23035563528538, Val Loss: 6.766456961631775\n",
      "Time: 0.03s, CPU: 31.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 119, Train Loss: 48.85286474227905, Val Loss: 6.715948730707169\n",
      "Time: 0.03s, CPU: 20.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 120, Train Loss: 48.73330281674862, Val Loss: 6.752826422452927\n",
      "Time: 0.03s, CPU: 22.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 121, Train Loss: 48.38597249984741, Val Loss: 6.6083769500255585\n",
      "Time: 0.03s, CPU: 27.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 122, Train Loss: 48.494755044579506, Val Loss: 6.747303754091263\n",
      "Time: 0.04s, CPU: 32.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 123, Train Loss: 48.102058321237564, Val Loss: 6.645935475826263\n",
      "Time: 0.03s, CPU: 32.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 124, Train Loss: 48.15331853926182, Val Loss: 6.768734157085419\n",
      "Time: 0.03s, CPU: 20.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 125, Train Loss: 47.804005831480026, Val Loss: 6.740382760763168\n",
      "Time: 0.04s, CPU: 24.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 126, Train Loss: 47.59757126867771, Val Loss: 6.553390324115753\n",
      "Time: 0.03s, CPU: 21.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 127, Train Loss: 47.850282430648804, Val Loss: 6.766644269227982\n",
      "Time: 0.03s, CPU: 19.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 128, Train Loss: 47.27781097590923, Val Loss: 6.575708985328674\n",
      "Time: 0.03s, CPU: 26.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 129, Train Loss: 47.46876420080662, Val Loss: 6.715887039899826\n",
      "Time: 0.03s, CPU: 25.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 130, Train Loss: 47.17401513457298, Val Loss: 6.679236888885498\n",
      "Time: 0.03s, CPU: 33.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 131, Train Loss: 47.04044754803181, Val Loss: 6.54623955488205\n",
      "Time: 0.04s, CPU: 18.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 132, Train Loss: 47.234619453549385, Val Loss: 6.791312545537949\n",
      "Time: 0.04s, CPU: 23.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 133, Train Loss: 46.659975960850716, Val Loss: 6.578951328992844\n",
      "Time: 0.03s, CPU: 20.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 134, Train Loss: 46.75255882740021, Val Loss: 6.5870144963264465\n",
      "Time: 0.03s, CPU: 24.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 135, Train Loss: 46.771079659461975, Val Loss: 6.749417334794998\n",
      "Time: 0.02s, CPU: 14.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 136, Train Loss: 46.26446621119976, Val Loss: 6.531306356191635\n",
      "Time: 0.03s, CPU: 11.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 137, Train Loss: 46.53600151836872, Val Loss: 6.747687757015228\n",
      "Time: 0.03s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 138, Train Loss: 46.08743320405483, Val Loss: 6.568077653646469\n",
      "Time: 0.04s, CPU: 32.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 139, Train Loss: 46.23117792606354, Val Loss: 6.652186363935471\n",
      "Time: 0.03s, CPU: 24.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 140, Train Loss: 46.068605437874794, Val Loss: 6.747581362724304\n",
      "Time: 0.03s, CPU: 34.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 141, Train Loss: 45.67023894190788, Val Loss: 6.467442065477371\n",
      "Time: 0.04s, CPU: 53.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 142, Train Loss: 46.108267426490784, Val Loss: 6.704018265008926\n",
      "Time: 0.04s, CPU: 37.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 143, Train Loss: 45.67619903385639, Val Loss: 6.663580387830734\n",
      "Time: 0.04s, CPU: 38.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 144, Train Loss: 45.472824424505234, Val Loss: 6.517179161310196\n",
      "Time: 0.03s, CPU: 47.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 145, Train Loss: 45.706433147192, Val Loss: 6.687898188829422\n",
      "Time: 0.03s, CPU: 45.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 146, Train Loss: 45.378320798277855, Val Loss: 6.578658074140549\n",
      "Time: 0.04s, CPU: 47.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 147, Train Loss: 45.49668212234974, Val Loss: 6.542638689279556\n",
      "Time: 0.04s, CPU: 38.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 148, Train Loss: 45.59352347254753, Val Loss: 6.658552587032318\n",
      "Time: 0.04s, CPU: 39.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 149, Train Loss: 45.253175646066666, Val Loss: 6.558235287666321\n",
      "Time: 0.03s, CPU: 40.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 150, Train Loss: 45.23962776362896, Val Loss: 6.640696227550507\n",
      "Time: 0.04s, CPU: 43.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 151, Train Loss: 44.972322911024094, Val Loss: 6.747890263795853\n",
      "Time: 0.03s, CPU: 63.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 152, Train Loss: 44.70311978459358, Val Loss: 6.556889712810516\n",
      "Time: 0.04s, CPU: 48.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 153, Train Loss: 44.84858758747578, Val Loss: 6.577646881341934\n",
      "Time: 0.04s, CPU: 64.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 154, Train Loss: 44.82772612571716, Val Loss: 6.729629337787628\n",
      "Time: 0.05s, CPU: 57.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.00%\n",
      "Epoch 155, Train Loss: 44.36118225753307, Val Loss: 6.461847424507141\n",
      "Time: 0.04s, CPU: 40.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.00%\n",
      "Epoch 156, Train Loss: 44.79295012354851, Val Loss: 6.671639531850815\n",
      "Time: 0.04s, CPU: 41.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 157, Train Loss: 44.484966829419136, Val Loss: 6.6678473353385925\n",
      "Time: 0.04s, CPU: 17.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 158, Train Loss: 44.16656692326069, Val Loss: 6.43323227763176\n",
      "Time: 0.05s, CPU: 48.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 159, Train Loss: 44.57214018702507, Val Loss: 6.700328439474106\n",
      "Time: 0.03s, CPU: 20.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 160, Train Loss: 44.18651609122753, Val Loss: 6.69287234544754\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 161, Train Loss: 44.02500541508198, Val Loss: 6.493164896965027\n",
      "Time: 0.03s, CPU: 32.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 162, Train Loss: 44.23084236681461, Val Loss: 6.6353800892829895\n",
      "Time: 0.03s, CPU: 23.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 163, Train Loss: 43.9528344720602, Val Loss: 6.568566262722015\n",
      "Time: 0.03s, CPU: 28.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 164, Train Loss: 43.96742321550846, Val Loss: 6.573508679866791\n",
      "Time: 0.03s, CPU: 22.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 165, Train Loss: 43.92986902594566, Val Loss: 6.682737171649933\n",
      "Time: 0.04s, CPU: 24.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 166, Train Loss: 43.64662981033325, Val Loss: 6.536208093166351\n",
      "Time: 0.04s, CPU: 26.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 167, Train Loss: 43.813877671957016, Val Loss: 6.520190387964249\n",
      "Time: 0.04s, CPU: 23.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 168, Train Loss: 43.86961278319359, Val Loss: 6.7580558359622955\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 169, Train Loss: 43.4005536288023, Val Loss: 6.53353750705719\n",
      "Time: 0.04s, CPU: 20.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 170, Train Loss: 43.50203002989292, Val Loss: 6.520388871431351\n",
      "Time: 0.03s, CPU: 21.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 171, Train Loss: 43.753948375582695, Val Loss: 6.850826740264893\n",
      "Time: 0.03s, CPU: 14.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 172, Train Loss: 43.12960374355316, Val Loss: 6.465236395597458\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 173, Train Loss: 43.435581997036934, Val Loss: 6.535371243953705\n",
      "Time: 0.04s, CPU: 20.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 174, Train Loss: 43.484260112047195, Val Loss: 6.730615943670273\n",
      "Time: 0.04s, CPU: 33.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 175, Train Loss: 43.04686050117016, Val Loss: 6.544293612241745\n",
      "Time: 0.03s, CPU: 25.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 176, Train Loss: 43.10504953563213, Val Loss: 6.535751223564148\n",
      "Time: 0.04s, CPU: 22.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 177, Train Loss: 43.26755340397358, Val Loss: 6.713573187589645\n",
      "Time: 0.04s, CPU: 33.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 178, Train Loss: 42.8671490252018, Val Loss: 6.495228409767151\n",
      "Time: 0.04s, CPU: 15.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 179, Train Loss: 43.073313027620316, Val Loss: 6.590612679719925\n",
      "Time: 0.04s, CPU: 22.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 180, Train Loss: 43.04227328300476, Val Loss: 6.703846603631973\n",
      "Time: 0.03s, CPU: 27.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 181, Train Loss: 42.62826953828335, Val Loss: 6.495198905467987\n",
      "Time: 0.03s, CPU: 61.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 182, Train Loss: 42.79375395178795, Val Loss: 6.521301716566086\n",
      "Time: 0.03s, CPU: 22.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 183, Train Loss: 42.973809599876404, Val Loss: 6.659927219152451\n",
      "Time: 0.03s, CPU: 36.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 184, Train Loss: 42.70545303821564, Val Loss: 6.693588048219681\n",
      "Time: 0.03s, CPU: 25.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 185, Train Loss: 42.44600065052509, Val Loss: 6.523097902536392\n",
      "Time: 0.04s, CPU: 42.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 186, Train Loss: 42.59208922088146, Val Loss: 6.538422703742981\n",
      "Time: 0.03s, CPU: 20.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 187, Train Loss: 42.644269451498985, Val Loss: 6.759667843580246\n",
      "Time: 0.04s, CPU: 31.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 188, Train Loss: 42.18352694809437, Val Loss: 6.418202072381973\n",
      "Time: 0.03s, CPU: 25.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 189, Train Loss: 42.54920634627342, Val Loss: 6.623560935258865\n",
      "Time: 0.03s, CPU: 18.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 190, Train Loss: 42.304919853806496, Val Loss: 6.747005134820938\n",
      "Time: 0.03s, CPU: 28.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 191, Train Loss: 41.91477929055691, Val Loss: 6.277248412370682\n",
      "Time: 0.04s, CPU: 26.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 192, Train Loss: 42.63039776682854, Val Loss: 6.664174050092697\n",
      "Time: 0.03s, CPU: 21.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 193, Train Loss: 42.23707976937294, Val Loss: 6.730496138334274\n",
      "Time: 0.03s, CPU: 27.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 194, Train Loss: 41.812477961182594, Val Loss: 6.389544457197189\n",
      "Time: 0.03s, CPU: 31.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 195, Train Loss: 42.21381786465645, Val Loss: 6.670845150947571\n",
      "Time: 0.03s, CPU: 20.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 196, Train Loss: 41.917716175317764, Val Loss: 6.618066430091858\n",
      "Time: 0.03s, CPU: 17.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 197, Train Loss: 41.74732868373394, Val Loss: 6.4340803027153015\n",
      "Time: 0.03s, CPU: 45.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 198, Train Loss: 41.97397734224796, Val Loss: 6.5895697474479675\n",
      "Time: 0.03s, CPU: 23.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 199, Train Loss: 41.806756034493446, Val Loss: 6.634562015533447\n",
      "Time: 0.03s, CPU: 31.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 200, Train Loss: 41.61652532219887, Val Loss: 6.4393821358680725\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 201, Train Loss: 41.642759785056114, Val Loss: 6.501621454954147\n",
      "Time: 0.04s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 202, Train Loss: 41.55708381533623, Val Loss: 6.460826396942139\n",
      "Time: 0.03s, CPU: 31.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 203, Train Loss: 41.6630097925663, Val Loss: 6.637133806943893\n",
      "Time: 0.03s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 204, Train Loss: 41.29718114435673, Val Loss: 6.348048895597458\n",
      "Time: 0.03s, CPU: 29.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 205, Train Loss: 41.516621962189674, Val Loss: 6.436190754175186\n",
      "Time: 0.03s, CPU: 18.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 206, Train Loss: 41.53290590643883, Val Loss: 6.606740802526474\n",
      "Time: 0.02s, CPU: 21.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 207, Train Loss: 41.03023383021355, Val Loss: 6.294885724782944\n",
      "Time: 0.03s, CPU: 27.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 208, Train Loss: 41.3557395786047, Val Loss: 6.399355083703995\n",
      "Time: 0.03s, CPU: 21.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 209, Train Loss: 41.37551100552082, Val Loss: 6.65472686290741\n",
      "Time: 0.03s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 210, Train Loss: 40.87151303887367, Val Loss: 6.2738339602947235\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 211, Train Loss: 41.141165018081665, Val Loss: 6.4028750360012054\n",
      "Time: 0.03s, CPU: 17.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 212, Train Loss: 41.01748068630695, Val Loss: 6.452427059412003\n",
      "Time: 0.02s, CPU: 30.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 213, Train Loss: 40.697567731142044, Val Loss: 6.256855130195618\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 214, Train Loss: 41.23273992538452, Val Loss: 6.479940265417099\n",
      "Time: 0.03s, CPU: 24.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 215, Train Loss: 40.78424574434757, Val Loss: 6.35993018746376\n",
      "Time: 0.03s, CPU: 29.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 216, Train Loss: 40.63272511959076, Val Loss: 6.3002461194992065\n",
      "Time: 0.04s, CPU: 19.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 217, Train Loss: 40.60986329615116, Val Loss: 6.327235847711563\n",
      "Time: 0.04s, CPU: 29.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 218, Train Loss: 40.570738792419434, Val Loss: 6.3926683366298676\n",
      "Time: 0.03s, CPU: 21.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 219, Train Loss: 40.39045824110508, Val Loss: 6.193132996559143\n",
      "Time: 0.03s, CPU: 23.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 220, Train Loss: 40.462815538048744, Val Loss: 6.2983909249305725\n",
      "Time: 0.03s, CPU: 25.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 221, Train Loss: 40.35648135840893, Val Loss: 6.307251155376434\n",
      "Time: 0.04s, CPU: 23.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 222, Train Loss: 40.15352027118206, Val Loss: 6.229061633348465\n",
      "Time: 0.03s, CPU: 25.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 223, Train Loss: 40.16305109858513, Val Loss: 6.148201525211334\n",
      "Time: 0.03s, CPU: 19.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 224, Train Loss: 40.3310379832983, Val Loss: 6.319472640752792\n",
      "Time: 0.04s, CPU: 22.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 225, Train Loss: 40.034737944602966, Val Loss: 6.237053722143173\n",
      "Time: 0.03s, CPU: 31.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 226, Train Loss: 39.85728184878826, Val Loss: 6.171372681856155\n",
      "Time: 0.04s, CPU: 20.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 227, Train Loss: 39.83031080663204, Val Loss: 6.185793578624725\n",
      "Time: 0.03s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 228, Train Loss: 39.8295718729496, Val Loss: 6.1442676186561584\n",
      "Time: 0.04s, CPU: 29.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 229, Train Loss: 39.74769584834576, Val Loss: 6.089607328176498\n",
      "Time: 0.04s, CPU: 32.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.50%\n",
      "Epoch 230, Train Loss: 39.70687600970268, Val Loss: 6.182331293821335\n",
      "Time: 0.04s, CPU: 38.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 231, Train Loss: 39.54233121871948, Val Loss: 6.132792681455612\n",
      "Time: 0.03s, CPU: 22.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 232, Train Loss: 39.487850561738014, Val Loss: 6.035066395998001\n",
      "Time: 0.03s, CPU: 24.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 233, Train Loss: 39.52650384604931, Val Loss: 6.084095388650894\n",
      "Time: 0.04s, CPU: 24.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 234, Train Loss: 39.56300914287567, Val Loss: 6.203411221504211\n",
      "Time: 0.04s, CPU: 32.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 235, Train Loss: 39.278289794921875, Val Loss: 5.994358509778976\n",
      "Time: 0.04s, CPU: 35.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 236, Train Loss: 39.23060467839241, Val Loss: 5.989785343408585\n",
      "Time: 0.03s, CPU: 20.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 237, Train Loss: 39.41762188076973, Val Loss: 6.208099722862244\n",
      "Time: 0.04s, CPU: 21.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 238, Train Loss: 39.12316916882992, Val Loss: 6.091842502355576\n",
      "Time: 0.04s, CPU: 29.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 239, Train Loss: 38.9793281853199, Val Loss: 5.971241891384125\n",
      "Time: 0.03s, CPU: 21.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 240, Train Loss: 39.072284907102585, Val Loss: 5.906659662723541\n",
      "Time: 0.03s, CPU: 25.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 241, Train Loss: 39.4036932438612, Val Loss: 6.2445129454135895\n",
      "Time: 0.03s, CPU: 20.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 242, Train Loss: 38.90029864013195, Val Loss: 6.082117259502411\n",
      "Time: 0.03s, CPU: 29.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 243, Train Loss: 38.805034801363945, Val Loss: 5.979059636592865\n",
      "Time: 0.03s, CPU: 19.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 244, Train Loss: 38.940756648778915, Val Loss: 6.037943065166473\n",
      "Time: 0.03s, CPU: 26.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 245, Train Loss: 38.78707882761955, Val Loss: 5.935344994068146\n",
      "Time: 0.03s, CPU: 28.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 246, Train Loss: 39.0238985568285, Val Loss: 6.195657402276993\n",
      "Time: 0.04s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 247, Train Loss: 38.6981171220541, Val Loss: 6.034777611494064\n",
      "Time: 0.03s, CPU: 24.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 248, Train Loss: 38.54443380236626, Val Loss: 5.97026601433754\n",
      "Time: 0.05s, CPU: 25.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 249, Train Loss: 38.71301767230034, Val Loss: 5.932788848876953\n",
      "Time: 0.03s, CPU: 28.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 250, Train Loss: 38.92538832128048, Val Loss: 6.301949322223663\n",
      "Time: 0.03s, CPU: 15.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 251, Train Loss: 38.327951684594154, Val Loss: 5.9244704246521\n",
      "Time: 0.03s, CPU: 29.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 252, Train Loss: 38.43223997950554, Val Loss: 5.833966881036758\n",
      "Time: 0.04s, CPU: 20.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 253, Train Loss: 38.87969043850899, Val Loss: 6.146498322486877\n",
      "Time: 0.04s, CPU: 36.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 254, Train Loss: 38.4085439145565, Val Loss: 6.12487256526947\n",
      "Time: 0.03s, CPU: 35.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 255, Train Loss: 38.320520386099815, Val Loss: 5.91052383184433\n",
      "Time: 0.04s, CPU: 42.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 256, Train Loss: 38.52634820342064, Val Loss: 6.037062853574753\n",
      "Time: 0.04s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 257, Train Loss: 38.35284362733364, Val Loss: 6.052332669496536\n",
      "Time: 0.05s, CPU: 41.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 258, Train Loss: 38.125709518790245, Val Loss: 5.919360816478729\n",
      "Time: 0.04s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 259, Train Loss: 38.45616842806339, Val Loss: 6.103608012199402\n",
      "Time: 0.03s, CPU: 23.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 260, Train Loss: 38.186572670936584, Val Loss: 5.996686667203903\n",
      "Time: 0.03s, CPU: 12.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 261, Train Loss: 37.972723349928856, Val Loss: 5.934411138296127\n",
      "Time: 0.03s, CPU: 32.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 262, Train Loss: 38.23533008992672, Val Loss: 6.119747757911682\n",
      "Time: 0.04s, CPU: 21.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 263, Train Loss: 38.000929564237595, Val Loss: 5.944911539554596\n",
      "Time: 0.04s, CPU: 30.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 264, Train Loss: 38.160815358161926, Val Loss: 5.942143946886063\n",
      "Time: 0.03s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 265, Train Loss: 38.17274233698845, Val Loss: 6.150871217250824\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 266, Train Loss: 37.74675543606281, Val Loss: 5.918285250663757\n",
      "Time: 0.04s, CPU: 37.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 267, Train Loss: 37.726370722055435, Val Loss: 5.797263085842133\n",
      "Time: 0.03s, CPU: 28.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 268, Train Loss: 38.237425953149796, Val Loss: 6.285122483968735\n",
      "Time: 0.04s, CPU: 37.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 269, Train Loss: 37.62618537247181, Val Loss: 5.978773534297943\n",
      "Time: 0.03s, CPU: 39.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 270, Train Loss: 37.54607217013836, Val Loss: 5.744182616472244\n",
      "Time: 0.04s, CPU: 37.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 271, Train Loss: 38.068069860339165, Val Loss: 6.049236953258514\n",
      "Time: 0.04s, CPU: 63.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 272, Train Loss: 37.96792924404144, Val Loss: 6.308491677045822\n",
      "Time: 0.03s, CPU: 45.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 273, Train Loss: 37.4115392267704, Val Loss: 5.780728608369827\n",
      "Time: 0.04s, CPU: 37.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 274, Train Loss: 37.97527155280113, Val Loss: 6.103918254375458\n",
      "Time: 0.04s, CPU: 34.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 275, Train Loss: 38.03524361550808, Val Loss: 6.13456204533577\n",
      "Time: 0.04s, CPU: 38.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 276, Train Loss: 37.631486520171165, Val Loss: 5.8776529133319855\n",
      "Time: 0.04s, CPU: 10.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 277, Train Loss: 37.660746052861214, Val Loss: 6.083664894104004\n",
      "Time: 0.03s, CPU: 18.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Early stopping at epoch 278\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzXklEQVR4nO3dd1wT9/8H8FcSQhghYS8ZAqKg4kJF3IOKo35dXda2aq1Wi7Za7fDbulpb++3+aat2+FU71NZ+Xa1b62gV91ZEUQSUpawwAyT3++MgNuJAVgi8no9HHpq7y937zuC9+Hw+dycRBEEAERERkRmSmroAIiIioqpikCEiIiKzxSBDREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdlikCGqJWPHjkXTpk2r9Nl58+ZBIpHUbEH1zPXr1yGRSLBy5co637ZEIsG8efMM71euXAmJRILr168/9LNNmzbF2LFja7Se6nxXiBo7BhlqdCQSSaVe+/btM3Wpjd6rr74KiUSCuLi4+y7zzjvvQCKR4OzZs3VY2aNLTk7GvHnzcPr0aVOXYlAeJj/99FNTl0JUZRamLoCorv34449G73/44Qfs2rWrwvTg4OBqbee7776DXq+v0mffffddvP3229XafkMwevRoLF68GKtXr8acOXPuucyaNWsQEhKCNm3aVHk7zz//PJ555hkoFIoqr+NhkpOTMX/+fDRt2hTt2rUzmled7wpRY8cgQ43Oc889Z/T+8OHD2LVrV4XpdysoKICNjU2ltyOXy6tUHwBYWFjAwoI/nmFhYWjWrBnWrFlzzyATHR2N+Ph4fPTRR9Xajkwmg0wmq9Y6qqM63xWixo5dS0T30Lt3b7Ru3RonTpxAz549YWNjg3//+98AgE2bNmHw4MHw9PSEQqFAQEAA3n//feh0OqN13D3u4Z/N+N9++y0CAgKgUCjQqVMnHDt2zOiz9xojI5FIMGXKFGzcuBGtW7eGQqFAq1atsH379gr179u3Dx07doSVlRUCAgLwzTffVHrczV9//YUnn3wSPj4+UCgU8Pb2xvTp01FYWFhh/5RKJW7evIlhw4ZBqVTCxcUFM2fOrHAssrOzMXbsWKjVatjb22PMmDHIzs5+aC2A2Cpz6dIlnDx5ssK81atXQyKRYNSoUSguLsacOXMQGhoKtVoNW1tb9OjRA3v37n3oNu41RkYQBCxYsABeXl6wsbFBnz59cOHChQqfzczMxMyZMxESEgKlUgmVSoWBAwfizJkzhmX27duHTp06AQDGjRtn6L4sHx90rzEy+fn5mDFjBry9vaFQKNCiRQt8+umnEATBaLlH+V5UVXp6OsaPHw83NzdYWVmhbdu2WLVqVYXl1q5di9DQUNjZ2UGlUiEkJAT/93//Z5hfUlKC+fPnIzAwEFZWVnByckL37t2xa9euGquVGh/+ykd0HxkZGRg4cCCeeeYZPPfcc3BzcwMgnvSUSiVef/11KJVK/Pnnn5gzZw40Gg0++eSTh6539erVyM3NxcsvvwyJRIKPP/4YI0aMwLVr1x76m/nff/+N9evX45VXXoGdnR0WLVqEkSNHIjExEU5OTgCAU6dOYcCAAfDw8MD8+fOh0+nw3nvvwcXFpVL7vW7dOhQUFGDy5MlwcnLC0aNHsXjxYty4cQPr1q0zWlan0yEyMhJhYWH49NNPsXv3bnz22WcICAjA5MmTAYiBYOjQofj7778xadIkBAcHY8OGDRgzZkyl6hk9ejTmz5+P1atXo0OHDkbb/vXXX9GjRw/4+Pjg9u3b+P777zFq1ChMmDABubm5WL58OSIjI3H06NEK3TkPM2fOHCxYsACDBg3CoEGDcPLkSfTv3x/FxcVGy127dg0bN27Ek08+CT8/P6SlpeGbb75Br169cPHiRXh6eiI4OBjvvfce5syZg4kTJ6JHjx4AgK5du95z24Ig4F//+hf27t2L8ePHo127dtixYwfeeOMN3Lx5E1988YXR8pX5XlRVYWEhevfujbi4OEyZMgV+fn5Yt24dxo4di+zsbLz22msAgF27dmHUqFHo168f/vOf/wAAYmJicPDgQcMy8+bNw8KFC/HSSy+hc+fO0Gg0OH78OE6ePInHHnusWnVSIyYQNXJRUVHC3T8KvXr1EgAIy5Ytq7B8QUFBhWkvv/yyYGNjIxQVFRmmjRkzRvD19TW8j4+PFwAITk5OQmZmpmH6pk2bBADC77//bpg2d+7cCjUBECwtLYW4uDjDtDNnzggAhMWLFxumDRkyRLCxsRFu3rxpmHblyhXBwsKiwjrv5V77t3DhQkEikQgJCQlG+wdAeO+994yWbd++vRAaGmp4v3HjRgGA8PHHHxumlZaWCj169BAACCtWrHhoTZ06dRK8vLwEnU5nmLZ9+3YBgPDNN98Y1qnVao0+l5WVJbi5uQkvvvii0XQAwty5cw3vV6xYIQAQ4uPjBUEQhPT0dMHS0lIYPHiwoNfrDcv9+9//FgAIY8aMMUwrKioyqksQxH9rhUJhdGyOHTt23/29+7tSfswWLFhgtNwTTzwhSCQSo+9AZb8X91L+nfzkk0/uu8yXX34pABB++uknw7Ti4mIhPDxcUCqVgkajEQRBEF577TVBpVIJpaWl911X27ZthcGDBz+wJqJHxa4lovtQKBQYN25chenW1taGv+fm5uL27dvo0aMHCgoKcOnSpYeu9+mnn4aDg4Phfflv59euXXvoZyMiIhAQEGB436ZNG6hUKsNndToddu/ejWHDhsHT09OwXLNmzTBw4MCHrh8w3r/8/Hzcvn0bXbt2hSAIOHXqVIXlJ02aZPS+R48eRvuydetWWFhYGFpoAHFMytSpUytVDyCOa7px4wYOHDhgmLZ69WpYWlriySefNKzT0tISAKDX65GZmYnS0lJ07Njxnt1SD7J7924UFxdj6tSpRt1x06ZNq7CsQqGAVCr+V6rT6ZCRkQGlUokWLVo88nbLbd26FTKZDK+++qrR9BkzZkAQBGzbts1o+sO+F9WxdetWuLu7Y9SoUYZpcrkcr776KvLy8rB//34AgL29PfLz8x/YTWRvb48LFy7gypUr1a6LqByDDNF9NGnSxHBi/KcLFy5g+PDhUKvVUKlUcHFxMQwUzsnJeeh6fXx8jN6Xh5qsrKxH/mz558s/m56ejsLCQjRr1qzCcveadi+JiYkYO3YsHB0dDeNeevXqBaDi/llZWVXosvpnPQCQkJAADw8PKJVKo+VatGhRqXoA4JlnnoFMJsPq1asBAEVFRdiwYQMGDhxoFApXrVqFNm3aGMZfuLi4YMuWLZX6d/mnhIQEAEBgYKDRdBcXF6PtAWJo+uKLLxAYGAiFQgFnZ2e4uLjg7Nmzj7zdf27f09MTdnZ2RtPLr6Qrr6/cw74X1ZGQkIDAwEBDWLtfLa+88gqaN2+OgQMHwsvLCy+++GKFcTrvvfcesrOz0bx5c4SEhOCNN96o95fNU/3HIEN0H/9smSiXnZ2NXr164cyZM3jvvffw+++/Y9euXYYxAZW5hPZ+V8cIdw3irOnPVoZOp8Njjz2GLVu24K233sLGjRuxa9cuw6DUu/evrq70cXV1xWOPPYb//e9/KCkpwe+//47c3FyMHj3asMxPP/2EsWPHIiAgAMuXL8f27duxa9cu9O3bt1Yvbf7www/x+uuvo2fPnvjpp5+wY8cO7Nq1C61ataqzS6pr+3tRGa6urjh9+jQ2b95sGN8zcOBAo7FQPXv2xNWrV/Hf//4XrVu3xvfff48OHTrg+++/r7M6qeHhYF+iR7Bv3z5kZGRg/fr16Nmzp2F6fHy8Cau6w9XVFVZWVve8gdyDbipX7ty5c7h8+TJWrVqFF154wTC9OleV+Pr6Ys+ePcjLyzNqlYmNjX2k9YwePRrbt2/Htm3bsHr1aqhUKgwZMsQw/7fffoO/vz/Wr19v1B00d+7cKtUMAFeuXIG/v79h+q1btyq0cvz222/o06cPli9fbjQ9Ozsbzs7OhvePcqdmX19f7N69G7m5uUatMuVdl+X11QVfX1+cPXsWer3eqFXmXrVYWlpiyJAhGDJkCPR6PV555RV88803mD17tqFF0NHREePGjcO4ceOQl5eHnj17Yt68eXjppZfqbJ+oYWGLDNEjKP/N95+/6RYXF2PJkiWmKsmITCZDREQENm7ciOTkZMP0uLi4CuMq7vd5wHj/BEEwuoT2UQ0aNAilpaVYunSpYZpOp8PixYsfaT3Dhg2DjY0NlixZgm3btmHEiBGwsrJ6YO1HjhxBdHT0I9ccEREBuVyOxYsXG63vyy+/rLCsTCar0PKxbt063Lx502iara0tAFTqsvNBgwZBp9Phq6++Mpr+xRdfQCKRVHq8U00YNGgQUlNT8csvvximlZaWYvHixVAqlYZux4yMDKPPSaVSw00KtVrtPZdRKpVo1qyZYT5RVbBFhugRdO3aFQ4ODhgzZozh9vk//vhjnTbhP8y8efOwc+dOdOvWDZMnTzacEFu3bv3Q2+MHBQUhICAAM2fOxM2bN6FSqfC///2vWmMthgwZgm7duuHtt9/G9evX0bJlS6xfv/6Rx48olUoMGzbMME7mn91KAPD4449j/fr1GD58OAYPHoz4+HgsW7YMLVu2RF5e3iNtq/x+OAsXLsTjjz+OQYMG4dSpU9i2bZtRK0v5dt977z2MGzcOXbt2xblz5/Dzzz8bteQAQEBAAOzt7bFs2TLY2dnB1tYWYWFh8PPzq7D9IUOGoE+fPnjnnXdw/fp1tG3bFjt37sSmTZswbdo0o4G9NWHPnj0oKiqqMH3YsGGYOHEivvnmG4wdOxYnTpxA06ZN8dtvv+HgwYP48ssvDS1GL730EjIzM9G3b194eXkhISEBixcvRrt27QzjaVq2bInevXsjNDQUjo6OOH78OH777TdMmTKlRveHGhnTXCxFVH/c7/LrVq1a3XP5gwcPCl26dBGsra0FT09P4c033xR27NghABD27t1rWO5+l1/f61JX3HU58P0uv46KiqrwWV9fX6PLgQVBEPbs2SO0b99esLS0FAICAoTvv/9emDFjhmBlZXWfo3DHxYsXhYiICEGpVArOzs7ChAkTDJfz/vPS4TFjxgi2trYVPn+v2jMyMoTnn39eUKlUglqtFp5//nnh1KlTlb78utyWLVsEAIKHh0eFS571er3w4YcfCr6+voJCoRDat28v/PHHHxX+HQTh4ZdfC4Ig6HQ6Yf78+YKHh4dgbW0t9O7dWzh//nyF411UVCTMmDHDsFy3bt2E6OhooVevXkKvXr2Mtrtp0yahZcuWhkvhy/f9XjXm5uYK06dPFzw9PQW5XC4EBgYKn3zyidHl4OX7Utnvxd3Kv5P3e/3444+CIAhCWlqaMG7cOMHZ2VmwtLQUQkJCKvy7/fbbb0L//v0FV1dXwdLSUvDx8RFefvllISUlxbDMggULhM6dOwv29vaCtbW1EBQUJHzwwQdCcXHxA+skehCJINSjXyWJqNYMGzaMl74SUYPDMTJEDdDdjxO4cuUKtm7dit69e5umICKiWsIWGaIGyMPDA2PHjoW/vz8SEhKwdOlSaLVanDp1qsK9UYiIzBkH+xI1QAMGDMCaNWuQmpoKhUKB8PBwfPjhhwwxRNTgsEWGiIiIzBbHyBAREZHZMmmQyc3NxbRp0+Dr6wtra2t07doVx44dM8wXBAFz5syBh4cHrK2tERERwSsuiIiIyMCkY2ReeuklnD9/Hj/++CM8PT3x008/ISIiAhcvXkSTJk3w8ccfY9GiRVi1ahX8/Pwwe/ZsREZG4uLFi0Z39HwQvV6P5ORk2NnZPdItwomIiMh0BEFAbm4uPD09Kzy09O4FTaKgoECQyWTCH3/8YTS9Q4cOwjvvvCPo9XrB3d3d6OZh2dnZgkKhENasWVPp7SQlJT3whk988cUXX3zxxVf9fSUlJT3wPG+yFpnS0lLodLoKLSvW1tb4+++/ER8fj9TUVERERBjmqdVqhIWFITo6Gs8888w916vVao2e2yGUjWVOSkqCSqWqhT0hIiKimqbRaODt7W304NR7MVmQsbOzQ3h4ON5//30EBwfDzc0Na9asQXR0NJo1a4bU1FQAgJubm9Hn3NzcDPPuZeHChZg/f36F6SqVikGGiIjIzDxsWIhJB/uWP2yvSZMmUCgUWLRoEUaNGvXgvrCHmDVrFnJycgyvpKSkGqyYiIiI6hOTBpmAgADs378feXl5SEpKwtGjR1FSUgJ/f3+4u7sDANLS0ow+k5aWZph3LwqFwtD6wlYYIiKihq1e3EfG1tYWHh4eyMrKwo4dOzB06FD4+fnB3d0de/bsMSyn0Whw5MgRhIeHm7BaIiIiqi9Mevn1jh07IAgCWrRogbi4OLzxxhsICgrCuHHjIJFIMG3aNCxYsACBgYGGy689PT0xbNgwU5ZNRNSo6HQ6lJSUmLoMamDkcjlkMlm112PSIJOTk4NZs2bhxo0bcHR0xMiRI/HBBx9ALpcDAN58803k5+dj4sSJyM7ORvfu3bF9+/ZK30OGiIiqThAEpKamIjs729SlUANlb28Pd3f3at3nrcE/a0mj0UCtViMnJ4fjZYiIHkFKSgqys7Ph6uoKGxsb3lSUaowgCCgoKEB6ejrs7e3h4eFRYZnKnr/59GsiIqpAp9MZQoyTk5Opy6EGyNraGgCQnp4OV1fXKncz1YvBvkREVL+Uj4mxsbExcSXUkJV/v6ozBotBhoiI7ovdSVSbauL7xSBDREREZotBhoiI6CGaNm2KL7/8stLL79u3DxKJhFd81QEGGSIiajAkEskDX/PmzavSeo8dO4aJEydWevmuXbsiJSUFarW6SturLAYmXrVUZfnaUmTmF0OpsICDraWpyyEiIoiXjJf75ZdfMGfOHMTGxhqmKZVKw98FQYBOp4OFxcNPhS4uLo9Uh6Wl5QMfp0M1hy0yVTR703n0+HgvfjnOh1ISEdUX7u7uhpdarYZEIjG8v3TpEuzs7LBt2zaEhoZCoVDg77//xtWrVzF06FC4ublBqVSiU6dO2L17t9F67+5akkgk+P777zF8+HDY2NggMDAQmzdvNsy/u6Vk5cqVsLe3x44dOxAcHAylUokBAwYYBa/S0lK8+uqrsLe3h5OTE9566y2MGTOmWnezz8rKwgsvvAAHBwfY2Nhg4MCBuHLlimF+QkIChgwZAgcHB9ja2qJVq1bYunWr4bOjR4+Gi4sLrK2tERgYiBUrVlS5ltrCIFNF9tZiK0x2AW/bTUSNgyAIKCguNcmrJu/d+vbbb+Ojjz5CTEwM2rRpg7y8PAwaNAh79uzBqVOnMGDAAAwZMgSJiYkPXM/8+fPx1FNP4ezZsxg0aBBGjx6NzMzM+y5fUFCATz/9FD/++CMOHDiAxMREzJw50zD/P//5D37++WesWLECBw8ehEajwcaNG6u1r2PHjsXx48exefNmREdHQxAEDBo0yHC5c1RUFLRaLQ4cOIBz587hP//5j6HVavbs2bh48SK2bduGmJgYLF26FM7OztWqpzawa6mK7G3ExyjkFBabuBIiorpRWKJDyzk7TLLti+9FwsayZk5Z7733Hh577DHDe0dHR7Rt29bw/v3338eGDRuwefNmTJky5b7rGTt2LEaNGgUA+PDDD7Fo0SIcPXoUAwYMuOfyJSUlWLZsGQICAgAAU6ZMwXvvvWeYv3jxYsyaNQvDhw8HAHz11VeG1pGquHLlCjZv3oyDBw+ia9euAICff/4Z3t7e2LhxI5588kkkJiZi5MiRCAkJAQD4+/sbPp+YmIj27dujY8eOAMRWqfqILTJVdCfIsEWGiMiclJ+Yy+Xl5WHmzJkIDg6Gvb09lEolYmJiHtoi06ZNG8PfbW1toVKpkJ6eft/lbWxsDCEGADw8PAzL5+TkIC0tDZ07dzbMl8lkCA0NfaR9+6eYmBhYWFggLCzMMM3JyQktWrRATEwMAODVV1/FggUL0K1bN8ydOxdnz541LDt58mSsXbsW7dq1w5tvvolDhw5VuZbaxBaZKlJbi0GGXUtE1FhYy2W4+F6kybZdU2xtbY3ez5w5E7t27cKnn36KZs2awdraGk888QSKix/c4l7+gONyEokEer3+kZY39eMOX3rpJURGRmLLli3YuXMnFi5ciM8++wxTp07FwIEDkZCQgK1bt2LXrl3o168foqKi8Omnn5q05ruxRaaKGGSIqLGRSCSwsbQwyas27zB88OBBjB07FsOHD0dISAjc3d1x/fr1WtvevajVari5ueHYsWOGaTqdDidPnqzyOoODg1FaWoojR44YpmVkZCA2NhYtW7Y0TPP29sakSZOwfv16zJgxA999951hnouLC8aMGYOffvoJX375Jb799tsq11Nb2CJTRfY24mBfdi0REZm3wMBArF+/HkOGDIFEIsHs2bMf2LJSW6ZOnYqFCxeiWbNmCAoKwuLFi5GVlVWpEHfu3DnY2dkZ3kskErRt2xZDhw7FhAkT8M0338DOzg5vv/02mjRpgqFDhwIApk2bhoEDB6J58+bIysrC3r17ERwcDACYM2cOQkND0apVK2i1Wvzxxx+GefUJg0wV2RtaZDjYl4jInH3++ed48cUX0bVrVzg7O+Ott96CRqOp8zreeustpKam4oUXXoBMJsPEiRMRGRlZqadC9+zZ0+i9TCZDaWkpVqxYgddeew2PP/44iouL0bNnT2zdutXQzaXT6RAVFYUbN25ApVJhwIAB+OKLLwCI98KZNWsWrl+/Dmtra/To0QNr166t+R2vJolg6g66WqbRaKBWq5GTkwOVSlVj680uKEa793YBAK58MBByGXvpiKjhKCoqQnx8PPz8/GBlZWXqcholvV6P4OBgPPXUU3j//fdNXU6teND3rLLnb7bIVJGdlRwSCSAIYveSs1Jh6pKIiMiMJSQkYOfOnejVqxe0Wi2++uorxMfH49lnnzV1afUamxGqSCaVQGXFAb9ERFQzpFIpVq5ciU6dOqFbt244d+4cdu/eXS/HpdQnbJGpBrW1HDmFJbwpHhERVZu3tzcOHjxo6jLMDltkqqH8pnhskSEiIjINBplq4L1kiIiITItBphrK7yWTzXvJEBERmQSDTDWU30smh/eSISIiMgkGmWrggyOJiIhMi0GmGgxjZBhkiIiITIJBpho42JeIqGHq3bs3pk2bZnjftGlTfPnllw/8jEQiwcaNG6u97ZpaT2PBIFMNHOxLRFS/DBkyBAMGDLjnvL/++gsSiQRnz5595PUeO3YMEydOrG55RubNm4d27dpVmJ6SkoKBAwfW6LbutnLlStjb29fqNuoKg0w1GMbIcLAvEVG9MH78eOzatQs3btyoMG/FihXo2LEj2rRp88jrdXFxgY2NTU2U+FDu7u5QKPjYm8pikKkGe46RISKqVx5//HG4uLhg5cqVRtPz8vKwbt06jB8/HhkZGRg1ahSaNGkCGxsbhISEYM2aNQ9c791dS1euXEHPnj1hZWWFli1bYteuXRU+89Zbb6F58+awsbGBv78/Zs+ejZIS8XyxcuVKzJ8/H2fOnIFEIoFEIjHUfHfX0rlz59C3b19YW1vDyckJEydORF5enmH+2LFjMWzYMHz66afw8PCAk5MToqKiDNuqisTERAwdOhRKpRIqlQpPPfUU0tLSDPPPnDmDPn36wM7ODiqVCqGhoTh+/DgA8ZlRQ4YMgYODA2xtbdGqVSts3bq1yrU8DB9RUA3qshYZTWEJ9HoBUqnExBUREdUiQQBKCkyzbbkNIHn4/7EWFhZ44YUXsHLlSrzzzjuQlH1m3bp10Ol0GDVqFPLy8hAaGoq33noLKpUKW7ZswfPPP4+AgAB07tz5odvQ6/UYMWIE3NzccOTIEeTk5BiNpylnZ2eHlStXwtPTE+fOncOECRNgZ2eHN998E08//TTOnz+P7du3Y/fu3QAAtVpdYR35+fmIjIxEeHg4jh07hvT0dLz00kuYMmWKUVjbu3cvPDw8sHfvXsTFxeHpp59Gu3btMGHChIfuz732rzzE7N+/H6WlpYiKisLTTz+Nffv2AQBGjx6N9u3bY+nSpZDJZDh9+jTkcvGcGBUVheLiYhw4cAC2tra4ePEilErlI9dRWQwy1VA+2FcvALnaUsN7IqIGqaQA+NDTNNv+dzJgaVupRV988UV88skn2L9/P3r37g1A7FYaOXIk1Go11Go1Zs6caVh+6tSp2LFjB3799ddKBZndu3fj0qVL2LFjBzw9xePx4YcfVhjX8u677xr+3rRpU8ycORNr167Fm2++CWtrayiVSlhYWMDd3f2+21q9ejWKiorwww8/wNZW3P+vvvoKQ4YMwX/+8x+4ubkBABwcHPDVV19BJpMhKCgIgwcPxp49e6oUZPbs2YNz584hPj4e3t7eAIAffvgBrVq1wrFjx9CpUyckJibijTfeQFBQEAAgMDDQ8PnExESMHDkSISEhAAB/f/9HruFRmLRrSafTYfbs2fDz84O1tTUCAgLw/vvvQxAEwzKCIGDOnDnw8PCAtbU1IiIicOXKFRNWfYfCQgZnpTjg98i1DBNXQ0REABAUFISuXbviv//9LwAgLi4Of/31F8aPHw9APPe8//77CAkJgaOjI5RKJXbs2IHExMRKrT8mJgbe3t6GEAMA4eHhFZb75Zdf0K1bN7i7u0OpVOLdd9+t9Db+ua22bdsaQgwAdOvWDXq9HrGxsYZprVq1gkwmM7z38PBAenr6I23rn9v09vY2hBgAaNmyJezt7RETEwMAeP311/HSSy8hIiICH330Ea5evWpY9tVXX8WCBQvQrVs3zJ07t0qDqx+FSVtk/vOf/2Dp0qVYtWoVWrVqhePHj2PcuHFQq9V49dVXAQAff/wxFi1ahFWrVsHPzw+zZ89GZGQkLl68CCsrK1OWDwAYGeqFb/Zfw6ro6+jf6v6pmojI7MltxJYRU237EYwfPx5Tp07F119/jRUrViAgIAC9evUCAHzyySf4v//7P3z55ZcICQmBra0tpk2bhuLimrtwIzo6GqNHj8b8+fMRGRkJtVqNtWvX4rPPPquxbfxTebdOOYlEAr1eXyvbAsQrrp599lls2bIF27Ztw9y5c7F27VoMHz4cL730EiIjI7Flyxbs3LkTCxcuxGeffYapU6fWSi0mbZE5dOgQhg4disGDB6Np06Z44okn0L9/fxw9ehSA2Brz5Zdf4t1338XQoUPRpk0b/PDDD0hOTq4319g/38UXUglwMC4DV9JyTV0OEVHtkUjE7h1TvCoxPuafnnrqKUilUqxevRo//PADXnzxRcN4mYMHD2Lo0KF47rnn0LZtW/j7++Py5cuVXndwcDCSkpKQkpJimHb48GGjZQ4dOgRfX1+888476NixIwIDA5GQkGC0jKWlJXQ63UO3debMGeTn5xumHTx4EFKpFC1atKh0zY+ifP+SkpIM0y5evIjs7Gy0bNnSMK158+aYPn06du7ciREjRmDFihWGed7e3pg0aRLWr1+PGTNm4LvvvquVWgETB5muXbtiz549hi/QmTNn8Pfffxv6GePj45GamoqIiAjDZ9RqNcLCwhAdHX3PdWq1Wmg0GqNXbfJysMFjLcU+ynm/X0B6blGtbo+IiB5OqVTi6aefxqxZs5CSkoKxY8ca5gUGBmLXrl04dOgQYmJi8PLLLxtdkfMwERERaN68OcaMGYMzZ87gr7/+wjvvvGO0TGBgIBITE7F27VpcvXoVixYtwoYNG4yWadq0KeLj43H69Gncvn0bWq22wrZGjx4NKysrjBkzBufPn8fevXsxdepUPP/884bxMVWl0+lw+vRpo1dMTAwiIiIQEhKC0aNH4+TJkzh69CheeOEF9OrVCx07dkRhYSGmTJmCffv2ISEhAQcPHsSxY8cQHBwMAJg2bRp27NiB+Ph4nDx5Env37jXMqw0mDTJvv/02nnnmGQQFBUEul6N9+/aYNm0aRo8eDQBITU0FgAr/WG5uboZ5d1u4cKFhMJdarTbq46stk3oFwEIqwcG4DER8th+xqWyZISIytfHjxyMrKwuRkZFG41neffdddOjQAZGRkejduzfc3d0xbNiwSq9XKpViw4YNKCwsROfOnfHSSy/hgw8+MFrmX//6F6ZPn44pU6agXbt2OHToEGbPnm20zMiRIzFgwAD06dMHLi4u97wE3MbGBjt27EBmZiY6deqEJ554Av369cNXX331aAfjHvLy8tC+fXuj15AhQyCRSLBp0yY4ODigZ8+eiIiIgL+/P3755RcAgEwmQ0ZGBl544QU0b94cTz31FAYOHIj58+cDEANSVFQUgoODMWDAADRv3hxLliypdr33IxH+ObK2jq1duxZvvPEGPvnkE7Rq1QqnT5/GtGnT8Pnnn2PMmDE4dOgQunXrhuTkZHh4eBg+99RTT0EikRgO6j9ptVqjVKvRaODt7Y2cnByoVKpa25fzN3Pwxm9nEZOiQd8gV/x3bKda2xYRUW0rKipCfHw8/Pz86sV4RGqYHvQ902g0UKvVDz1/m7RF5o033jC0yoSEhOD555/H9OnTsXDhQgAwXJJ2d5NfWlrafS9XUygUUKlURq+60LqJGktGd4BMKsGfl9JxKjGrTrZLRETUmJk0yBQUFEAqNS5BJpMZRlr7+fnB3d0de/bsMczXaDQ4cuTIPS91MzU/Z1uMaN8EAPDF7vpxiTgREVFDZtLLr4cMGYIPPvgAPj4+aNWqFU6dOoXPP/8cL774IgDx8rFp06ZhwYIFCAwMNFx+7enp+Uj9mXVpat9ArDtxAwcu38KtXC1c7Pi8DCIiotpi0iCzePFizJ49G6+88grS09Ph6emJl19+GXPmzDEs8+abbyI/Px8TJ05EdnY2unfvju3bt9fbPlsfJxsEudvhUmoujsZnYnAbj4d/iIiIiKrEpIN960JlBwvVpHmbL2Dloet4IdwX7w1tXSfbJCKqSeWDMJs2bQpra2tTl0MNVGFhIa5fv26+g30bqjA/RwDAkWuZJq6EiKhqyu8UW1BgoodEUqNQ/v26+87Ej4IPjawFncuCTGxaLjLzi+Foa2niioiIHo1MJoO9vb3heT02NjaGO+MSVZcgCCgoKEB6ejrs7e2NnhP1qBhkaoGTUoHmbkpcTsvD0fgMDGjNcTJEZH7Kb3NR1YcPEj2Mvb39A5/+XRkMMrUkzM8Jl9PycCQ+k0GGiMySRCKBh4cHXF1dUVJSYupyqIGRy+XVaokpxyBTSzr42uPHwwk4fzPH1KUQEVWLTCarkRMOUW3gYN9aEuwhjrC+lJKLBn5hGBERkckwyNSSABclLGVS5GpLcSOr0NTlEBERNUgMMrVELpMi0E0JALiYojFxNURERA0Tg0wtKu9eupjMIENERFQbGGRqUXmQiWGLDBERUa1gkKlFLcuDTCqDDBERUW1gkKlF5UEmKbMQmiLeg4GIiKimMcjUIrWNHJ5q8SFYl1JyTVwNERFRw8MgU8s4ToaIiKj2MMjUspaeDDJERES1hUGmlrFFhoiIqPYwyNQyw6MKUnNRqtObuBoiIqKGhUGmlvk62sDGUgZtqR7XM/JNXQ4REVGDwiBTy6RSCYLc7QAAF3nlEhERUY1ikKkDfFQBERFR7WCQqQMc8EtERFQ7GGTqQPkl2BeScyAIgomrISIiajgYZOpASw8VLGVS3M4rRmJmganLISIiajAYZOqAlVyG1k3EVpnj17NMXA0REVHDwSBTRzo2dQQAHE9gkCEiIqopDDJ1JNTXAQBwIiHTxJUQERE1HAwydaQ8yFxOy0NOQYmJqyEiImoYGGTqiLNSAT9nWwDAyUR2LxEREdUEBpk61MFHbJU5xSBDRERUIxhk6lD5owqu3uYzl4iIiGqCSYNM06ZNIZFIKryioqIAAEVFRYiKioKTkxOUSiVGjhyJtLQ0U5ZcLeVdS9duMcgQERHVBJMGmWPHjiElJcXw2rVrFwDgySefBABMnz4dv//+O9atW4f9+/cjOTkZI0aMMGXJ1eLvIgaZ+Nt50Ot5h18iIqLqsjDlxl1cXIzef/TRRwgICECvXr2Qk5OD5cuXY/Xq1ejbty8AYMWKFQgODsbhw4fRpUsXU5RcLd6ONrCQSlBUokeqpgie9tamLomIiMis1ZsxMsXFxfjpp5/w4osvQiKR4MSJEygpKUFERIRhmaCgIPj4+CA6Ovq+69FqtdBoNEav+kIuk8LH0QYAu5eIiIhqQr0JMhs3bkR2djbGjh0LAEhNTYWlpSXs7e2NlnNzc0Nqaup917Nw4UKo1WrDy9vbuxarfnT/7F4iIiKi6qk3QWb58uUYOHAgPD09q7WeWbNmIScnx/BKSkqqoQprhr+LEgBwlS0yRERE1WbSMTLlEhISsHv3bqxfv94wzd3dHcXFxcjOzjZqlUlLS4O7u/t916VQKKBQKGqz3GoxXLnES7CJiIiqrV60yKxYsQKurq4YPHiwYVpoaCjkcjn27NljmBYbG4vExESEh4eboswa4e/MriUiIqKaYvIWGb1ejxUrVmDMmDGwsLhTjlqtxvjx4/H666/D0dERKpUKU6dORXh4uFlesVSuvGvpRlYhikp0sJLLTFwRERGR+TJ5kNm9ezcSExPx4osvVpj3xRdfQCqVYuTIkdBqtYiMjMSSJUtMUGXNcVZaQm0tR05hCa6k5SHES23qkoiIiMyWRBCEBn1nNo1GA7VajZycHKhUKlOXAwAYt+Io9sbewuzHW2J8dz9Tl0NERFTvVPb8XS/GyDQ2nf2cAABH4zNMXAkREZF5Y5Axgc5+jgCAY9ez0MAbxIiIiGoVg4wJhDRRw0ouRWZ+Ma7e4tVLREREVcUgYwKWFlK093YAAByJzzRxNUREROaLQcZEyruXjjLIEBERVRmDjImEB4gDfvfF3kJxqd7E1RAREZknBhkT6dTUEa52CuQUluDA5VumLoeIiMgsMciYiEwqweNtxAdkbjqTbOJqiIiIzBODjAn9q50YZHZfTENBcamJqyEiIjI/DDIm1NZLDV8nGxSW6LD1XKqpyyEiIjI7DDImJJFI8FRHbwDA939d483xiIiIHhGDjIk9F+YLW0sZLqXmYl8sB/0SERE9CgYZE1PbyDG6iy8A4NOdsbzTLxER0SNgkKkHXuzmB2u5DBeSNej32X58vP0Su5mIiIgqgUGmHnBXW2HdpHBEBLsBAJbsu4p5my+gqERn4sqIiIjqNwaZeqJ1EzW+H9MRC4a1BgCsik5At4/+xPd/XYNez9YZIiKie2GQqWee6+KLxaPao4m9NTLyi7FgSwzGrDiK9NwiU5dGRERU7zDI1END2npi/xu98cHw1rCSS/HXldsY9H9/YT8fZUBERGSEQaaespBJMTrMF39M7Y4gdzvczivGmP8exYdbY/iQSSIiojIMMvVcM1c7bIzqhhfCxUu0vz1wDc9+dxhZ+cUmroyIiMj0GGTMgJVchveGtsY3z4fCzsoCxxOyMHLpIVxIzjF1aURERCbFIGNGIlu543+Tu6KJvTWu3c7HkMV/48OtMdCW8jJtIiJqnBhkzExzN7GraXAbD+gFsatpxJJDuMY7AhMRUSPEIGOGXOwU+PrZDvjuhY5wsJHjQrIGjy/+G7+duME7AhMRUaPCIGPGHmvphm2v9UQXf0cUFOswc90ZTFl9CtkFHAhMRESNA4OMmXNXW+Hnl7pgZv/msJBKsOVcCgYv+hsxKRpTl0ZERFTrGGQaAJlUgil9A/G/yV3R1MkGN7MLMXLpIWw6fZNdTURE1KAxyDQgbb3tsTGqG7oGOKGgWIfX1p7GlNWnkMl7zhARUQPFINPA2NtYYtWLnTE94k5XU/8vDuAAH29AREQNEINMAySXSfFaRCA2vNINga5K3M7TYtzKY9h0+qapSyMiIqpRDDINWIiXGr9P7Y5h7Tyh0wuY9stpzN10nlc1ERFRg2HyIHPz5k0899xzcHJygrW1NUJCQnD8+HHDfEEQMGfOHHh4eMDa2hoRERG4cuWKCSs2L1ZyGT5/qh3Gdm0KQQBWRSeg/xcHcCUt19SlERERVZtJg0xWVha6desGuVyObdu24eLFi/jss8/g4OBgWObjjz/GokWLsGzZMhw5cgS2traIjIxEUVGRCSs3L1KpBPP+1QqrJ4TB38UW6blaPPPtYZy/yWc1ERGReZMIJrw+9+2338bBgwfx119/3XO+IAjw9PTEjBkzMHPmTABATk4O3NzcsHLlSjzzzDMP3YZGo4FarUZOTg5UKlWN1m+OsvKL8dzyI7iQrIGlTIrpjzXHyz39IZVKTF0aERGRQWXP3yZtkdm8eTM6duyIJ598Eq6urmjfvj2+++47w/z4+HikpqYiIiLCME2tViMsLAzR0dH3XKdWq4VGozF60R0OtpZY/VIX9AtyRbFOj/9sv4QJPxyHpqjE1KURERE9MpMGmWvXrmHp0qUIDAzEjh07MHnyZLz66qtYtWoVACA1NRUA4ObmZvQ5Nzc3w7y7LVy4EGq12vDy9vau3Z0wQ2obOb4f0xEfj2wDhYUUey6lY9jXB3GVD54kIiIzY9Igo9fr0aFDB3z44Ydo3749Jk6ciAkTJmDZsmVVXuesWbOQk5NjeCUlJdVgxQ2HRCLBU5288dukrvBQW+HarXwM++og1p/kgyeJiMh8mDTIeHh4oGXLlkbTgoODkZiYCABwd3cHAKSlpRktk5aWZph3N4VCAZVKZfSi+wvxUmPzlO7o6OuAXG0pXv/1DJ7+5jCSMgtMXRoREdFDmTTIdOvWDbGxsUbTLl++DF9fXwCAn58f3N3dsWfPHsN8jUaDI0eOIDw8vE5rbchc7BRYPaEL3ohsAWu5DEevZ2Lwor+w7ngSSnV6U5dHRER0XyYNMtOnT8fhw4fx4YcfIi4uDqtXr8a3336LqKgoAGL3x7Rp07BgwQJs3rwZ586dwwsvvABPT08MGzbMlKU3OJYWUkT1aYad03uivY89NEWleOO3s+jz2T4cirtt6vKIiIjuyaSXXwPAH3/8gVmzZuHKlSvw8/PD66+/jgkTJhjmC4KAuXPn4ttvv0V2dja6d++OJUuWoHnz5pVaPy+/fnQlOj2+/yse3/91DRn5xZBKgGkRzfFidz8oFRamLo+IiBqByp6/TR5kahuDTNUVFJdizqYL+O3EDQCAnZUFRnX2wZiuTdHE3trE1RERUUPGIFOGQaZ6BEHAhlM38dWfcbh2Ox8AIJEArT3V6BHojB6BLgj1dYClhcmfdkFERA0Ig0wZBpmaodcL2Bubju//ikf0tQyjeTaWMnRr5oynOnrDViFDTkEJerVwgY0lu6GIiKhqGGTKMMjUvHRNEf6Ou42/rtzGX1du4XZexadpB3uo8N0LofBysDFBhUREZO4YZMowyNQuvV5ATKoGm04nY+Opm5DLpCgoLkVWQQnsbeSY0b8FnunkDbmMXU9ERFR5DDJlGGTq3s3sQrz843Gcvyk+58rGUoaOTR3xck9/dGvmbOLqiIjIHDDIlGGQMY1SnR4/H0nEoj1XkJF/p+spyN0O7X3s8WRHb3TwcTBhhUREVJ8xyJRhkDEtvV7A5fRcrD2ahNVHElH8jzsF927hgp6BLugX7ApfJ1sTVklERPUNg0wZBpn641auFsevZ2J3TDo2nLoBfdk3TyIBujdzhkwqgZeDNd4cEASVldy0xRIRkUkxyJRhkKmfrt7Kw7ZzKYi+loGDccaXcwe6KjF/aCuENFHDjoGGiKhRYpApwyBT/8Wl52FfbDoUFlJ8tTcOaRotAEAuk2BSrwBM6dsMCguZiaskIqK6xCBThkHGvKTmFOHDrTE4dj0TKTlFAMQWmv880YaDg4mIGhEGmTIMMuZJEARsO5+KOZvO43ZeMSQSoKmTLQJdlXgjsgUC3exMXSIREdUiBpkyDDLmLSu/GO9vuYj1J28aplnKpHg2zAf9W7khzM8JMqnEhBUSEVFtYJApwyDTMCRnFyIhowDfHriKvbG3DNO9HKwxsoMXQn0d0NnPEVZyjqUhImoIGGTKMMg0LIIgYE9MOradT8XumDTkFJYY5jkrFRgZ2gTxt/LhpFTg1X7NYC2XIbeoFN6OfOYTEZE5YZApwyDTcBWV6PD7mWT8deU2jsRnGK52KmdpIUWJTg9BAN4f2gqjw3yRVVAMR1tLSCTsjiIiqs8YZMowyDQOxaV6/O/kDRyNz0SgmxJ7YtJxIiHLaBl3lRVSNUXo4GOP1x9rgTbeauRrS5Gv1SHAxZbhhoioHmGQKcMg0ziVP5XbRanA8r/j8c2Baw9cvnszZ7TyVGHb+VQ8G+aDSb0C6qhSIiK6FwaZMgwyJAgC/jibAplUglaeKizbfxV/XkpHmkYLmVQCCYBSvfGPwZsDWqCoWAcXOwWe7OjNQcRERHWMQaYMgwzdT25RCazkMiRnF2Lh1ksoKNHBRanA/07eMFrOQ22FAa3d0dbLHi52CrT3sYeNpYWJqiYiahwqe/7m/8bUaJU/x8nXyRbLng8FIHZJSSXA72eT0TPQBedv5iA5pwgrDl43fK6JvTWWj+0IpcICtpYWcLC1NEX5REQEtsgQPVBRiQ47L6bhaHwG4tLzEJeej9t5d66OksskeLGbH1p6qiCXSRER7AZLC6kJKyYiahjYtVSGQYZqUlZ+MV75+SSir2VAJpVAd9fYmhZudpj+WCBaeqjhqlJwbA0RURUxyJRhkKGaJggCbmQVwsVOgUNXb+P7v+IhCEBsWi4y84uNlnWxU6BXcxe09FDBTWWFXi1coFSwR5eI6GEYZMowyFBdycovxv/tuYLD1zJw7VY+inX6CssoFRZo46VGdkEJQn0dMKarL5o62cJCxu4oIqJ/YpApwyBDpiAIAjSFpbiQkoP9sbdwI7sQMckaXLudX2FZiUTskvpXO08MbdcETeytTVAxEVH9wiBThkGG6gtBEHAkPhM3swphbSnDuuNJ2H/5Fu4aZoNOTR3QI9AFzVyVcFYq0MHHni02RNToMMiUYZCh+qxUp8ftvGLsi03HxtM3cfhaZoVlvBysMaqzD4Lc7RDgooSXgzWDDRE1eAwyZRhkyJzczC7E3kvpOHwtA6k5RYi7lYfsghKjZWwsZRjevgmeDfNBSw8VnxFFRA0Sg0wZBhkyZ4XFOvzv5A0cvpaBq7fyce1WHrSldwYRe6it0MLdDk2dbNHB1wE9mjnzBn1E1CCYRZCZN28e5s+fbzStRYsWuHTpEgCgqKgIM2bMwNq1a6HVahEZGYklS5bAzc2t0ttgkKGGRK8XcDg+Az8cSsDe2HSjUAMAlhZSDGjlDgcbOeys5OjWzBmdmjqwK4qIzI7ZPKKgVatW2L17t+G9hcWdkqZPn44tW7Zg3bp1UKvVmDJlCkaMGIGDBw+aolQik5NKJega4IyuAc4oKtHhZEIWEjILEJuai8PXMnApNRebzyQblv9qbxy8HKwxrpsfvBysEeiqhL+L0oR7QERUs0weZCwsLODu7l5hek5ODpYvX47Vq1ejb9++AIAVK1YgODgYhw8fRpcuXeq6VKJ6xUouQ9dmzuha9l4QBJxOysaemHRIJMCNrELsjU3HjaxCvP/HRcPnOjd1RHZhMXKLSrF4VHt0bOpomh0gIqoBJg8yV65cgaenJ6ysrBAeHo6FCxfCx8cHJ06cQElJCSIiIgzLBgUFwcfHB9HR0QwyRHeRSCRo7+OA9j4OhmlFJTqsPZqIPZfSkVNYgvM3c3D0+p0ro8b89ygGhnjgZlYhnunsjaHtmqCoRAdLmRRSKQcRE1H9Z9IgExYWhpUrV6JFixZISUnB/Pnz0aNHD5w/fx6pqamwtLSEvb290Wfc3NyQmpp633VqtVpotXce6qfRaGqrfKJ6z0ouw9hufhjbzQ8AkJRZgF0X0+ChtsLPRxLxd9xt/HbiBgAg+loGlu67istpuejV3AXfvtAR8n+MrdHpBWQVFMNZqTDJvhAR3YtJg8zAgQMNf2/Tpg3CwsLg6+uLX3/9FdbWVbu76cKFCysMICYikbejDV7sLoaaPkGu+Hh7LPSCAAupBMsPxuNSai4AYG/sLcxafw6DQtyhspLDSanA5J9O4Ep6HhY90x6D23iYcjeIiAzq3eXXnTp1QkREBB577DH069cPWVlZRq0yvr6+mDZtGqZPn37Pz9+rRcbb25tXLRE9xJFrGYi7lQeFhQxv/namwh2Hy9lYyrAxqhuau9nVbYFE1KhU9qqlenVNZl5eHq5evQoPDw+EhoZCLpdjz549hvmxsbFITExEeHj4fdehUCigUqmMXkT0cGH+Thgd5osnQr3wwfAQeDtao3UTFext5ACAkCZqhPk5oqBYh/GrjuFmdqGJKyYiMnGLzMyZMzFkyBD4+voiOTkZc+fOxenTp3Hx4kW4uLhg8uTJ2Lp1K1auXAmVSoWpU6cCAA4dOlTpbfA+MkTVo9MLuHorD37OttAUlmD4kkNIzCxAE3trzBnSEj0CnWFjafLrBoioganV+8gkJSVBIpHAy8sLAHD06FGsXr0aLVu2xMSJEyu9nhs3bmDUqFHIyMiAi4sLunfvjsOHD8PFxQUA8MUXX0AqlWLkyJFGN8Qjorojk0oM3UhOSgV+ebkLnv3uCOJv5+PlH0/AwUaOJaNDER7ghBKd3miAMBFRbatSi0yPHj0wceJEPP/880hNTUWLFi3QqlUrXLlyBVOnTsWcOXNqo9YqYYsMUc27nafF13vjsON8KpJzimApk8LT3go3sgox91+t8HwXX1OXSERmrlYfUeDg4IDDhw+jRYsWWLRoEX755RccPHgQO3fuxKRJk3Dt2rVqFV+TGGSIak9RiQ7T1p7G9gvGt0QY1dkbMqkEQ9p4IszfyUTVEZE5q9WupZKSEigU4r0kdu/ejX/9618AxBvWpaSkVGWVRGSGrOQyfD26AzadvglbhQWOX8/Ed3/FY83RJADA2qNJ+GhkGzwR6oXCYh0OX8tAeIATrOQyE1dORA1FlYJMq1atsGzZMgwePBi7du3C+++/DwBITk6GkxN/+yJqTGRSCUZ0EMfL9W/phkBXO5xKysat3CLsjknHzHVnsO1cCq6k5yExswB9g1yxfExHSCS8czARVV+Vupb27duH4cOHQ6PRYMyYMfjvf/8LAPj3v/+NS5cuYf369TVeaFWxa4nINPR6AZ/tisXSfVcr3JPmpe5+uJ6Rj5aeakzt24wDhImoglodIwMAOp0OGo0GDg53nuty/fp12NjYwNXVtSqrrBUMMkSmdfVWHpbtuwoHW0vYWMrw5e4rRvNDfR0wPaI5MvK1WHf8BkaGNsHw9l4mqpaI6otaDTKFhYUQBAE2NjYAgISEBGzYsAHBwcGIjIysetW1gEGGqP7Q6QWMX3UMh69l4PE2nthxPhW52lKjZSxlUvw+tTtauPPOwUSNWa0Gmf79+2PEiBGYNGkSsrOzERQUBLlcjtu3b+Pzzz/H5MmTq1V8TWKQIapf9HoBAsSxNYkZBVi6/yq2nxcvEnBXWyMmRQN/Z1t42FshJacIOr2A1x9rjqHtmpi2cCKqU7UaZJydnbF//360atUK33//PRYvXoxTp07hf//7H+bMmYOYmJhqFV+TGGSI6j9BECCRSJCuKUL/Lw8gu6DEaL6lhRTz/9UKvx5PQpsmaswd0gpSKQcLEzVktXr5dUFBAezsxGbfnTt3YsSIEZBKpejSpQsSEhKqVjERNVrlVzC5qqzw7fMdsfH0TbT0UCHARYnv/rqGPy+lY9b6cwCAU4nZAABnpQLFOj2Gt28CfxclAKC4VA+pBLDg4GGiRqNKQaZZs2bYuHEjhg8fjh07dhieRJ2ens5WDyKqls5+jujs52h439JDhSFf/Y3EzAJ0DXDCoasZWBV95xemxX/G4YVwX4zt2hSjvz8CuUyKNRO7oIm9tSnKJ6I6VqWupd9++w3PPvssdDod+vbti127dgEAFi5ciAMHDmDbtm01XmhVsWuJyPzlFJYgObsQwR4qLNt/FZ/tjEUXfyfIZVLsjU2HIABWcimKSvQAgAAXW/z6cjjU1nLsuJAGH0cbhHipTbwXRPQoav3y69TUVKSkpKBt27aQSsVm3KNHj0KlUiEoKKhqVdcCBhmihqd8TA0AbDh1A6//egaCAPg42qBEp0dKThFUVhZwU1nhSnoeLKQSzBoUDEsLKQ5cvoVj1zPRt4UrPhwRwrsME9VTtR5kyt24cQMADE/Crm8YZIgavm3nUvDHuRS80b8FSvV6RP18CrFpuQDEy7mLdfp7fq6tlxpd/J3Qxsseg0LcebdhonqkVoOMXq/HggUL8NlnnyEvLw8AYGdnhxkzZuCdd94xtNDUBwwyRI2PTi9gy7kUXE3PwwvhvlhzNBHfHLiGlh4q9GzuAk97K8zZdAG5RXfuYdM3yBUfjQiBq8rKhJUTUblaDTKzZs3C8uXLMX/+fHTr1g0A8Pfff2PevHmYMGECPvjgg6pXXsMYZIjoXq7fzseGUzdxK0+L347fQLFOD3sbOZ7p5INDV28jp7AEbnZWeHNACwR5qPD13ji09bLHgNbupi6dqFGo1SDj6emJZcuWGZ56XW7Tpk145ZVXcPPmzUevuJYwyBDRw8Sm5uL1X0/jQrKmwjyVlQWC3FU4ej0TAPBiNz/8e1AQkrOL8H97rmBwG3f0DXKr65KJGrxaDTJWVlY4e/YsmjdvbjQ9NjYW7dq1Q2Fh4aNXXEsYZIioMopL9ViyLw7Hr2dhQGt3tHC3w0fbLuFEQhYA8aZ8xaXiWJsu/o5IyizEzexCyKQSfPl0Owxp64miEh1OJWajvY89BxETVVOtBpmwsDCEhYVh0aJFRtOnTp2Ko0eP4siRI49ecS1hkCGiqrqdp8WIJYeQqinC9y90REFxKV7/9QwKinUAAGu5DIUlOkgkQPdmzrianofknCI0d1Ni3pBWsFFYQKmQwV1tDaWiSrftImq0ajXI7N+/H4MHD4aPjw/Cw8MBANHR0UhKSsLWrVvRo0ePqldewxhkiKg6ikp0yNeWwkmpAABcTsvFq2tOAQCWj+2Eb/ZfxQ/RD76juYVUgpd7+WNgaw9sOn0TV9LzoNML+HB4CLwdxYfvpmuKkFlQjCB3/j9FBNTB5dfJycn4+uuvcenSJQBAcHAwJk6ciAULFuDbb7+tWtW1gEGGiGpa+X+b5ZdrX7+dj81nkuFga4l+Qa5YuO0Soq/ehqVMilxtqdHVUf/U3E2JleM644+zyfh812VoS/VYOa4zejV3QVGJDsnZhVBZy+FcFqKIGpM6u4/MP505cwYdOnSATqerqVVWG4MMEZna1nMpePt/Z5FfrENkKzeEBzhj8Z4rSM/VVljW29Ea/Vu6Y9Wh6yjVC7C1lGH1hC5o620PALiUqoFSYQEvB5s63guiulWrD40kIqLKGxTigW7NnFGi0xtaV9o0UeOZbw+jsEQHf2dbjOvuh6V745CUWYjlf8cDAGRSCfKLdXhx5TE81ckbB+Nu4+yNHNhYyvDj+M7QlupxOikb9taW6BPkAg/1nedLFZXooLCQ8iZ/1OCxRYaIyETSc4ug1wPuavEmfLsvpuGlH47DTmGBT55si+6Bzhj17WGcu5lT4bMWUglK9Xf++1Zby/HLy12gLdFj+d/x2HouBR18HPDxE23Q1Nm2zvaJqKawa6kMgwwRmZPzN3Pgaqcw3GH4Vq4Wn+y4BEsLKVq42aFPkCtm/HoGR+IzYSmT4rGWbohNy0Vcep7RJeLlrOUyvD0wCF0DnLDzYhqu3sqDpUyKfw8OhpWFDGduZKONlxoKC/Fy8TNJ2dhw6iZe6R3AuxyTSdVKkBkxYsQD52dnZ2P//v0MMkREtaiwWIftF1LQqakjvBxskFNQgqe/jcal1FzIZRIMaeOJoe2bYNm+q4i+lnHPdfQIdIa2VI+j8Znwd7HFwuEhaOdjj4jP9yMpsxCBrkqsndjFcLUWUV2rlSAzbty4Si23YsWKyq6y1jHIEFFjkFNQgt0xaegR6GxoSdHrBfx4OAEfbbuEYp0evZu7oFUTNb47cA2FJca/cEolQN8gN+yOSTNMa+WpwuqXumDJvjj8HXcb7w5uifAApzrdL2q8TNK1VB8xyBBRY5eZXwwJAAdbSwDAjgupmPzTCTjYWGLxs+3x67EkbDydbFh+Uq8A/HYiCbfziuFoa4nM/GIAYtiZ0MMf/YLd8M3+q7iZXYjnw30xsoMX72RMNY5BpgyDDBFRRQkZ+bC3sYTaWg69XsA7G89hzdEkNHdTYuurPXAlPQ/PfHsYOYUlkEiArgFOOBh3724qJ1tLPNHRC5YyKWwVFghyt8Ol1FxkFRQjqk8zqKzkKNXpkV+sg5VcahiPQ/QgDDJlGGSIiB5Orxdw6GoGWnmqDC03527k4OMdl/BEqBeGtmuCnRdS8d1f13DsehYigt3QqakDfohOwM3s+z9fr6WHCm297fHbiSSU6ASoreVYNykcnvbW2BebjqyCEgS42KJrgHNd7SqZCQaZMgwyREQ1K19bCtuyZ0eV6vTYci4Ff125DRtLGW7naXEpJRdNnW1x9kY2bucVV/i8f9nl4Ndu5wMAJBJg5bjO8HW0wemkbIT6Ohge3fDtgas4lZiN94e15h2OGxkGmTIMMkREpnHtVh5e+fkk7KwsMLN/C/g62WL4koNIySkCALjaKeCutsLZGzmws7KAtlRvuHy8Z3MXPNbSDbM3ngcAtPO2x5oJXWBtyW6pxqKy529pHdb0QB999BEkEgmmTZtmmFZUVISoqCg4OTlBqVRi5MiRSEtLu/9KiIio3vB3UWL7tJ5YN6krwvyd4K62wpLRHWBvI0dnP0f88Wp3/PpyONp4qZFbVIriUj38nG0hlQAHLt8yhBipBDidlI2RSw/hx8MJOBh3GzEpGqRritDAfxenSqgXLTLHjh3DU089BZVKhT59+uDLL78EAEyePBlbtmzBypUroVarMWXKFEilUhw8eLDS62aLDBFR/VKi00Muu/N7dEpOIT7eHotwfyc82dELcel5mPDDcVzPKEAHH3vMjGyB8SuPV7hkHACCPVQY3t4TKTlFsLW0QPdAZ3Rq6giZ9M6jGbSlOg4wNkNm07WUl5eHDh06YMmSJViwYAHatWuHL7/8Ejk5OXBxccHq1avxxBNPAAAuXbqE4OBgREdHo0uXLpVaP4MMEZH5ySkowa6YNDwW7Aa1jRzpuUVYf/Im9sWmIyOvGFkFxcjML4b+HmewQFclXujaFACw8dRNnEzMwovd/PDWgCBYWtSbjgh6CLMJMmPGjIGjoyO++OIL9O7d2xBk/vzzT/Tr1w9ZWVmwt7c3LO/r64tp06Zh+vTp91yfVquFVnvnibIajQbe3t4MMkREDUx2QTF+OpyA00nZ8HWyRUaeFnsupSO3qPSey7duosKrfQPRwdcBVnIZrCyksJAx2NRXZvH067Vr1+LkyZM4duxYhXmpqamwtLQ0CjEA4ObmhtTU1Puuc+HChZg/f35Nl0pERPWMvY0lpvQNNJqWU1iCFQfjcSIhCxZSCdp628PXyQZzN13A+ZsaTPzxhNHyzkoF2nip4eNoA38XWwxp42m4/JzMg8mCTFJSEl577TXs2rULVlY192CyWbNm4fXXXze8L2+RISKihk9tLce0iOYVpndr5oyVB6/jl2NJyMi/c0n47Twt/ryUbnj/wZYYdPZzhIfaCjmFJUjP1SKnoAR9g1wxM7IFrOQy6PQCzt7IRrCHqsIdjYtKdMjTlvJS8Tpksq6ljRs3Yvjw4ZDJ7nwJdDodJBIJpFIpduzYgYiIiEfuWrobx8gQEdE/6fUCinV6FBbrEJ+Rjws3c3AzuwgHLt/CxRTNfT/XzFWJcH8n/HXlFq5nFKCZqxJLRndAczc7AOKVVRN+OI7cohL8/FIXhPo61NUuNUj1foxMbm4uEhISjKaNGzcOQUFBeOutt+Dt7Q0XFxesWbMGI0eOBADExsYiKCiIg32JiKjGCYKA8zc1iEnRIFVTBAcbOVzsFCgq0eP9Py4ateSUs5RJ8VhLN1haSLH1XAq0ZffBcVdZ4Y9Xu8PJ1hJx6XmQSiXwd7bF72dTcOFmDib1CqjQhXUjqwDOSoVRK49eL0D6jyuwGpN6H2Tu5Z+DfQHx8uutW7di5cqVUKlUmDp1KgDg0KFDlV4ngwwREVXX7Twtdl5IQ1JWAdxVVugb5Ip3N57H/su3jJbr08IFiZkFuHorH9ZyGZztLJGUKT7CwU2lQJpGvBjF18kG3zwfiiB3FWJTc/HpzljsupiGdt72WDcpHHKZFHti0vDmb2cxMtQLswYGQSJpXIHGLAb7PswXX3wBqVSKkSNHQqvVIjIyEkuWLDF1WURE1Mg4KxV4NszHaNrKcZ1wIVmDredSYCGTIszPEeH+Trh2Ow/jVh5DUmYhkjILYWkhhSAISNNoYSWXwt7aEgkZBRjw5V9o5qpEXHqeYZ2nk7Lx7YFr6NbMGVNWn0JhiQ7fHrgGD7UVxnXzQ2xqLvZcSkMHHweE+TlCIpGguFSPy2m5CPZQGd0/p7GoVy0ytYEtMkREVNf0egFxt/JwI6sAHZs6orhUj+PXMxHiZQ8rCyneXn8Ou2PSIAjinYsHtHZHSw8VPt15GVIJDPfH8XKwxo0ssUWnqZMNEjILUH7WbuOlRlSfZliy7yrOJGWjs58jPnuyLbwdbXArV4uLKRp0C3CChUyKohId8rWlUFpZGN0c8NqtPBQU69C6ibquD9FDmWXXUm1gkCEiovroRlYBjl/PQsemDvBysIEgCBi/6rjhKqqezV2wZHQHfLYzFisOXjd8rnNTR5y9mY2iEn2FdVrJpRje3gtbziZDU1SKYA8VPNVW2BubDr0A2NvI8c1zoQjzd0Jceh6GfvU3Ckt0+N/krmjv4wBBELBs/zXcztMi3N8JfYJcIZNKcO1WHo7EZyIjT4tnOvvUyVVZDDJlGGSIiMhcFBbrcPR6Jlp5qozCQkaeFmdv5sBTbY0W7nbIyNPii92XsfpIIpo62+L9oa2xaM8VHInPNHxGIgHudYa3U1jggxEh+OrPK7icJnZrtXCzw+9Tu+Onwwl474+LhmWHtfPE05188PzyIygtayYKdFVi7cQucKrlMMMgU4ZBhoiIGqrbeVqorOSGcTg7L6Zh1aHr6NbMGU+GeuGH6AToBAEjO3jB094KY/97DEev3wk7LnYK6PQCMvOL0dnPEacSs1CiE9A3yBUHLt9CqV6AncICudpStG6iQrpGi/RcLZQKCwiCAJW1HE3srTG+ux8GhnjU6L4xyJRhkCEiIhLlFJZgwR8XEZuWC0EA5v2rFZKzC/Hq2lOG1puBrd2xZHQHfPVnHD7bdRkA4ONog22v9UCqpgjPfHsYt3K1Ruv9+Ik2eKpjzd58lkGmDIMMERHRg11IzkH01QzkFJZgYk9/2FnJUarT4/nlR3EyMQurJ4Qh1NcRgNj9dSU9F3ZWcmQXFONmdiHaedvDy8GmRmtikCnDIENERFQ1JTo9Cop1UFvL63zbDeI+MkRERGQ6cpkUauv6/YTw+l0dERER0QMwyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERktkwaZJYuXYo2bdpApVJBpVIhPDwc27ZtM8wvKipCVFQUnJycoFQqMXLkSKSlpZmwYiIiIqpPTBpkvLy88NFHH+HEiRM4fvw4+vbti6FDh+LChQsAgOnTp+P333/HunXrsH//fiQnJ2PEiBGmLJmIiIjqEYkgCIKpi/gnR0dHfPLJJ3jiiSfg4uKC1atX44knngAAXLp0CcHBwYiOjkaXLl0qtT6NRgO1Wo2cnByoVKraLJ2IiIhqSGXP3/VmjIxOp8PatWuRn5+P8PBwnDhxAiUlJYiIiDAsExQUBB8fH0RHR993PVqtFhqNxuhFREREDZPJg8y5c+egVCqhUCgwadIkbNiwAS1btkRqaiosLS1hb29vtLybmxtSU1Pvu76FCxdCrVYbXt7e3rW8B0RERGQqJg8yLVq0wOnTp3HkyBFMnjwZY8aMwcWLF6u8vlmzZiEnJ8fwSkpKqsFqiYiIqD6xMHUBlpaWaNasGQAgNDQUx44dw//93//h6aefRnFxMbKzs41aZdLS0uDu7n7f9SkUCigUitoum4iIiOoBk7fI3E2v10Or1SI0NBRyuRx79uwxzIuNjUViYiLCw8NNWCERERHVFyZtkZk1axYGDhwIHx8f5ObmYvXq1di3bx927NgBtVqN8ePH4/XXX4ejoyNUKhWmTp2K8PDwSl+xRERERA2bSYNMeno6XnjhBaSkpECtVqNNmzbYsWMHHnvsMQDAF198AalUipEjR0Kr1SIyMhJLliwxZclERERUj9S7+8jUNN5HhoiIyPyY3X1kiIiIiB4VgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdlikCEiIiKzxSBDREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdlikCEiIiKzxSBDREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdlikCEiIiKzxSBDREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdkyaZBZuHAhOnXqBDs7O7i6umLYsGGIjY01WqaoqAhRUVFwcnKCUqnEyJEjkZaWZqKKiYiIqD4xaZDZv38/oqKicPjwYezatQslJSXo378/8vPzDctMnz4dv//+O9atW4f9+/cjOTkZI0aMMGHVREREVF9IBEEQTF1EuVu3bsHV1RX79+9Hz549kZOTAxcXF6xevRpPPPEEAODSpUsIDg5GdHQ0unTp8tB1ajQaqNVq5OTkQKVS1fYuEBERUQ2o7Pm7Xo2RycnJAQA4OjoCAE6cOIGSkhJEREQYlgkKCoKPjw+io6NNUiMRERHVHxamLqCcXq/HtGnT0K1bN7Ru3RoAkJqaCktLS9jb2xst6+bmhtTU1HuuR6vVQqvVGt5rNJpaq5mIiIhMq960yERFReH8+fNYu3ZttdazcOFCqNVqw8vb27uGKiQiIqL6pl4EmSlTpuCPP/7A3r174eXlZZju7u6O4uJiZGdnGy2flpYGd3f3e65r1qxZyMnJMbySkpJqs3QiIiIyIZMGGUEQMGXKFGzYsAF//vkn/Pz8jOaHhoZCLpdjz549hmmxsbFITExEeHj4PdepUCigUqmMXkRERNQwmXSMTFRUFFavXo1NmzbBzs7OMO5FrVbD2toaarUa48ePx+uvvw5HR0eoVCpMnToV4eHhlbpiiYiIiBo2k15+LZFI7jl9xYoVGDt2LADxhngzZszAmjVroNVqERkZiSVLlty3a+luvPyaiIjI/FT2/F2v7iNTGxhkiIiIzI9Z3keGiIiI6FEwyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMERERmS0GGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpCprrxbQEGmqasgIiJqlCxMXYDZ+vMD4PRqQHMDUKiAcdsA99amroqIiKhRYYtMVRXniyEGALQaYM0zYusMERER1RkGmaoKHQOM3QJMOw84+gM5ScCqx4GMq6aujIiIqNGQCIIgmLqI2qTRaKBWq5GTkwOVSlU7G7l9BVj5OJCXKnYzBf8LsLAEMq8BwUOA0BcBKTMjERFRZVX2/G3Ss+uBAwcwZMgQeHp6QiKRYOPGjUbzBUHAnDlz4OHhAWtra0RERODKlSumKfZBnAOBl/cDXp3FbqbTPwHH/wtc2wdsmQGsHAwUZpu6SiIiogbHpEEmPz8fbdu2xddff33P+R9//DEWLVqEZcuW4ciRI7C1tUVkZCSKiorquNJKsHMHXtwOPL8R6PoqED4F6PMOYKkEEg8Bm6cADbvxi4iIqM7Vm64liUSCDRs2YNiwYQDE1hhPT0/MmDEDM2fOBADk5OTAzc0NK1euxDPPPFOp9dZJ19KD3DwBLI8E9CVAYCSgdAU6TwQ82tR9LURERGbCLLqWHiQ+Ph6pqamIiIgwTFOr1QgLC0N0dPR9P6fVaqHRaIxeJtUkFIj8QPz7lR3AqR+B7/sBh74CdKWmrY2IiMjM1dv7yKSmpgIA3NzcjKa7ubkZ5t3LwoULMX/+/Fqt7ZF1nghYqYHcFCDxMHB5O7DzHTHUuLUGdFqg+QBA6Q6kngFsnAGHpoC1vfg5awfxTyIiIjJSb4NMVc2aNQuvv/664b1Go4G3t7cJKwIgkQBty7rCBAE4sQLY8x5w65L4AoCY3x+8Dgc/wK0VoLADHAPEm+/ZOIljc1RevCqKiIgapXobZNzd3QEAaWlp8PDwMExPS0tDu3bt7vs5hUIBhUJR2+VVnUQCdHwRaDUCOLMG0OvEm+td3AiUFgGeHYDCLPG+NEUaoCgHKC0EsuLF173IbcQrp5xbAC7Ny/4MApwCAKmsTnePiIioLtXbIOPn5wd3d3fs2bPHEFw0Gg2OHDmCyZMnm7a4mmBtD3T5x370fuv+yxblADeOAZnx4uXd6TFiS05RDqBJAUoKgJQz4uufLKwB52ZiV5VXRyBoMGDnIbbkyOS1sltERER1yaRBJi8vD3FxcYb38fHxOH36NBwdHeHj44Np06ZhwYIFCAwMhJ+fH2bPng1PT0/DlU2NhpUaaBZx73m6EiDrOnArFrgdC9y6LIac25fFgJN6Tlzu2l7gwCfi3xUqIPAxoGl3wLM94NpKvIEfERGRmTHp5df79u1Dnz59KkwfM2YMVq5cCUEQMHfuXHz77bfIzs5G9+7dsWTJEjRv3rzS2zD55demoteJdxbOug5okoErO4Hrf4utOLjrn1xmKXZp+fcWu6Mc/MSAI6u3DXZERNTAVfb8XW/uI1NbGm2QuR+9Xry3zeXtQPJJ4OZJoCi74nJWasC5OWDrAviEAz5dAHsfwNaVA4uJiKjWMciUYZB5CEEQW27iDwCJ0WLrTdp5ccDxvcgsAbW3GGocfMXLxO19xdDj2pIhh4iIagSDTBkGmSrQ68SBw5pkIDtBfGZU2kUgNxkQ9Pf/nI2zeOdiXQng6CeGG5cW4pgcS1vAox2gdKmrvSAiIjPGIFOGQaYG6UoAzU0gO1F8ZSWIQScrQWzFKc57+DosrAGJVByL49wcsLACXIPE8TnF+WLgcWstXqZORESNVmXP3xzNSZUnk4tdSQ5NK84rLQaST4n3vIEEyLxadiXVFaCkECjIEK+qKi0Ul089K77uxbm5uA1dCaAvFS8Xdw8BCjLF8Txqb/Hy9fJuLoUSyL8lXn3l3ExsUZJIxTAkCOKLXV5ERA0SW2So7hTlAIXZYkBJvyi25pQUAgkHgRsnABsHIO/WnbBTFfa+4qMgLJVA0CDxSq2cG0DLoWLXV/pFsdVH5SmO6clLB/LTgZbDxPpOrAC8uwDtRolXfOlKATs3QOkmhrH4A+Lfm/YQHyehKxUHQutLgbw0cd8srMTB0oJeDH82ToCuWNx3pasYsPJuAVYqwKIe3rxRVyIGQd5MkYhMiF1LZRhkzEyRBojbLd4DRyoXT6Y5SUDaBfGKKWsH8X1xnhiCshPFZa3UQMpZQNCZeg8qkinE52kBZTcjVIjjjSQyQN1E/BNlLUcQxJYmWxcg/7bYlWfrDKiaiDczLMwSW59kcjEwWSjEP/U6MUjZOIotU/oSIOem2DJm4yQOzi4PeC5BQMxm8di1HglY2Ytdg57tgcJM4PRqcfttnhKDoWtLMaxd2wdc/0sMcmpv8fEYWo24fa9Od27W6NpSDHbxB8TQpvYBmnQQuw6v7QV8ugK2TmKI1JeK25LJAW0ecG6d+KDV8qfDlxQBt2LEVjpLW3Hslo2zeNwAMXzeuiyOxbJSiSGsOE/cp392TwrCg7srC7OA9EviY0Csyv6f0JUCGXHitu/XoqfNFWtU2AFyq8p/J/JuiaG6aXfxOy4IwLHvxeMXMffOs9UKs8uOkfOdzxblAFIL8XiUKy4A5NYV91FXKn7+n7Xl3ACyk8R/0/Llb5fdz8u5Wdnnyu5PZechtngSmQCDTBkGmUYkL128AaCjn/gfc9xusUvKNVg8QQqCeLLWFYuPe0iPAawdxZsBnl0nrqPDC+LJOu28eD8dS1sxIOTfFgOGb1cxHGQniidzqcWdR0dYO4jLlBaJJxuJrH4Gq6pQeQGaG/efr3QXQ5CuWGzNAYwHhruFlB3HdPGRGq7B4m0AAPEY+vcWuyJzksRpzQeKgSJut9gSpnQXw0r8fvG4NosQQ17aBQCCGK5CxwJHvxVDidwGCOgrBsCza8Xw6N9b7LLMSwXiywKZS3PxFgTl67FSAyFPiSf/2G3isr7dxO0d+UasxUot3pW7KAeI/lr8N5YpxO1b2ooBzs5d/H7otEDSMSDjilhPQF+xe/TgIjH4ubcBOo4DEg6J31FAvKdT33eAyzuAE6vEUNpyGODdWQw/Z34RQ0vXqWKQurZPDHj23kCPGeL3MOEQEPOHeIwkUiB4CBDQRwx9x74X6+o8Ufx3OLxM7PaVyIDR68R/lwOfistY2gH/WgRc+gO4+idg5wn49RCfHXdtH5B0FMhNFYNs2MviMdGViMH01I9i1/Jj88VwmnNTbPEszAL6vCOG7ttxwI5ZYhAf8Z0YcAGxq1omF4NWqVb8BYcXCjQ6DDJlGGSoUkq1ACR37nBcqjXu9tGViidmC0sxEBVmiScMiUQMOZa24smlXHkLQKn2TkuIwk5sNdIVA57txP/0sxPF7Uokd/4sLRJDmZVabBEpyBBPSLkpYkuDnbt4oi0tEtdfWiRuU+kmLnPrMmBpI7ZgOfmL9eXcEH+7zr8lhj2vTuKDR8+sFU907iHik9n1peIJLj8diNsjfvbaPrG7T2ohPiOstKwlLC9drCc3uexGixBbSwpui393CxF/m085I7aaAYDcFijJLztIEnHb/wx7Nk7i/v6TVC6ezIGy5e+6cs5ondVgZX/veyrVlrv3RSITvyfanLqr4W5Gx7qGgrhULgamtAt31ufQVPwOxvx+5/vr2lJsLUw4JAZItQ8QMhI49bP4fXQJvnM/q5KCOxcc+IYD7Z4Tu3pzU8Vj6hQo/tJi5wYknxa/w/Y+YnB09K+ZMXP3a+XT5okBU+VRcd6D6PVA7FbxWDkFVL++ysq8Jv78erY3nl6YDVzaAjQfcCdg1jEGmTIMMkTVlH9bPOE07S4+nPRupVpxnJPSTeya0aQAEMRxSIA4SPvESnGAdrvRZeOWkoDA/mJLS0ac2NUltxZbNTLixBAlkYgnNr9ewMlV4mM3wqeILRlxe8RavMPEy/t3zRFbMMKjgPbPif85n/sVyE0DQp4Uu1YSD4snOgsrsXUmL01sXfNoJ7a02TiLdSQdEVt0PNqIJ89tb4rdLD1miC0q1/8StwcAQ/5P/I8+fr/YsiGVic80K8wWT75SC/Ehrk4B4rpvXRaDbNBg8XVokdhqIZGKLRo2zsC6MeLJ2D1EfMCsjRNw8gfxOMqtgbajxGN09hcx6Pr3BnzCgAsbgAsbxQDu6A+0Gi627uSlAsf/K7aIKOyAkCfEf7MNk8Sg2WOm2MX4y2hx3wFgwEdA6Dhg81TxODoFAoM+Ebvtji0Xuwi9Ooufs3ES/30T/hbDs9xarLW82+zSH3e+K77dxH/77MQ70/x7i916eak1950td69gLLcRW04FQeyitPcRj3vqWfH7oW4iXjnZJFRsZUw8It5jy70N0OsNcR+PLxe7YO19gBYDxRZEr45iWFv9tBjuA/qJ3z3PduIvIhZWgNoL2P5vIPEQ0PoJ8Tvg0FT8JeOP6cCZ1WIr2HO/iaEt+TTw5wIxuKmaiN9vex/xZ0irEYNvQF/xux2/X/zO+HYVu2tPrxZbnVv+SxzTl5cO/PWZ2ILc662yei8C30eIvwh0fRXoN1e8o/vlHcDvr4m/GLm3Acb8LrZIpp0TW886jQc82gOnfxKDm9xKbLX0aFuj/3wMMmUYZIioxulKIY5nMuOHrxZmi6GjvOUx7xawZ74YHNs8KU4TBLGb1SnQeJyNXl+xVaO0uOIz2wRB7JIqyhZDp9pLDDnRX4nzA/qVdddeA3bPFbtzWwwST+4xm4Hz68VusTZPiV126RfvBDI7d/GEfXy52OrnFiLeykHQiyfoW5cACGKYbBYhtkamXbjTAlTTLKzE/S0fD1cdchux5is7K9Z7d0vZvVopIYHRo2jkNmJ4/efnmg8Ux59lXb8zzbODGOCOfWe8Oiv1nVbXchbWxhdmPP6FGLxrEINMGQYZIqIG7l4hSpsnhhm1lxh6ADGAZl0XT/yCXhz3lZ0kts45NxfH12UnATeOiQHO1lUMRz7hYgtH7FZxW65BQM83xYAWuw2I23XnRO/XE+j/gRjE4vaIY7/svcXu6Lw0MSyER4ndNmnnxe4xnVYMBkO/As6sEceGlQuMBNo+DcRuF1vHALEbTu0FZFwVWxUtlWLgS48R16kvFcdo+XYDLm66Ezi8OolhsXw9gNjC0/MNYMc7YitPubBJ4ud/fV58b+0gtt5lXRfHWUEQW2u8w8T1txsthtIaxCBThkGGiIhqlV4n3k6iMFvsXrnXrQsEQezmsnEyHluj14vdago78aXXiV1HSUfFcTZtn73T+pWdKAaw8nt5CYLYVWTj/I+r7UrELiE7T7GbSJsntkZJLcTwI5GILVYxm8XQ0+cdcVxObiqwex5w/aA4QLv1CHF9x74Hru4FHnvvztidlDNiC15A31q9RxeDTBkGGSIiIvNT2fM3b3dKREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbDHIEBERkdlikCEiIiKzxSBDREREZotBhoiIiMwWgwwRERGZLQYZIiIiMlsMMkRERGS2GGSIiIjIbFmYuoDaJggCAPFx4ERERGQeys/b5efx+2nwQSY3NxcA4O3tbeJKiIiI6FHl5uZCrVbfd75EeFjUMXN6vR7Jycmws7ODRCKpsfVqNBp4e3sjKSkJKpWqxtbb2PA4Vh+PYc3gcaw+HsPq4zG8QxAE5ObmwtPTE1Lp/UfCNPgWGalUCi8vr1pbv0qlavRftprA41h9PIY1g8ex+ngMq4/HUPSglphyHOxLREREZotBhoiIiMwWg0wVKRQKzJ07FwqFwtSlmDUex+rjMawZPI7Vx2NYfTyGj67BD/YlIiKihostMkRERGS2GGSIiIjIbDHIEBERkdlikCEiIiKzxSBTRV9//TWaNm0KKysrhIWF4ejRo6Yuqd6aN28eJBKJ0SsoKMgwv6ioCFFRUXBycoJSqcTIkSORlpZmworrhwMHDmDIkCHw9PSERCLBxo0bjeYLgoA5c+bAw8MD1tbWiIiIwJUrV4yWyczMxOjRo6FSqWBvb4/x48cjLy+vDvfCtB52DMeOHVvhuzlgwACjZRr7MVy4cCE6deoEOzs7uLq6YtiwYYiNjTVapjI/w4mJiRg8eDBsbGzg6uqKN954A6WlpXW5KyZTmWPYu3fvCt/FSZMmGS3TmI/hgzDIVMEvv/yC119/HXPnzsXJkyfRtm1bREZGIj093dSl1VutWrVCSkqK4fX3338b5k2fPh2///471q1bh/379yM5ORkjRowwYbX1Q35+Ptq2bYuvv/76nvM//vhjLFq0CMuWLcORI0dga2uLyMhIFBUVGZYZPXo0Lly4gF27duGPP/7AgQMHMHHixLraBZN72DEEgAEDBhh9N9esWWM0v7Efw/379yMqKgqHDx/Grl27UFJSgv79+yM/P9+wzMN+hnU6HQYPHozi4mIcOnQIq1atwsqVKzFnzhxT7FKdq8wxBIAJEyYYfRc//vhjw7zGfgwfSKBH1rlzZyEqKsrwXqfTCZ6ensLChQtNWFX9NXfuXKFt27b3nJednS3I5XJh3bp1hmkxMTECACE6OrqOKqz/AAgbNmwwvNfr9YK7u7vwySefGKZlZ2cLCoVCWLNmjSAIgnDx4kUBgHDs2DHDMtu2bRMkEolw8+bNOqu9vrj7GAqCIIwZM0YYOnTofT/DY1hRenq6AEDYv3+/IAiV+xneunWrIJVKhdTUVMMyS5cuFVQqlaDVaut2B+qBu4+hIAhCr169hNdee+2+n+ExvD+2yDyi4uJinDhxAhEREYZpUqkUERERiI6ONmFl9duVK1fg6ekJf39/jB49GomJiQCAEydOoKSkxOh4BgUFwcfHh8fzAeLj45Gammp03NRqNcLCwgzHLTo6Gvb29ujYsaNhmYiICEilUhw5cqTOa66v9u3bB1dXV7Ro0QKTJ09GRkaGYR6PYUU5OTkAAEdHRwCV+xmOjo5GSEgI3NzcDMtERkZCo9HgwoULdVh9/XD3MSz3888/w9nZGa1bt8asWbNQUFBgmMdjeH8N/qGRNe327dvQ6XRGXyYAcHNzw6VLl0xUVf0WFhaGlStXokWLFkhJScH8+fPRo0cPnD9/HqmpqbC0tIS9vb3RZ9zc3JCammqags1A+bG51/ewfF5qaipcXV2N5ltYWMDR0ZHHtsyAAQMwYsQI+Pn54erVq/j3v/+NgQMHIjo6GjKZjMfwLnq9HtOmTUO3bt3QunVrAKjUz3Bqauo9v6vl8xqTex1DAHj22Wfh6+sLT09PnD17Fm+99RZiY2Oxfv16ADyGD8IgQ7Vu4MCBhr+3adMGYWFh8PX1xa+//gpra2sTVkaN3TPPPGP4e0hICNq0aYOAgADs27cP/fr1M2Fl9VNUVBTOnz9vNMaNHs39juE/x12FhITAw8MD/fr1w9WrVxEQEFDXZZoVdi09ImdnZ8hksgoj8tPS0uDu7m6iqsyLvb09mjdvjri4OLi7u6O4uBjZ2dlGy/B4Plj5sXnQ99Dd3b3CAPTS0lJkZmby2N6Hv78/nJ2dERcXB4DH8J+mTJmCP/74A3v37oWXl5dhemV+ht3d3e/5XS2f11jc7xjeS1hYGAAYfRd5DO+NQeYRWVpaIjQ0FHv27DFM0+v12LNnD8LDw01YmfnIy8vD1atX4eHhgdDQUMjlcqPjGRsbi8TERB7PB/Dz84O7u7vRcdNoNDhy5IjhuIWHhyM7OxsnTpwwLPPnn39Cr9cb/pMkYzdu3EBGRgY8PDwA8BgC4mX+U6ZMwYYNG/Dnn3/Cz8/PaH5lfobDw8Nx7tw5o1C4a9cuqFQqtGzZsm52xIQedgzv5fTp0wBg9F1szMfwgUw92tgcrV27VlAoFMLKlSuFixcvChMnThTs7e2NRpPTHTNmzBD27dsnxMfHCwcPHhQiIiIEZ2dnIT09XRAEQZg0aZLg4+Mj/Pnnn8Lx48eF8PBwITw83MRVm15ubq5w6tQp4dSpUwIA4fPPPxdOnTolJCQkCIIgCB999JFgb28vbNq0STh79qwwdOhQwc/PTygsLDSsY8CAAUL79u2FI0eOCH///bcQGBgojBo1ylS7VOcedAxzc3OFmTNnCtHR0UJ8fLywe/duoUOHDkJgYKBQVFRkWEdjP4aTJ08W1Gq1sG/fPiElJcXwKigoMCzzsJ/h0tJSoXXr1kL//v2F06dPC9u3bxdcXFyEWbNmmWKX6tzDjmFcXJzw3nvvCcePHxfi4+OFTZs2Cf7+/kLPnj0N62jsx/BBGGSqaPHixYKPj49gaWkpdO7cWTh8+LCpS6q3nn76acHDw0OwtLQUmjRpIjz99NNCXFycYX5hYaHwyiuvCA4ODoKNjY0wfPhwISUlxYQV1w979+4VAFR4jRkzRhAE8RLs2bNnC25uboJCoRD69esnxMbGGq0jIyNDGDVqlKBUKgWVSiWMGzdOyM3NNcHemMaDjmFBQYHQv39/wcXFRZDL5YKvr68wYcKECr+QNPZjeK/jB0BYsWKFYZnK/Axfv35dGDhwoGBtbS04OzsLM2bMEEpKSup4b0zjYccwMTFR6Nmzp+Do6CgoFAqhWbNmwhtvvCHk5OQYracxH8MHkQiCINRd+w8RERFRzeEYGSIiIjJbDDJERERkthhkiIiIyGwxyBAREZHZYpAhIiIis8UgQ0RERGaLQYaIiIjMFoMMETU6EokEGzduNHUZRFQDGGSIqE6NHTsWEomkwmvAgAGmLo2IzJCFqQsgosZnwIABWLFihdE0hUJhomqIyJyxRYaI6pxCoYC7u7vRy8HBAYDY7bN06VIMHDgQ1tbW8Pf3x2+//Wb0+XPnzqFv376wtraGk5MTJk6ciLy8PKNl/vvf/6JVq1ZQKBTw8PDAlClTjObfvn0bw4cPh42NDQIDA7F58+ba3WkiqhUMMkRU78yePRsjR47EmTNnMHr0aDzzzDOIiYkBAOTn5yMyMhIODg44duwY1q1bh927dxsFlaVLlyIqKgoTJ07EuXPnsHnzZjRr1sxoG/Pnz8dTTz2Fs2fPYtCgQRg9ejQyMzPrdD+JqAaY+qmVRNS4jBkzRpDJZIKtra3R64MPPhAEQXxS8KRJk4w+ExYWJkyePFkQBEH49ttvBQcHByEvL88wf8uWLYJUKjU8udrT01N455137lsDAOHdd981vM/LyxMACNu2baux/SSiusExMkRU5/r06YOlS5caTXN0dDT8PTw83GheeHg4Tp8+DQCIiYlB27ZtYWtra5jfrVs36PV6xMbGQiKRIDk5Gf369XtgDW3atDH83dbWFiqVCunp6VXdJSIyEQYZIqpztra2Fbp6aoq1tXWllpPL5UbvJRIJ9Hp9bZRERLWIY2SIqN45fPhwhffBwcEAgODgYJw5cwb5+fmG+QcPHoRUKkWLFi1gZ2eHpk2bYs+ePXVaMxGZBltkiKjOabVapKamGk2zsLCAs7MzAGDdunXo2LEjunfvjp9//hlHjx7F8uXLAQCjR4/G3LlzMWbMGMybNw+3bt3C1KlT8fzzz8PNzQ0AMG/ePEyaNAmurq4YOHAgcnNzcfDgQUydOrVud5SIah2DDBHVue3bt8PDw8NoWosWLXDp0iUA4hVFa9euxSuvvAIPDw+sWbMGLVu2BADY2Nhgx44deO2119CpUyfY2Nhg5MiR+Pzzzw3rGjNmDIqKivDFF19g5syZcHZ2xhNPPFF3O0hEdUYiCIJg6iKIiMpJJBJs2LABw4YNM3UpRGQGOEaGiIiIzBaDDBEREZktjpEhonqFvd1E9CjYIkNERERmi0GGiIiIzBaDDBEREZktBhkiIiIyWwwyREREZLYYZIiIiMhsMcgQERGR2WKQISIiIrPFIENERERm6/8BtksBFZ7eKU8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_2l_mutag.pth\n",
      "Average Time per Epoch: 0.03s\n",
      "Average CPU Usage: 28.03%\n",
      "Average Memory Usage: 3.34GB\n",
      "Average GPU Usage: 0.34GB\n",
      "Average GPU Utilization: 3.35%\n",
      "\n",
      "Total Training Time: 9.49s\n",
      "Max CPU Usage: 64.35%\n",
      "Max Memory Usage: 3.34GB\n",
      "Max GPU Usage: 0.34GB\n",
      "Max GPU Utilization: 5.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn2_mutag = GCN2Layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes)\n",
    "print(gcn2_mutag)\n",
    "print(f\"Total number of trainable parameters: {(gcn2_mutag.count_parameters())*2}\\n\")\n",
    "single_train(gcn2_mutag, mutag_train_loader, mutag_val_loader, lr=0.01, num_epochs=500, step_size=500, save_path='models/gcn_2l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211\n",
      "Average Sensitivity (Recall): 0.8846\n",
      "Average Specificity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "single_test(gcn2_mutag, mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1508\n",
      "\n",
      "Epoch 1, Train Loss: 71.56606495380402, Val Loss: 7.8566460609436035\n",
      "Time: 0.07s, CPU: 17.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 68.20633161067963, Val Loss: 7.5821457505226135\n",
      "Time: 0.06s, CPU: 26.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 1.50%\n",
      "Epoch 3, Train Loss: 66.41744589805603, Val Loss: 7.683176159858704\n",
      "Time: 0.06s, CPU: 25.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 4, Train Loss: 66.86147153377533, Val Loss: 7.593149483203888\n",
      "Time: 0.07s, CPU: 23.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 5, Train Loss: 66.37889873981476, Val Loss: 7.639395236968994\n",
      "Time: 0.06s, CPU: 21.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 6, Train Loss: 66.55304217338562, Val Loss: 7.621022880077362\n",
      "Time: 0.07s, CPU: 21.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 7, Train Loss: 66.47210013866425, Val Loss: 7.626465797424316\n",
      "Time: 0.06s, CPU: 30.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 8, Train Loss: 66.42699337005615, Val Loss: 7.636011183261871\n",
      "Time: 0.06s, CPU: 28.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 9, Train Loss: 66.44254720211029, Val Loss: 7.634184539318085\n",
      "Time: 0.05s, CPU: 23.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 10, Train Loss: 66.3828272819519, Val Loss: 7.643800735473633\n",
      "Time: 0.06s, CPU: 37.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 11, Train Loss: 66.35438787937164, Val Loss: 7.65119743347168\n",
      "Time: 0.06s, CPU: 24.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 12, Train Loss: 66.29779040813446, Val Loss: 7.660110414028168\n",
      "Time: 0.07s, CPU: 22.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 13, Train Loss: 66.25069856643677, Val Loss: 7.665300071239471\n",
      "Time: 0.06s, CPU: 26.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 14, Train Loss: 66.18305695056915, Val Loss: 7.6774187088012695\n",
      "Time: 0.05s, CPU: 26.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 15, Train Loss: 66.12681233882904, Val Loss: 7.694367170333862\n",
      "Time: 0.06s, CPU: 37.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 16, Train Loss: 66.02928268909454, Val Loss: 7.712449669837952\n",
      "Time: 0.06s, CPU: 24.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 17, Train Loss: 65.93350028991699, Val Loss: 7.730756759643555\n",
      "Time: 0.06s, CPU: 25.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 18, Train Loss: 65.83479881286621, Val Loss: 7.751762390136719\n",
      "Time: 0.05s, CPU: 24.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 19, Train Loss: 65.74845278263092, Val Loss: 7.777189195156097\n",
      "Time: 0.06s, CPU: 22.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 20, Train Loss: 65.66269707679749, Val Loss: 7.8093178272247314\n",
      "Time: 0.06s, CPU: 26.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 21, Train Loss: 65.56019496917725, Val Loss: 7.834611177444458\n",
      "Time: 0.06s, CPU: 39.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 22, Train Loss: 65.42757630348206, Val Loss: 7.847656548023224\n",
      "Time: 0.06s, CPU: 24.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 23, Train Loss: 65.31892013549805, Val Loss: 7.870755136013031\n",
      "Time: 0.05s, CPU: 24.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 24, Train Loss: 65.21144950389862, Val Loss: 7.898065209388733\n",
      "Time: 0.05s, CPU: 20.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 25, Train Loss: 65.04088687896729, Val Loss: 7.932383596897125\n",
      "Time: 0.07s, CPU: 21.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 26, Train Loss: 64.92694568634033, Val Loss: 7.946376025676727\n",
      "Time: 0.06s, CPU: 23.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 27, Train Loss: 64.7970324754715, Val Loss: 7.959762215614319\n",
      "Time: 0.05s, CPU: 28.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 28, Train Loss: 64.6058509349823, Val Loss: 7.984940707683563\n",
      "Time: 0.05s, CPU: 26.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 29, Train Loss: 64.45126211643219, Val Loss: 8.00045919418335\n",
      "Time: 0.05s, CPU: 27.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 30, Train Loss: 64.14468133449554, Val Loss: 8.02693921327591\n",
      "Time: 0.06s, CPU: 24.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 31, Train Loss: 63.0821567773819, Val Loss: 8.010631382465363\n",
      "Time: 0.07s, CPU: 48.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 32, Train Loss: 62.92669141292572, Val Loss: 8.020021438598633\n",
      "Time: 0.10s, CPU: 34.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 33, Train Loss: 62.83062970638275, Val Loss: 8.03582501411438\n",
      "Time: 0.07s, CPU: 24.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 34, Train Loss: 62.80440545082092, Val Loss: 8.046227633953094\n",
      "Time: 0.05s, CPU: 21.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 35, Train Loss: 62.74710559844971, Val Loss: 8.055936396121979\n",
      "Time: 0.06s, CPU: 19.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 36, Train Loss: 62.682087659835815, Val Loss: 8.06651109457016\n",
      "Time: 0.06s, CPU: 23.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 37, Train Loss: 62.63132667541504, Val Loss: 8.076099753379822\n",
      "Time: 0.08s, CPU: 23.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 38, Train Loss: 62.578999161720276, Val Loss: 8.085631549358368\n",
      "Time: 0.07s, CPU: 20.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 39, Train Loss: 62.52431499958038, Val Loss: 8.095424830913544\n",
      "Time: 0.06s, CPU: 36.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 40, Train Loss: 62.470741391181946, Val Loss: 8.104083716869354\n",
      "Time: 0.06s, CPU: 29.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 41, Train Loss: 62.417243123054504, Val Loss: 8.113160610198975\n",
      "Time: 0.07s, CPU: 22.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 42, Train Loss: 62.362364411354065, Val Loss: 8.122471034526825\n",
      "Time: 0.06s, CPU: 33.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 43, Train Loss: 62.30995810031891, Val Loss: 8.130831003189087\n",
      "Time: 0.06s, CPU: 28.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 44, Train Loss: 62.25412034988403, Val Loss: 8.14054125547409\n",
      "Time: 0.06s, CPU: 30.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 45, Train Loss: 62.1987110376358, Val Loss: 8.150600910186768\n",
      "Time: 0.06s, CPU: 23.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 46, Train Loss: 62.14663314819336, Val Loss: 8.159420847892761\n",
      "Time: 0.06s, CPU: 21.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 47, Train Loss: 62.09247040748596, Val Loss: 8.168413996696472\n",
      "Time: 0.05s, CPU: 24.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 48, Train Loss: 62.03883385658264, Val Loss: 8.176904439926147\n",
      "Time: 0.07s, CPU: 22.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 49, Train Loss: 61.985321402549744, Val Loss: 8.186121702194214\n",
      "Time: 0.06s, CPU: 21.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 50, Train Loss: 61.92580306529999, Val Loss: 8.195370852947235\n",
      "Time: 0.06s, CPU: 28.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 51, Train Loss: 61.87095856666565, Val Loss: 8.204080939292908\n",
      "Time: 0.06s, CPU: 24.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 52, Train Loss: 61.81245529651642, Val Loss: 8.21248471736908\n",
      "Time: 0.07s, CPU: 18.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 53, Train Loss: 61.75274932384491, Val Loss: 8.221361994743347\n",
      "Time: 0.06s, CPU: 24.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 54, Train Loss: 61.69422745704651, Val Loss: 8.231541991233826\n",
      "Time: 0.07s, CPU: 24.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 55, Train Loss: 61.638254284858704, Val Loss: 8.240476071834564\n",
      "Time: 0.07s, CPU: 20.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 56, Train Loss: 61.58094394207001, Val Loss: 8.249671339988708\n",
      "Time: 0.07s, CPU: 32.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 57, Train Loss: 61.52298700809479, Val Loss: 8.258466243743896\n",
      "Time: 0.07s, CPU: 38.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 58, Train Loss: 61.46646296977997, Val Loss: 8.267918288707733\n",
      "Time: 0.05s, CPU: 39.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 59, Train Loss: 61.40185749530792, Val Loss: 8.277461171150208\n",
      "Time: 0.06s, CPU: 37.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 60, Train Loss: 61.341580390930176, Val Loss: 8.286508083343506\n",
      "Time: 0.06s, CPU: 46.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 61, Train Loss: 61.23192870616913, Val Loss: 8.287185192108154\n",
      "Time: 0.05s, CPU: 31.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 62, Train Loss: 61.22475063800812, Val Loss: 8.287682354450226\n",
      "Time: 0.07s, CPU: 37.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 63, Train Loss: 61.21622943878174, Val Loss: 8.288259983062744\n",
      "Time: 0.06s, CPU: 25.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.50%\n",
      "Epoch 64, Train Loss: 61.207823157310486, Val Loss: 8.28898960351944\n",
      "Time: 0.06s, CPU: 23.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 2.00%\n",
      "Epoch 65, Train Loss: 61.20009768009186, Val Loss: 8.289759635925293\n",
      "Time: 0.05s, CPU: 23.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 66, Train Loss: 61.192832589149475, Val Loss: 8.29050999879837\n",
      "Time: 0.06s, CPU: 27.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 67, Train Loss: 61.18608582019806, Val Loss: 8.291286408901215\n",
      "Time: 0.06s, CPU: 25.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 68, Train Loss: 61.179532170295715, Val Loss: 8.292202115058899\n",
      "Time: 0.06s, CPU: 22.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 69, Train Loss: 61.17340135574341, Val Loss: 8.293159544467926\n",
      "Time: 0.06s, CPU: 23.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 70, Train Loss: 61.16686391830444, Val Loss: 8.294037520885468\n",
      "Time: 0.06s, CPU: 27.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 71, Train Loss: 61.160537004470825, Val Loss: 8.294801473617554\n",
      "Time: 0.05s, CPU: 25.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 72, Train Loss: 61.15397870540619, Val Loss: 8.29562622308731\n",
      "Time: 0.06s, CPU: 25.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 73, Train Loss: 61.14755594730377, Val Loss: 8.296653628349304\n",
      "Time: 0.05s, CPU: 28.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 74, Train Loss: 61.14115262031555, Val Loss: 8.297840654850006\n",
      "Time: 0.06s, CPU: 32.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 75, Train Loss: 61.13526403903961, Val Loss: 8.298965394496918\n",
      "Time: 0.06s, CPU: 19.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 76, Train Loss: 61.129019141197205, Val Loss: 8.299893736839294\n",
      "Time: 0.06s, CPU: 20.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 77, Train Loss: 61.122628688812256, Val Loss: 8.300875723361969\n",
      "Time: 0.06s, CPU: 24.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 78, Train Loss: 61.116613030433655, Val Loss: 8.301930248737335\n",
      "Time: 0.07s, CPU: 28.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 79, Train Loss: 61.11009621620178, Val Loss: 8.302925765514374\n",
      "Time: 0.07s, CPU: 22.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 80, Train Loss: 61.10411810874939, Val Loss: 8.303982198238373\n",
      "Time: 0.06s, CPU: 27.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 81, Train Loss: 61.09763753414154, Val Loss: 8.305062413215637\n",
      "Time: 0.05s, CPU: 24.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 82, Train Loss: 61.09163439273834, Val Loss: 8.306163907051086\n",
      "Time: 0.07s, CPU: 28.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 83, Train Loss: 61.08519089221954, Val Loss: 8.307192325592041\n",
      "Time: 0.05s, CPU: 22.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 84, Train Loss: 61.078848123550415, Val Loss: 8.308288395404816\n",
      "Time: 0.05s, CPU: 19.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 85, Train Loss: 61.072670698165894, Val Loss: 8.309281408786774\n",
      "Time: 0.06s, CPU: 23.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 86, Train Loss: 61.06631636619568, Val Loss: 8.310216307640076\n",
      "Time: 0.06s, CPU: 25.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 87, Train Loss: 61.060033202171326, Val Loss: 8.311296343803406\n",
      "Time: 0.05s, CPU: 23.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 88, Train Loss: 61.05389368534088, Val Loss: 8.312398970127106\n",
      "Time: 0.06s, CPU: 28.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 89, Train Loss: 61.047547698020935, Val Loss: 8.313301622867584\n",
      "Time: 0.05s, CPU: 16.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 90, Train Loss: 61.04101610183716, Val Loss: 8.31434565782547\n",
      "Time: 0.05s, CPU: 13.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 91, Train Loss: 61.030649065971375, Val Loss: 8.314450979232788\n",
      "Time: 0.05s, CPU: 6.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 92, Train Loss: 61.03001523017883, Val Loss: 8.31455647945404\n",
      "Time: 0.05s, CPU: 11.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 93, Train Loss: 61.02935469150543, Val Loss: 8.314640879631042\n",
      "Time: 0.05s, CPU: 12.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 94, Train Loss: 61.02868926525116, Val Loss: 8.314727544784546\n",
      "Time: 0.06s, CPU: 7.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 95, Train Loss: 61.028026819229126, Val Loss: 8.314826369285583\n",
      "Time: 0.05s, CPU: 14.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 96, Train Loss: 61.02738392353058, Val Loss: 8.314932942390442\n",
      "Time: 0.05s, CPU: 10.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 97, Train Loss: 61.02671730518341, Val Loss: 8.315031170845032\n",
      "Time: 0.04s, CPU: 10.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 98, Train Loss: 61.026047587394714, Val Loss: 8.315139651298523\n",
      "Time: 0.04s, CPU: 15.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 99, Train Loss: 61.02540397644043, Val Loss: 8.315249621868134\n",
      "Time: 0.04s, CPU: 8.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 100, Train Loss: 61.02474021911621, Val Loss: 8.315362870693207\n",
      "Time: 0.05s, CPU: 8.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 101, Train Loss: 61.024104595184326, Val Loss: 8.315459549427032\n",
      "Time: 0.04s, CPU: 14.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 102, Train Loss: 61.02344036102295, Val Loss: 8.31555986404419\n",
      "Time: 0.05s, CPU: 12.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 103, Train Loss: 61.022775411605835, Val Loss: 8.315666139125824\n",
      "Time: 0.04s, CPU: 16.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 104, Train Loss: 61.02214562892914, Val Loss: 8.31577742099762\n",
      "Time: 0.04s, CPU: 12.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 105, Train Loss: 61.02147424221039, Val Loss: 8.315873920917511\n",
      "Time: 0.04s, CPU: 7.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 106, Train Loss: 61.020819544792175, Val Loss: 8.31598025560379\n",
      "Time: 0.04s, CPU: 10.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 107, Train Loss: 61.02015674114227, Val Loss: 8.316098272800446\n",
      "Time: 0.04s, CPU: 13.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 108, Train Loss: 61.01952075958252, Val Loss: 8.316196620464325\n",
      "Time: 0.04s, CPU: 5.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 109, Train Loss: 61.01888370513916, Val Loss: 8.316305220127106\n",
      "Time: 0.05s, CPU: 9.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 110, Train Loss: 61.01821684837341, Val Loss: 8.316412508487701\n",
      "Time: 0.05s, CPU: 17.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 111, Train Loss: 61.01757740974426, Val Loss: 8.316513001918793\n",
      "Time: 0.05s, CPU: 6.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 112, Train Loss: 61.016918301582336, Val Loss: 8.316622495651245\n",
      "Time: 0.06s, CPU: 9.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 113, Train Loss: 61.016287088394165, Val Loss: 8.316732704639435\n",
      "Time: 0.06s, CPU: 14.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 114, Train Loss: 61.01562011241913, Val Loss: 8.3168443441391\n",
      "Time: 0.05s, CPU: 12.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 115, Train Loss: 61.014999985694885, Val Loss: 8.316950023174286\n",
      "Time: 0.04s, CPU: 6.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 116, Train Loss: 61.014344453811646, Val Loss: 8.317060351371765\n",
      "Time: 0.05s, CPU: 42.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 117, Train Loss: 61.01368749141693, Val Loss: 8.317175030708313\n",
      "Time: 0.04s, CPU: 14.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 118, Train Loss: 61.0130478143692, Val Loss: 8.317280173301697\n",
      "Time: 0.05s, CPU: 8.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 119, Train Loss: 61.01239538192749, Val Loss: 8.317372798919678\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 120, Train Loss: 61.01174807548523, Val Loss: 8.31747978925705\n",
      "Time: 0.04s, CPU: 13.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 121, Train Loss: 61.01071894168854, Val Loss: 8.317495584487915\n",
      "Time: 0.06s, CPU: 13.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 122, Train Loss: 61.01065266132355, Val Loss: 8.317507266998291\n",
      "Time: 0.05s, CPU: 16.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 123, Train Loss: 61.010597705841064, Val Loss: 8.31751811504364\n",
      "Time: 0.05s, CPU: 11.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 124, Train Loss: 61.01052474975586, Val Loss: 8.317531645298004\n",
      "Time: 0.05s, CPU: 12.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 125, Train Loss: 61.010456919670105, Val Loss: 8.317544162273407\n",
      "Time: 0.04s, CPU: 10.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 126, Train Loss: 61.01039457321167, Val Loss: 8.317557871341705\n",
      "Time: 0.05s, CPU: 10.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 127, Train Loss: 61.01032042503357, Val Loss: 8.317569136619568\n",
      "Time: 0.04s, CPU: 15.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 128, Train Loss: 61.010258197784424, Val Loss: 8.317578077316284\n",
      "Time: 0.05s, CPU: 13.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 129, Train Loss: 61.01018929481506, Val Loss: 8.317591428756714\n",
      "Time: 0.05s, CPU: 13.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 130, Train Loss: 61.01012873649597, Val Loss: 8.317603468894958\n",
      "Time: 0.04s, CPU: 7.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 131, Train Loss: 61.01006376743317, Val Loss: 8.317617118358612\n",
      "Time: 0.04s, CPU: 11.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 132, Train Loss: 61.009998202323914, Val Loss: 8.317626535892487\n",
      "Time: 0.05s, CPU: 7.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 133, Train Loss: 61.00993800163269, Val Loss: 8.31764006614685\n",
      "Time: 0.04s, CPU: 12.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 134, Train Loss: 61.00987648963928, Val Loss: 8.31765478849411\n",
      "Time: 0.04s, CPU: 6.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 135, Train Loss: 61.009801745414734, Val Loss: 8.31766539812088\n",
      "Time: 0.05s, CPU: 11.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 136, Train Loss: 61.00973880290985, Val Loss: 8.317675352096558\n",
      "Time: 0.07s, CPU: 24.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 137, Train Loss: 61.00966775417328, Val Loss: 8.317687451839447\n",
      "Time: 0.06s, CPU: 33.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 138, Train Loss: 61.00960040092468, Val Loss: 8.317696869373322\n",
      "Time: 0.06s, CPU: 18.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 139, Train Loss: 61.00953733921051, Val Loss: 8.317706823348999\n",
      "Time: 0.06s, CPU: 32.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 140, Train Loss: 61.00947713851929, Val Loss: 8.31772094964981\n",
      "Time: 0.05s, CPU: 29.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 141, Train Loss: 61.00940418243408, Val Loss: 8.317728698253632\n",
      "Time: 0.05s, CPU: 21.85%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 142, Train Loss: 61.009339928627014, Val Loss: 8.317743599414825\n",
      "Time: 0.05s, CPU: 23.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 143, Train Loss: 61.009268283843994, Val Loss: 8.317750632762909\n",
      "Time: 0.05s, CPU: 11.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 144, Train Loss: 61.009209513664246, Val Loss: 8.317761540412903\n",
      "Time: 0.05s, CPU: 10.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 145, Train Loss: 61.009137868881226, Val Loss: 8.317774713039398\n",
      "Time: 0.06s, CPU: 16.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 146, Train Loss: 61.009077310562134, Val Loss: 8.31778484582901\n",
      "Time: 0.05s, CPU: 23.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 147, Train Loss: 61.009013414382935, Val Loss: 8.317796051502228\n",
      "Time: 0.06s, CPU: 12.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 148, Train Loss: 61.008947014808655, Val Loss: 8.31780856847763\n",
      "Time: 0.05s, CPU: 42.65%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 149, Train Loss: 61.00888204574585, Val Loss: 8.317817449569702\n",
      "Time: 0.05s, CPU: 9.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 150, Train Loss: 61.00881254673004, Val Loss: 8.31783276796341\n",
      "Time: 0.05s, CPU: 9.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 151, Train Loss: 61.008713126182556, Val Loss: 8.31783264875412\n",
      "Time: 0.05s, CPU: 13.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 152, Train Loss: 61.008711099624634, Val Loss: 8.31783276796341\n",
      "Time: 0.05s, CPU: 9.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 153, Train Loss: 61.008702635765076, Val Loss: 8.317832469940186\n",
      "Time: 0.05s, CPU: 6.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 154, Train Loss: 61.00869536399841, Val Loss: 8.317837119102478\n",
      "Time: 0.06s, CPU: 11.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 155, Train Loss: 61.00869393348694, Val Loss: 8.317834854125977\n",
      "Time: 0.05s, CPU: 14.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 156, Train Loss: 61.008681774139404, Val Loss: 8.317837953567505\n",
      "Time: 0.04s, CPU: 11.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 157, Train Loss: 61.00868463516235, Val Loss: 8.31784063577652\n",
      "Time: 0.05s, CPU: 16.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 158, Train Loss: 61.00867819786072, Val Loss: 8.317841172218323\n",
      "Time: 0.05s, CPU: 15.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 159, Train Loss: 61.00867521762848, Val Loss: 8.31784063577652\n",
      "Time: 0.05s, CPU: 22.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 160, Train Loss: 61.00867319107056, Val Loss: 8.317843794822693\n",
      "Time: 0.04s, CPU: 12.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 161, Train Loss: 61.00865912437439, Val Loss: 8.317843556404114\n",
      "Time: 0.04s, CPU: 12.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 162, Train Loss: 61.0086544752121, Val Loss: 8.317845106124878\n",
      "Time: 0.05s, CPU: 6.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 163, Train Loss: 61.00864827632904, Val Loss: 8.317846417427063\n",
      "Time: 0.05s, CPU: 21.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 164, Train Loss: 61.00864803791046, Val Loss: 8.317847609519958\n",
      "Time: 0.05s, CPU: 9.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 165, Train Loss: 61.00864088535309, Val Loss: 8.317845821380615\n",
      "Time: 0.04s, CPU: 16.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 166, Train Loss: 61.0086315870285, Val Loss: 8.31784850358963\n",
      "Time: 0.05s, CPU: 13.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 167, Train Loss: 61.008625626564026, Val Loss: 8.317849814891815\n",
      "Time: 0.04s, CPU: 6.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 168, Train Loss: 61.00862467288971, Val Loss: 8.317851662635803\n",
      "Time: 0.05s, CPU: 19.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 169, Train Loss: 61.00861442089081, Val Loss: 8.317852437496185\n",
      "Time: 0.05s, CPU: 22.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 170, Train Loss: 61.00861179828644, Val Loss: 8.317852854728699\n",
      "Time: 0.05s, CPU: 21.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 171, Train Loss: 61.008607387542725, Val Loss: 8.317855894565582\n",
      "Time: 0.05s, CPU: 11.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 172, Train Loss: 61.00860261917114, Val Loss: 8.317859053611755\n",
      "Time: 0.05s, CPU: 12.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 173, Train Loss: 61.008593916893005, Val Loss: 8.317857146263123\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 174, Train Loss: 61.008588910102844, Val Loss: 8.317857503890991\n",
      "Time: 0.04s, CPU: 11.45%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 175, Train Loss: 61.008583545684814, Val Loss: 8.317857265472412\n",
      "Time: 0.04s, CPU: 12.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 176, Train Loss: 61.008580446243286, Val Loss: 8.317861199378967\n",
      "Time: 0.04s, CPU: 11.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 177, Train Loss: 61.00857663154602, Val Loss: 8.3178631067276\n",
      "Time: 0.04s, CPU: 10.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 178, Train Loss: 61.008570194244385, Val Loss: 8.317863821983337\n",
      "Time: 0.05s, CPU: 12.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 179, Train Loss: 61.00856637954712, Val Loss: 8.31786334514618\n",
      "Time: 0.05s, CPU: 12.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 180, Train Loss: 61.00856137275696, Val Loss: 8.317866146564484\n",
      "Time: 0.05s, CPU: 11.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 181, Train Loss: 61.00855302810669, Val Loss: 8.317866384983063\n",
      "Time: 0.04s, CPU: 12.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 182, Train Loss: 61.008551836013794, Val Loss: 8.31786572933197\n",
      "Time: 0.04s, CPU: 10.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 183, Train Loss: 61.00855267047882, Val Loss: 8.317865788936615\n",
      "Time: 0.05s, CPU: 27.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 184, Train Loss: 61.008551359176636, Val Loss: 8.317866265773773\n",
      "Time: 0.04s, CPU: 16.05%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 185, Train Loss: 61.00855207443237, Val Loss: 8.317865669727325\n",
      "Time: 0.05s, CPU: 14.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 186, Train Loss: 61.008551836013794, Val Loss: 8.31786447763443\n",
      "Time: 0.05s, CPU: 20.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 187, Train Loss: 61.00855326652527, Val Loss: 8.317865788936615\n",
      "Time: 0.05s, CPU: 15.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 188, Train Loss: 61.0085506439209, Val Loss: 8.31786561012268\n",
      "Time: 0.06s, CPU: 17.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 189, Train Loss: 61.00855267047882, Val Loss: 8.317865133285522\n",
      "Time: 0.05s, CPU: 10.80%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 190, Train Loss: 61.008551239967346, Val Loss: 8.317866325378418\n",
      "Time: 0.05s, CPU: 16.40%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 191, Train Loss: 61.00855088233948, Val Loss: 8.317865550518036\n",
      "Time: 0.05s, CPU: 14.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 192, Train Loss: 61.008551597595215, Val Loss: 8.317865669727325\n",
      "Time: 0.06s, CPU: 7.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 193, Train Loss: 61.00855028629303, Val Loss: 8.317864954471588\n",
      "Time: 0.05s, CPU: 24.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 194, Train Loss: 61.00855088233948, Val Loss: 8.31786572933197\n",
      "Time: 0.05s, CPU: 13.35%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 195, Train Loss: 61.00854957103729, Val Loss: 8.31786459684372\n",
      "Time: 0.05s, CPU: 14.95%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 196, Train Loss: 61.008551478385925, Val Loss: 8.317867040634155\n",
      "Time: 0.04s, CPU: 12.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 197, Train Loss: 61.00855267047882, Val Loss: 8.317866384983063\n",
      "Time: 0.05s, CPU: 7.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.50%\n",
      "Epoch 198, Train Loss: 61.00855207443237, Val Loss: 8.317867159843445\n",
      "Time: 0.05s, CPU: 16.60%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 199, Train Loss: 61.00854992866516, Val Loss: 8.317867159843445\n",
      "Time: 0.05s, CPU: 13.90%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 200, Train Loss: 61.0085483789444, Val Loss: 8.317866146564484\n",
      "Time: 0.05s, CPU: 15.25%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 201, Train Loss: 61.00854825973511, Val Loss: 8.317866265773773\n",
      "Time: 0.05s, CPU: 12.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 6.00%\n",
      "Epoch 202, Train Loss: 61.008548974990845, Val Loss: 8.317865550518036\n",
      "Time: 0.06s, CPU: 12.50%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 203, Train Loss: 61.008548617362976, Val Loss: 8.317865550518036\n",
      "Time: 0.06s, CPU: 29.70%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 3.00%\n",
      "Epoch 204, Train Loss: 61.00854682922363, Val Loss: 8.317865669727325\n",
      "Time: 0.06s, CPU: 23.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 205, Train Loss: 61.008546590805054, Val Loss: 8.317866384983063\n",
      "Time: 0.07s, CPU: 23.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 206, Train Loss: 61.00854957103729, Val Loss: 8.317866265773773\n",
      "Time: 0.07s, CPU: 27.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 207, Train Loss: 61.00854957103729, Val Loss: 8.317866265773773\n",
      "Time: 0.07s, CPU: 26.55%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 208, Train Loss: 61.008551597595215, Val Loss: 8.317866742610931\n",
      "Time: 0.05s, CPU: 28.20%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.00%\n",
      "Epoch 209, Train Loss: 61.008551359176636, Val Loss: 8.317866384983063\n",
      "Time: 0.06s, CPU: 32.75%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 4.50%\n",
      "Epoch 210, Train Loss: 61.008554458618164, Val Loss: 8.317865788936615\n",
      "Time: 0.06s, CPU: 28.30%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 211, Train Loss: 61.00855052471161, Val Loss: 8.31786572933197\n",
      "Time: 0.05s, CPU: 23.10%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 212, Train Loss: 61.00855088233948, Val Loss: 8.31786572933197\n",
      "Time: 0.06s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 213, Train Loss: 61.00855207443237, Val Loss: 8.317865669727325\n",
      "Time: 0.06s, CPU: 25.00%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Epoch 214, Train Loss: 61.008551478385925, Val Loss: 8.31786572933197\n",
      "Time: 0.06s, CPU: 29.15%, Memory: 3.34GB, GPU: 0.34GB, GPU Util: 5.00%\n",
      "Early stopping at epoch 215\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_2l_emci.pth\n",
      "Average Time per Epoch: 0.05s\n",
      "Average CPU Usage: 20.03%\n",
      "Average Memory Usage: 3.34GB\n",
      "Average GPU Usage: 0.34GB\n",
      "Average GPU Utilization: 4.72%\n",
      "\n",
      "Total Training Time: 11.65s\n",
      "Max CPU Usage: 48.25%\n",
      "Max Memory Usage: 3.34GB\n",
      "Max GPU Usage: 0.34GB\n",
      "Max GPU Utilization: 6.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn2_emci = GCN2Layer(emci_num_features, 2*emci_num_features, emci_num_classes)\n",
    "print(gcn2_emci)\n",
    "print(f\"Total number of trainable parameters: {(gcn2_emci.count_parameters())*2}\\n\")\n",
    "single_train(gcn2_emci, emci_train_loader, emci_val_loader, \n",
    "            lr=0.01, num_epochs=500, step_size=30, patience=10, save_path='models/gcn_2l_emci.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5556\n",
      "Average Sensitivity (Recall): 0.5455\n",
      "Average Specificity: 0.5625\n",
      "\n",
      "Average Inference Time per Batch: 0.0005s\n",
      "Average CPU Usage: 14.90%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.13GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "gcn2_emci = GCN2Layer(emci_num_features, 2*emci_num_features, emci_num_classes)\n",
    "gcn2_emci.load_state_dict(torch.load('models/gcn_2l_emci.pth'))\n",
    "single_test(gcn2_emci.to(device), emci_test_loader)\n",
    "inference_performance(gcn2_emci.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1574\n",
      "\n",
      "Epoch 1, Train Loss: 508.5967903137207, Val Loss: 39.408928871154785\n",
      "Time: 0.22s, CPU: 40.60%, Memory: 3.36GB, GPU: 0.45GB, GPU Util: 4.00%\n",
      "Epoch 2, Train Loss: 278.14805006980896, Val Loss: 32.664613008499146\n",
      "Time: 0.15s, CPU: 51.15%, Memory: 3.36GB, GPU: 0.49GB, GPU Util: 13.00%\n",
      "Epoch 3, Train Loss: 258.1234874725342, Val Loss: 31.4747132062912\n",
      "Time: 0.13s, CPU: 53.50%, Memory: 3.37GB, GPU: 0.49GB, GPU Util: 20.00%\n",
      "Epoch 4, Train Loss: 241.13186943531036, Val Loss: 31.333887577056885\n",
      "Time: 0.14s, CPU: 43.65%, Memory: 3.39GB, GPU: 0.49GB, GPU Util: 22.50%\n",
      "Epoch 5, Train Loss: 240.39900135993958, Val Loss: 30.568740248680115\n",
      "Time: 0.14s, CPU: 48.30%, Memory: 3.37GB, GPU: 0.49GB, GPU Util: 22.00%\n",
      "Epoch 6, Train Loss: 229.95044255256653, Val Loss: 30.631211042404175\n",
      "Time: 0.35s, CPU: 34.30%, Memory: 3.38GB, GPU: 0.33GB, GPU Util: 10.50%\n",
      "Epoch 7, Train Loss: 228.39845669269562, Val Loss: 31.37990140914917\n",
      "Time: 0.13s, CPU: 44.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 9.00%\n",
      "Epoch 8, Train Loss: 229.8859989643097, Val Loss: 31.359842777252197\n",
      "Time: 0.13s, CPU: 55.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 9, Train Loss: 225.9438121318817, Val Loss: 30.016162276268005\n",
      "Time: 0.13s, CPU: 54.25%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 10, Train Loss: 216.48714327812195, Val Loss: 29.61976933479309\n",
      "Time: 0.12s, CPU: 47.60%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 11, Train Loss: 207.423588514328, Val Loss: 30.023343086242676\n",
      "Time: 0.12s, CPU: 42.20%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.50%\n",
      "Epoch 12, Train Loss: 206.41246902942657, Val Loss: 30.625739336013794\n",
      "Time: 0.13s, CPU: 43.00%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 13, Train Loss: 212.7642560005188, Val Loss: 30.179837107658386\n",
      "Time: 0.11s, CPU: 41.35%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.00%\n",
      "Epoch 14, Train Loss: 216.83119475841522, Val Loss: 32.98933947086334\n",
      "Time: 0.12s, CPU: 48.00%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 15, Train Loss: 216.55220210552216, Val Loss: 29.28573775291443\n",
      "Time: 0.16s, CPU: 59.25%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 16, Train Loss: 212.2523764371872, Val Loss: 28.44532871246338\n",
      "Time: 0.16s, CPU: 45.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 17, Train Loss: 209.8536273241043, Val Loss: 29.32589042186737\n",
      "Time: 0.12s, CPU: 41.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.50%\n",
      "Epoch 18, Train Loss: 194.58524322509766, Val Loss: 30.151718974113464\n",
      "Time: 0.12s, CPU: 44.20%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 19, Train Loss: 190.33886569738388, Val Loss: 29.282844185829163\n",
      "Time: 0.13s, CPU: 42.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 20, Train Loss: 196.5290065407753, Val Loss: 30.244322776794434\n",
      "Time: 0.12s, CPU: 47.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 21, Train Loss: 202.25500583648682, Val Loss: 34.06154930591583\n",
      "Time: 0.16s, CPU: 55.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 22, Train Loss: 208.62499487400055, Val Loss: 29.999940276145935\n",
      "Time: 0.12s, CPU: 56.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.00%\n",
      "Epoch 23, Train Loss: 190.89293217658997, Val Loss: 29.20633363723755\n",
      "Time: 0.14s, CPU: 55.80%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 24, Train Loss: 176.56161308288574, Val Loss: 29.537143349647522\n",
      "Time: 0.12s, CPU: 53.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.00%\n",
      "Epoch 25, Train Loss: 178.72099405527115, Val Loss: 29.39920485019684\n",
      "Time: 0.15s, CPU: 41.10%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 26, Train Loss: 183.26042139530182, Val Loss: 30.29635226726532\n",
      "Time: 0.12s, CPU: 47.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 27, Train Loss: 179.62852561473846, Val Loss: 30.26479661464691\n",
      "Time: 0.12s, CPU: 54.90%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.50%\n",
      "Epoch 28, Train Loss: 172.1364792585373, Val Loss: 29.20556116104126\n",
      "Time: 0.14s, CPU: 58.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 29, Train Loss: 167.8334423303604, Val Loss: 29.317206501960754\n",
      "Time: 0.13s, CPU: 38.90%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 30, Train Loss: 166.29684698581696, Val Loss: 28.7011696100235\n",
      "Time: 0.17s, CPU: 46.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.50%\n",
      "Epoch 31, Train Loss: 175.54487323760986, Val Loss: 30.92645788192749\n",
      "Time: 0.12s, CPU: 43.40%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 32, Train Loss: 167.91397804021835, Val Loss: 33.13581705093384\n",
      "Time: 0.22s, CPU: 53.35%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 33, Train Loss: 168.60378450155258, Val Loss: 31.929064393043518\n",
      "Time: 0.30s, CPU: 67.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 17.00%\n",
      "Epoch 34, Train Loss: 176.0509529709816, Val Loss: 30.71545922756195\n",
      "Time: 0.22s, CPU: 72.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 14.50%\n",
      "Epoch 35, Train Loss: 182.08405369520187, Val Loss: 30.347889304161072\n",
      "Time: 0.13s, CPU: 46.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 13.00%\n",
      "Epoch 36, Train Loss: 178.34554439783096, Val Loss: 34.22051525115967\n",
      "Time: 0.18s, CPU: 54.05%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 16.00%\n",
      "Epoch 37, Train Loss: 191.98205035924911, Val Loss: 33.15935182571411\n",
      "Time: 0.12s, CPU: 49.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 38, Train Loss: 183.86471563577652, Val Loss: 29.16345477104187\n",
      "Time: 0.12s, CPU: 54.65%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 39, Train Loss: 160.02426850795746, Val Loss: 30.4991272687912\n",
      "Time: 0.14s, CPU: 52.65%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.50%\n",
      "Epoch 40, Train Loss: 162.03855901956558, Val Loss: 30.673803448677063\n",
      "Time: 0.13s, CPU: 44.80%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 41, Train Loss: 165.78538209199905, Val Loss: 30.46633243560791\n",
      "Time: 0.14s, CPU: 55.25%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 42, Train Loss: 159.28507256507874, Val Loss: 30.495197296142578\n",
      "Time: 0.12s, CPU: 47.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 43, Train Loss: 151.6809527873993, Val Loss: 31.09639620780945\n",
      "Time: 0.14s, CPU: 35.05%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 44, Train Loss: 150.5009840130806, Val Loss: 29.47432816028595\n",
      "Time: 0.13s, CPU: 38.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 45, Train Loss: 156.25192630290985, Val Loss: 30.888744950294495\n",
      "Time: 0.14s, CPU: 41.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.50%\n",
      "Epoch 46, Train Loss: 161.08993411064148, Val Loss: 30.108186721801758\n",
      "Time: 0.17s, CPU: 62.50%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 18.00%\n",
      "Epoch 47, Train Loss: 160.45836901664734, Val Loss: 33.59637701511383\n",
      "Time: 0.14s, CPU: 47.10%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.00%\n",
      "Epoch 48, Train Loss: 161.1424726843834, Val Loss: 34.48826193809509\n",
      "Time: 0.12s, CPU: 49.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 49, Train Loss: 162.41692155599594, Val Loss: 33.17191421985626\n",
      "Time: 0.12s, CPU: 54.65%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 50, Train Loss: 158.19670581817627, Val Loss: 29.91296911239624\n",
      "Time: 0.13s, CPU: 53.65%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 51, Train Loss: 152.36103332042694, Val Loss: 28.93100380897522\n",
      "Time: 0.13s, CPU: 49.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 52, Train Loss: 152.3306275010109, Val Loss: 30.03386163711548\n",
      "Time: 0.13s, CPU: 52.35%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 53, Train Loss: 149.68728590011597, Val Loss: 30.234959721565247\n",
      "Time: 0.13s, CPU: 46.50%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 54, Train Loss: 145.57698595523834, Val Loss: 33.160571694374084\n",
      "Time: 0.14s, CPU: 40.05%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 55, Train Loss: 154.29242146015167, Val Loss: 29.466519713401794\n",
      "Time: 0.12s, CPU: 43.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.50%\n",
      "Epoch 56, Train Loss: 145.69685471057892, Val Loss: 31.361268639564514\n",
      "Time: 0.12s, CPU: 57.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 57, Train Loss: 144.57070195674896, Val Loss: 31.018218398094177\n",
      "Time: 0.12s, CPU: 52.90%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 58, Train Loss: 159.25979834794998, Val Loss: 33.23765516281128\n",
      "Time: 0.15s, CPU: 47.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 59, Train Loss: 154.71087563037872, Val Loss: 29.866005778312683\n",
      "Time: 0.16s, CPU: 59.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 60, Train Loss: 149.88564109802246, Val Loss: 29.479204416275024\n",
      "Time: 0.14s, CPU: 45.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 61, Train Loss: 133.57656568288803, Val Loss: 33.396077156066895\n",
      "Time: 0.13s, CPU: 55.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.50%\n",
      "Epoch 62, Train Loss: 136.26295697689056, Val Loss: 34.87098848819733\n",
      "Time: 0.14s, CPU: 54.95%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 63, Train Loss: 133.4630912542343, Val Loss: 32.47985601425171\n",
      "Time: 0.12s, CPU: 47.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 64, Train Loss: 139.6859883069992, Val Loss: 31.358442664146423\n",
      "Time: 0.13s, CPU: 49.00%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 65, Train Loss: 148.6179171204567, Val Loss: 30.972983479499817\n",
      "Time: 0.12s, CPU: 53.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 66, Train Loss: 135.41952693462372, Val Loss: 32.429834961891174\n",
      "Time: 0.12s, CPU: 51.20%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 28.00%\n",
      "Epoch 67, Train Loss: 122.73788043856621, Val Loss: 31.16415846347809\n",
      "Time: 0.19s, CPU: 52.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.00%\n",
      "Epoch 68, Train Loss: 125.32296150922775, Val Loss: 31.611798763275146\n",
      "Time: 0.13s, CPU: 55.50%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.00%\n",
      "Epoch 69, Train Loss: 130.07771474123, Val Loss: 32.27864849567413\n",
      "Time: 0.19s, CPU: 80.55%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.50%\n",
      "Epoch 70, Train Loss: 132.07931834459305, Val Loss: 30.826064944267273\n",
      "Time: 0.12s, CPU: 64.10%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 14.50%\n",
      "Epoch 71, Train Loss: 123.23915332555771, Val Loss: 30.99729073047638\n",
      "Time: 0.13s, CPU: 48.00%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 72, Train Loss: 113.46865060925484, Val Loss: 31.80360460281372\n",
      "Time: 0.12s, CPU: 50.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.50%\n",
      "Epoch 73, Train Loss: 113.55359929800034, Val Loss: 32.186588644981384\n",
      "Time: 0.14s, CPU: 58.75%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 74, Train Loss: 115.90315839648247, Val Loss: 32.06942331790924\n",
      "Time: 0.16s, CPU: 56.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.50%\n",
      "Epoch 75, Train Loss: 116.77877444028854, Val Loss: 32.83747708797455\n",
      "Time: 0.19s, CPU: 54.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.50%\n",
      "Epoch 76, Train Loss: 117.98849505186081, Val Loss: 33.72274124622345\n",
      "Time: 0.18s, CPU: 50.95%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 77, Train Loss: 110.5292095541954, Val Loss: 37.306055545806885\n",
      "Time: 0.12s, CPU: 69.40%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 15.50%\n",
      "Epoch 78, Train Loss: 109.7193595468998, Val Loss: 31.724908590316772\n",
      "Time: 0.13s, CPU: 49.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 17.00%\n",
      "Epoch 79, Train Loss: 107.93767553567886, Val Loss: 33.78833091259003\n",
      "Time: 0.12s, CPU: 46.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.50%\n",
      "Epoch 80, Train Loss: 102.48994833230972, Val Loss: 34.50792467594147\n",
      "Time: 0.12s, CPU: 46.15%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 81, Train Loss: 98.64446493983269, Val Loss: 33.52900421619415\n",
      "Time: 0.12s, CPU: 40.10%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 26.00%\n",
      "Epoch 82, Train Loss: 101.77763849496841, Val Loss: 33.117679953575134\n",
      "Time: 0.16s, CPU: 42.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.00%\n",
      "Epoch 83, Train Loss: 101.54942259192467, Val Loss: 33.30085337162018\n",
      "Time: 0.12s, CPU: 45.25%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 84, Train Loss: 95.64959400892258, Val Loss: 33.17002809047699\n",
      "Time: 0.13s, CPU: 56.15%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 85, Train Loss: 93.02628375589848, Val Loss: 32.898335337638855\n",
      "Time: 0.13s, CPU: 44.35%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 86, Train Loss: 93.16904582083225, Val Loss: 32.81103265285492\n",
      "Time: 0.13s, CPU: 46.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 87, Train Loss: 93.2236286252737, Val Loss: 33.01413595676422\n",
      "Time: 0.12s, CPU: 55.45%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 88, Train Loss: 92.00049792230129, Val Loss: 33.02308702468872\n",
      "Time: 0.13s, CPU: 52.40%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 25.50%\n",
      "Epoch 89, Train Loss: 90.57408046722412, Val Loss: 33.09760522842407\n",
      "Time: 0.15s, CPU: 40.95%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 90, Train Loss: 90.05994285643101, Val Loss: 32.67421102523804\n",
      "Time: 0.13s, CPU: 48.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.50%\n",
      "Epoch 91, Train Loss: 89.19681707024574, Val Loss: 33.15051341056824\n",
      "Time: 0.12s, CPU: 34.15%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.50%\n",
      "Epoch 92, Train Loss: 89.53401771187782, Val Loss: 33.57826244831085\n",
      "Time: 0.15s, CPU: 45.15%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 20.50%\n",
      "Epoch 93, Train Loss: 89.07875689864159, Val Loss: 34.572574496269226\n",
      "Time: 0.12s, CPU: 51.70%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.00%\n",
      "Epoch 94, Train Loss: 88.69908021390438, Val Loss: 34.869765400886536\n",
      "Time: 0.12s, CPU: 33.40%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Epoch 95, Train Loss: 86.89266887307167, Val Loss: 35.365962266922\n",
      "Time: 0.17s, CPU: 52.05%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.50%\n",
      "Epoch 96, Train Loss: 86.48482574522495, Val Loss: 34.50196051597595\n",
      "Time: 0.13s, CPU: 42.10%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 97, Train Loss: 83.38374117016792, Val Loss: 36.269073843955994\n",
      "Time: 0.14s, CPU: 43.30%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 23.00%\n",
      "Epoch 98, Train Loss: 86.08543682098389, Val Loss: 35.02447950839996\n",
      "Time: 0.14s, CPU: 37.65%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 99, Train Loss: 85.677036434412, Val Loss: 38.72373282909393\n",
      "Time: 0.13s, CPU: 59.60%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 19.00%\n",
      "Epoch 100, Train Loss: 90.79640358686447, Val Loss: 42.21199071407318\n",
      "Time: 0.12s, CPU: 55.90%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 21.00%\n",
      "Epoch 101, Train Loss: 99.43265381455421, Val Loss: 48.91444802284241\n",
      "Time: 0.12s, CPU: 53.85%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.50%\n",
      "Epoch 102, Train Loss: 110.34698368608952, Val Loss: 45.239205837249756\n",
      "Time: 0.42s, CPU: 71.60%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 17.50%\n",
      "Epoch 103, Train Loss: 110.59319424629211, Val Loss: 37.24709951877594\n",
      "Time: 0.21s, CPU: 63.50%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 13.00%\n",
      "Epoch 104, Train Loss: 98.17619878053665, Val Loss: 42.79315996170044\n",
      "Time: 0.13s, CPU: 52.15%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 18.50%\n",
      "Epoch 105, Train Loss: 115.32956951856613, Val Loss: 44.60901975631714\n",
      "Time: 0.14s, CPU: 54.80%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 22.00%\n",
      "Epoch 106, Train Loss: 115.70201522111893, Val Loss: 37.84430730342865\n",
      "Time: 0.12s, CPU: 48.90%, Memory: 3.37GB, GPU: 0.17GB, GPU Util: 24.00%\n",
      "Early stopping at epoch 107\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_2l_slim160.pth\n",
      "Average Time per Epoch: 0.15s\n",
      "Average CPU Usage: 50.35%\n",
      "Average Memory Usage: 3.37GB\n",
      "Average GPU Usage: 0.18GB\n",
      "Average GPU Utilization: 21.59%\n",
      "\n",
      "Total Training Time: 15.64s\n",
      "Max CPU Usage: 80.55%\n",
      "Max Memory Usage: 3.39GB\n",
      "Max GPU Usage: 0.49GB\n",
      "Max GPU Utilization: 28.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn2_slim160 = GCN2Layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes)\n",
    "print(gcn2_slim160)\n",
    "print(f\"Total number of trainable parameters: {(gcn2_slim160.count_parameters())*2}\\n\")\n",
    "single_train(gcn2_slim160, slim160_train_loader, slim160_val_loader, \n",
    "            lr=0.01, num_epochs=500, step_size=75, patience=10, gamma=0.5,\n",
    "            save_path='models/gcn_2l_slim160.pth', binary_classification=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4091\n",
      "Average Sensitivity (Recall): 0.4120\n",
      "Average Specificity: 0.7034\n",
      "\n",
      "Average Inference Time per Batch: 0.0015s\n",
      "Average CPU Usage: 36.37%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.20GB\n",
      "Average GPU Utilization: 1.33%\n"
     ]
    }
   ],
   "source": [
    "gcn2_slim160 = GCN2Layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes)\n",
    "gcn2_slim160.load_state_dict(torch.load('models/gcn_2l_slim160.pth'))\n",
    "single_test(gcn2_slim160.to(device), slim160_test_loader, binary_classification=False)\n",
    "inference_performance(gcn2_slim160.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCESN 1-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_1layer(\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 240\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 67.46716, Total Training Time: 7.99s\n",
      "  Accuracy: 0.7105, Sensitivity: 1.0000, Specificity: 0.0833\n",
      "\n",
      "Run 2/50 -> Loss: 66.33404, Total Training Time: 0.68s\n",
      "  Accuracy: 0.6842, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 3/50 -> Loss: 76.58904, Total Training Time: 7.44s\n",
      "  Accuracy: 0.7368, Sensitivity: 1.0000, Specificity: 0.1667\n",
      "\n",
      "Run 4/50 -> Loss: 65.63456, Total Training Time: 7.27s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 5/50 -> Loss: 72.87041, Total Training Time: 7.29s\n",
      "  Accuracy: 0.7368, Sensitivity: 1.0000, Specificity: 0.1667\n",
      "\n",
      "Run 6/50 -> Loss: 55.21590, Total Training Time: 7.11s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 7/50 -> Loss: 57.34009, Total Training Time: 6.44s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9615, Specificity: 0.4167\n",
      "\n",
      "Run 8/50 -> Loss: 53.78750, Total Training Time: 6.76s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 9/50 -> Loss: 58.54890, Total Training Time: 0.73s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9615, Specificity: 0.5000\n",
      "\n",
      "Run 10/50 -> Loss: 53.54439, Total Training Time: 0.36s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 11/50 -> Loss: 55.74368, Total Training Time: 0.30s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9615, Specificity: 0.2500\n",
      "\n",
      "Run 12/50 -> Loss: 52.16733, Total Training Time: 7.33s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 13/50 -> Loss: 59.44791, Total Training Time: 0.82s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 14/50 -> Loss: 64.32403, Total Training Time: 6.69s\n",
      "  Accuracy: 0.8684, Sensitivity: 1.0000, Specificity: 0.5833\n",
      "\n",
      "Run 15/50 -> Loss: 54.45733, Total Training Time: 6.35s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 16/50 -> Loss: 53.61293, Total Training Time: 6.84s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 17/50 -> Loss: 49.96268, Total Training Time: 7.18s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 18/50 -> Loss: 50.47324, Total Training Time: 0.53s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 19/50 -> Loss: 57.88165, Total Training Time: 7.17s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 20/50 -> Loss: 51.46622, Total Training Time: 0.68s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 21/50 -> Loss: 50.19096, Total Training Time: 7.21s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9615, Specificity: 0.5833\n",
      "\n",
      "Run 22/50 -> Loss: 123.74473, Total Training Time: 7.65s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 23/50 -> Loss: 52.19275, Total Training Time: 7.07s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 24/50 -> Loss: 53.09507, Total Training Time: 0.51s\n",
      "  Accuracy: 0.8684, Sensitivity: 1.0000, Specificity: 0.5833\n",
      "\n",
      "Run 25/50 -> Loss: 50.92911, Total Training Time: 0.81s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 26/50 -> Loss: 49.91459, Total Training Time: 6.68s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 27/50 -> Loss: 50.82685, Total Training Time: 7.50s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 28/50 -> Loss: 58.90998, Total Training Time: 0.33s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 29/50 -> Loss: 54.21667, Total Training Time: 0.81s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 30/50 -> Loss: 56.39665, Total Training Time: 0.68s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 31/50 -> Loss: 52.51799, Total Training Time: 0.68s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 32/50 -> Loss: 49.82785, Total Training Time: 0.89s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 33/50 -> Loss: 52.31840, Total Training Time: 0.33s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8846, Specificity: 0.5000\n",
      "\n",
      "Run 34/50 -> Loss: 48.88596, Total Training Time: 0.91s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 35/50 -> Loss: 54.63579, Total Training Time: 6.83s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 36/50 -> Loss: 54.72248, Total Training Time: 6.61s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 37/50 -> Loss: 56.56489, Total Training Time: 6.82s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 38/50 -> Loss: 51.96543, Total Training Time: 0.19s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 39/50 -> Loss: 51.59411, Total Training Time: 0.51s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 40/50 -> Loss: 56.76019, Total Training Time: 0.48s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 41/50 -> Loss: 54.28265, Total Training Time: 7.15s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 42/50 -> Loss: 55.39726, Total Training Time: 0.59s\n",
      "  Accuracy: 0.8421, Sensitivity: 1.0000, Specificity: 0.5000\n",
      "\n",
      "Run 43/50 -> Loss: 72.68358, Total Training Time: 7.75s\n",
      "  Accuracy: 0.6842, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 44/50 -> Loss: 66.11632, Total Training Time: 1.60s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 45/50 -> Loss: 54.91620, Total Training Time: 0.78s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 46/50 -> Loss: 64.68396, Total Training Time: 7.31s\n",
      "  Accuracy: 0.8158, Sensitivity: 1.0000, Specificity: 0.4167\n",
      "\n",
      "Run 47/50 -> Loss: 52.48962, Total Training Time: 7.17s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 48/50 -> Loss: 50.49824, Total Training Time: 7.62s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 49/50 -> Loss: 58.18599, Total Training Time: 7.76s\n",
      "  Accuracy: 0.8421, Sensitivity: 1.0000, Specificity: 0.5000\n",
      "\n",
      "Run 50/50 -> Loss: 51.61211, Total Training Time: 0.60s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "Best model saved to models/best_gcesn_1l_mutag.pth with accuracy 0.9474\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.8316 ± 0.06, Avg Sensitivity: 0.9431 ± 0.04, Avg Specificity: 0.5900 ± 0.23\n",
      "  Max Accuracy: 0.9474, Max Sensitivity: 1.0000, Max Specificity: 0.9167\n",
      "  Avg Num Epoch: 154.70, Avg Training Time: 4.16s, Avg Epoch Time: 0.03s\n",
      "  Avg CPU Usage: 20.71%, Avg GPU Usage: 0.13%, Avg Memory Usage: 3.38GB\n",
      "  Avg Max CPU Usage: 48.45%, Avg Max GPU Usage: 0.13GB, Avg Max Memory Usage: 3.38GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag = GCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "print(gcesn_mutag)\n",
    "print(f\"Total number of trainable parameters: {gcesn_mutag.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_mutag, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_gcesn_1l_mutag.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3158\n",
      "Average Sensitivity (Recall): 0.0000\n",
      "Average Specificity: 1.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0010s\n",
      "Average CPU Usage: 14.18%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.13GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag = GCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "gcesn_mutag.initialize_weights()\n",
    "gcesn_mutag.load_state_dict(torch.load('models/best_gcesn_1l_mutag.pth'))\n",
    "single_test(gcesn_mutag.to(device), mutag_test_loader)\n",
    "inference_performance(gcesn_mutag.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_1layer(\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 306\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 66.47801, Total Training Time: 0.41s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 2/50 -> Loss: 66.47172, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 3/50 -> Loss: 66.47406, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 4/50 -> Loss: 66.48687, Total Training Time: 0.34s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 5/50 -> Loss: 66.46151, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 6/50 -> Loss: 66.47197, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 7/50 -> Loss: 66.46766, Total Training Time: 0.34s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 8/50 -> Loss: 66.45696, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 9/50 -> Loss: 66.45214, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 10/50 -> Loss: 66.45858, Total Training Time: 0.40s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 11/50 -> Loss: 66.44223, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 12/50 -> Loss: 66.44688, Total Training Time: 0.34s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 13/50 -> Loss: 66.45540, Total Training Time: 0.37s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 14/50 -> Loss: 66.43713, Total Training Time: 0.39s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 15/50 -> Loss: 66.42445, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 16/50 -> Loss: 66.42868, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 17/50 -> Loss: 66.44141, Total Training Time: 0.41s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 18/50 -> Loss: 66.43349, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 19/50 -> Loss: 66.41465, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 20/50 -> Loss: 66.42441, Total Training Time: 0.38s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 21/50 -> Loss: 66.43568, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 22/50 -> Loss: 66.43020, Total Training Time: 0.39s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 23/50 -> Loss: 66.38417, Total Training Time: 8.87s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 24/50 -> Loss: 66.40159, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 25/50 -> Loss: 66.39318, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 26/50 -> Loss: 66.40292, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 27/50 -> Loss: 66.35262, Total Training Time: 0.35s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 28/50 -> Loss: 66.37518, Total Training Time: 6.60s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 29/50 -> Loss: 66.32346, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 30/50 -> Loss: 66.32146, Total Training Time: 0.34s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 31/50 -> Loss: 66.22848, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 32/50 -> Loss: 66.29172, Total Training Time: 0.36s\n",
      "  Accuracy: 0.4444, Sensitivity: 1.0000, Specificity: 0.0625\n",
      "\n",
      "Run 33/50 -> Loss: 66.27435, Total Training Time: 0.36s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.8182, Specificity: 0.0625\n",
      "\n",
      "Run 34/50 -> Loss: 66.26960, Total Training Time: 9.15s\n",
      "  Accuracy: 0.4815, Sensitivity: 1.0000, Specificity: 0.1250\n",
      "\n",
      "Run 35/50 -> Loss: 66.32452, Total Training Time: 7.53s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.9091, Specificity: 0.0625\n",
      "\n",
      "Run 36/50 -> Loss: 66.16661, Total Training Time: 0.43s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.8182, Specificity: 0.1250\n",
      "\n",
      "Run 37/50 -> Loss: 66.13299, Total Training Time: 0.41s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.6364, Specificity: 0.3125\n",
      "\n",
      "Run 38/50 -> Loss: 66.20515, Total Training Time: 0.47s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.6364, Specificity: 0.2500\n",
      "\n",
      "Run 39/50 -> Loss: 66.23930, Total Training Time: 9.11s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.8182, Specificity: 0.3125\n",
      "\n",
      "Run 40/50 -> Loss: 66.38901, Total Training Time: 0.32s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.9091, Specificity: 0.1250\n",
      "\n",
      "Run 41/50 -> Loss: 66.08507, Total Training Time: 0.38s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 42/50 -> Loss: 66.08058, Total Training Time: 0.43s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 43/50 -> Loss: 66.26064, Total Training Time: 0.48s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.7273, Specificity: 0.3750\n",
      "\n",
      "Run 44/50 -> Loss: 66.06946, Total Training Time: 8.76s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.5455, Specificity: 0.2500\n",
      "\n",
      "Run 45/50 -> Loss: 66.18967, Total Training Time: 0.34s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 46/50 -> Loss: 66.05405, Total Training Time: 0.38s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.5455, Specificity: 0.4375\n",
      "\n",
      "Run 47/50 -> Loss: 66.03081, Total Training Time: 8.57s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.5455, Specificity: 0.4375\n",
      "\n",
      "Run 48/50 -> Loss: 66.21090, Total Training Time: 9.08s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.6364, Specificity: 0.3750\n",
      "\n",
      "Run 49/50 -> Loss: 65.94174, Total Training Time: 0.43s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.5455, Specificity: 0.4375\n",
      "\n",
      "Run 50/50 -> Loss: 66.25477, Total Training Time: 0.38s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.6364, Specificity: 0.3750\n",
      "Best model saved to models/best_gcesn_1l_emci.pth with accuracy 0.5185\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.4237 ± 0.03, Avg Sensitivity: 0.8873 ± 0.17, Avg Specificity: 0.1050 ± 0.16\n",
      "  Max Accuracy: 0.5185, Max Sensitivity: 1.0000, Max Specificity: 0.4375\n",
      "  Avg Num Epoch: 39.88, Avg Training Time: 1.67s, Avg Epoch Time: 0.04s\n",
      "  Avg CPU Usage: 17.79%, Avg GPU Usage: 0.13%, Avg Memory Usage: 3.38GB\n",
      "  Avg Max CPU Usage: 30.94%, Avg Max GPU Usage: 0.13GB, Avg Max Memory Usage: 3.38GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_emci = GCESN_1layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=1, num_iterations=1)\n",
    "print(gcesn_emci)\n",
    "print(f\"Total number of trainable parameters: {gcesn_emci.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_emci, emci_train_loader, emci_val_loader, emci_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_gcesn_1l_emci.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5926\n",
      "Average Sensitivity (Recall): 0.0000\n",
      "Average Specificity: 1.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0005s\n",
      "Average CPU Usage: 8.65%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.13GB\n",
      "Average GPU Utilization: 3.00%\n"
     ]
    }
   ],
   "source": [
    "gcesn_emci = GCESN_1layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=1, num_iterations=1)\n",
    "gcesn_emci.initialize_weights()\n",
    "gcesn_emci.load_state_dict(torch.load('models/best_gcesn_1l_emci.pth'))\n",
    "single_test(gcesn_emci.to(device), emci_test_loader)\n",
    "inference_performance(gcesn_emci.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_1layer(\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 323\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 245.90678, Total Training Time: 40.15s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4074, Specificity: 0.7054\n",
      "\n",
      "Run 2/50 -> Loss: 251.17002, Total Training Time: 53.90s\n",
      "  Accuracy: 0.2879, Sensitivity: 0.3333, Specificity: 0.6677\n",
      "\n",
      "Run 3/50 -> Loss: 248.70386, Total Training Time: 94.14s\n",
      "  Accuracy: 0.3182, Sensitivity: 0.3472, Specificity: 0.6736\n",
      "\n",
      "Run 4/50 -> Loss: 247.93722, Total Training Time: 76.11s\n",
      "  Accuracy: 0.3939, Sensitivity: 0.4213, Specificity: 0.7073\n",
      "\n",
      "Run 5/50 -> Loss: 243.54635, Total Training Time: 98.12s\n",
      "  Accuracy: 0.3636, Sensitivity: 0.3935, Specificity: 0.6964\n",
      "\n",
      "Run 6/50 -> Loss: 240.13485, Total Training Time: 123.32s\n",
      "  Accuracy: 0.3939, Sensitivity: 0.4028, Specificity: 0.7054\n",
      "\n",
      "Run 7/50 -> Loss: 248.67035, Total Training Time: 60.01s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3241, Specificity: 0.6607\n",
      "\n",
      "Run 8/50 -> Loss: 250.66917, Total Training Time: 84.82s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3750, Specificity: 0.6835\n",
      "\n",
      "Run 9/50 -> Loss: 248.05318, Total Training Time: 74.88s\n",
      "  Accuracy: 0.4242, Sensitivity: 0.4398, Specificity: 0.7192\n",
      "\n",
      "Run 10/50 -> Loss: 251.63954, Total Training Time: 86.32s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3287, Specificity: 0.6647\n",
      "\n",
      "Run 11/50 -> Loss: 252.09984, Total Training Time: 78.88s\n",
      "  Accuracy: 0.3182, Sensitivity: 0.3472, Specificity: 0.6766\n",
      "\n",
      "Run 12/50 -> Loss: 246.77516, Total Training Time: 54.01s\n",
      "  Accuracy: 0.2879, Sensitivity: 0.3241, Specificity: 0.6558\n",
      "\n",
      "Run 13/50 -> Loss: 250.20557, Total Training Time: 111.86s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3380, Specificity: 0.6647\n",
      "\n",
      "Run 14/50 -> Loss: 244.45800, Total Training Time: 64.78s\n",
      "  Accuracy: 0.3182, Sensitivity: 0.3426, Specificity: 0.6776\n",
      "\n",
      "Run 15/50 -> Loss: 246.04853, Total Training Time: 72.91s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3611, Specificity: 0.6855\n",
      "\n",
      "Run 16/50 -> Loss: 251.54554, Total Training Time: 70.88s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3796, Specificity: 0.6885\n",
      "\n",
      "Run 17/50 -> Loss: 249.61882, Total Training Time: 89.39s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3426, Specificity: 0.6687\n",
      "\n",
      "Run 18/50 -> Loss: 244.51287, Total Training Time: 76.50s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4028, Specificity: 0.7004\n",
      "\n",
      "Run 19/50 -> Loss: 243.97809, Total Training Time: 86.64s\n",
      "  Accuracy: 0.3636, Sensitivity: 0.3981, Specificity: 0.6974\n",
      "\n",
      "Run 20/50 -> Loss: 242.35624, Total Training Time: 72.77s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4074, Specificity: 0.7034\n",
      "\n",
      "Run 21/50 -> Loss: 249.36049, Total Training Time: 91.81s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3194, Specificity: 0.6597\n",
      "\n",
      "Run 22/50 -> Loss: 252.77520, Total Training Time: 66.88s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3472, Specificity: 0.6716\n",
      "\n",
      "Run 23/50 -> Loss: 243.54721, Total Training Time: 68.16s\n",
      "  Accuracy: 0.4242, Sensitivity: 0.4352, Specificity: 0.7222\n",
      "\n",
      "Run 24/50 -> Loss: 252.43354, Total Training Time: 92.97s\n",
      "  Accuracy: 0.3636, Sensitivity: 0.3889, Specificity: 0.6954\n",
      "\n",
      "Run 25/50 -> Loss: 247.18619, Total Training Time: 102.09s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3796, Specificity: 0.6825\n",
      "\n",
      "Run 26/50 -> Loss: 250.67886, Total Training Time: 81.52s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3935, Specificity: 0.6944\n",
      "\n",
      "Run 27/50 -> Loss: 246.27276, Total Training Time: 49.36s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4120, Specificity: 0.7014\n",
      "\n",
      "Run 28/50 -> Loss: 254.40349, Total Training Time: 88.68s\n",
      "  Accuracy: 0.2424, Sensitivity: 0.2685, Specificity: 0.6329\n",
      "\n",
      "Run 29/50 -> Loss: 250.06974, Total Training Time: 50.51s\n",
      "  Accuracy: 0.3939, Sensitivity: 0.4306, Specificity: 0.7103\n",
      "\n",
      "Run 30/50 -> Loss: 247.13513, Total Training Time: 44.72s\n",
      "  Accuracy: 0.3939, Sensitivity: 0.4259, Specificity: 0.7123\n",
      "\n",
      "Run 31/50 -> Loss: 259.07959, Total Training Time: 111.96s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4120, Specificity: 0.6964\n",
      "\n",
      "Run 32/50 -> Loss: 249.40996, Total Training Time: 31.97s\n",
      "  Accuracy: 0.2576, Sensitivity: 0.2870, Specificity: 0.6399\n",
      "\n",
      "Run 33/50 -> Loss: 248.87644, Total Training Time: 70.26s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3750, Specificity: 0.6806\n",
      "\n",
      "Run 34/50 -> Loss: 248.24927, Total Training Time: 113.61s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3380, Specificity: 0.6667\n",
      "\n",
      "Run 35/50 -> Loss: 248.53414, Total Training Time: 50.27s\n",
      "  Accuracy: 0.3030, Sensitivity: 0.3287, Specificity: 0.6657\n",
      "\n",
      "Run 36/50 -> Loss: 244.49976, Total Training Time: 82.31s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3611, Specificity: 0.6786\n",
      "\n",
      "Run 37/50 -> Loss: 250.53007, Total Training Time: 54.11s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3472, Specificity: 0.6756\n",
      "\n",
      "Run 38/50 -> Loss: 250.65063, Total Training Time: 63.34s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3889, Specificity: 0.6954\n",
      "\n",
      "Run 39/50 -> Loss: 248.69612, Total Training Time: 63.45s\n",
      "  Accuracy: 0.4091, Sensitivity: 0.4398, Specificity: 0.7183\n",
      "\n",
      "Run 40/50 -> Loss: 247.13431, Total Training Time: 63.96s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3704, Specificity: 0.6855\n",
      "\n",
      "Run 41/50 -> Loss: 247.66933, Total Training Time: 65.99s\n",
      "  Accuracy: 0.3182, Sensitivity: 0.3333, Specificity: 0.6687\n",
      "\n",
      "Run 42/50 -> Loss: 246.39717, Total Training Time: 44.84s\n",
      "  Accuracy: 0.3788, Sensitivity: 0.4074, Specificity: 0.7004\n",
      "\n",
      "Run 43/50 -> Loss: 252.16355, Total Training Time: 35.97s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3611, Specificity: 0.6855\n",
      "\n",
      "Run 44/50 -> Loss: 250.41404, Total Training Time: 43.09s\n",
      "  Accuracy: 0.3182, Sensitivity: 0.3380, Specificity: 0.6726\n",
      "\n",
      "Run 45/50 -> Loss: 250.39068, Total Training Time: 73.30s\n",
      "  Accuracy: 0.2576, Sensitivity: 0.2870, Specificity: 0.6438\n",
      "\n",
      "Run 46/50 -> Loss: 249.24722, Total Training Time: 49.71s\n",
      "  Accuracy: 0.3939, Sensitivity: 0.4213, Specificity: 0.7103\n",
      "\n",
      "Run 47/50 -> Loss: 245.17471, Total Training Time: 60.99s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3843, Specificity: 0.6895\n",
      "\n",
      "Run 48/50 -> Loss: 254.26553, Total Training Time: 87.88s\n",
      "  Accuracy: 0.2879, Sensitivity: 0.3241, Specificity: 0.6587\n",
      "\n",
      "Run 49/50 -> Loss: 245.12593, Total Training Time: 76.33s\n",
      "  Accuracy: 0.3485, Sensitivity: 0.3889, Specificity: 0.6885\n",
      "\n",
      "Run 50/50 -> Loss: 243.28027, Total Training Time: 40.19s\n",
      "  Accuracy: 0.4091, Sensitivity: 0.4398, Specificity: 0.7183\n",
      "Best model saved to models/best_gcesn_1l_slim160.pth with accuracy 0.4242\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.3418 ± 0.04, Avg Sensitivity: 0.3710 ± 0.04, Avg Specificity: 0.6845 ± 0.02\n",
      "  Max Accuracy: 0.4242, Max Sensitivity: 0.4398, Max Specificity: 0.7222\n",
      "  Avg Num Epoch: 261.12, Avg Training Time: 71.81s, Avg Epoch Time: 0.28s\n",
      "  Avg CPU Usage: 60.78%, Avg GPU Usage: 0.11%, Avg Memory Usage: 3.38GB\n",
      "  Avg Max CPU Usage: 91.20%, Avg Max GPU Usage: 0.11GB, Avg Max Memory Usage: 3.38GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_slim160 = GCESN_1layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=1, num_iterations=1)\n",
    "print(gcesn_slim160)\n",
    "print(f\"Total number of trainable parameters: {gcesn_slim160.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_slim160, slim160_train_loader, slim160_val_loader, slim160_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=False, best_model_path='models/best_gcesn_1l_slim160.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333\n",
      "Average Sensitivity (Recall): 1.0000\n",
      "Average Specificity: 0.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0010s\n",
      "Average CPU Usage: 39.18%\n",
      "Average Memory Usage: 3.38GB\n",
      "Average GPU Usage: 0.09GB\n",
      "Average GPU Utilization: 15.67%\n"
     ]
    }
   ],
   "source": [
    "gcesn_slim160 = GCESN_1layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=1, num_iterations=1)\n",
    "gcesn_slim160.initialize_weights()\n",
    "gcesn_slim160.load_state_dict(torch.load('models/best_gcesn_1l_slim160.pth'))\n",
    "single_test(gcesn_slim160.to(device), slim160_test_loader)\n",
    "inference_performance(gcesn_slim160.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCESN 2-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_2layer(\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=28, bias=True)\n",
      "  )\n",
      "  (ridge_layer_2): RidgeLayer(\n",
      "    (linear): Linear(in_features=28, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 856\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 67.29538, Total Training Time: 7.85s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 2/50 -> Loss: 68.69862, Total Training Time: 6.10s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.8846, Specificity: 0.4167\n",
      "\n",
      "Run 3/50 -> Loss: 68.42492, Total Training Time: 6.47s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9231, Specificity: 0.3333\n",
      "\n",
      "Run 4/50 -> Loss: 65.64970, Total Training Time: 6.67s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.8846, Specificity: 0.4167\n",
      "\n",
      "Run 5/50 -> Loss: 57.23055, Total Training Time: 6.56s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 6/50 -> Loss: 56.81438, Total Training Time: 5.96s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 7/50 -> Loss: 57.50004, Total Training Time: 6.35s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 8/50 -> Loss: 66.06791, Total Training Time: 5.97s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9615, Specificity: 0.2500\n",
      "\n",
      "Run 9/50 -> Loss: 57.92268, Total Training Time: 6.54s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 10/50 -> Loss: 56.19120, Total Training Time: 6.49s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 11/50 -> Loss: 51.25170, Total Training Time: 6.07s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 12/50 -> Loss: 54.64120, Total Training Time: 5.84s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 13/50 -> Loss: 54.78713, Total Training Time: 6.10s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 14/50 -> Loss: 43.23310, Total Training Time: 6.47s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 15/50 -> Loss: 49.93015, Total Training Time: 5.98s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 16/50 -> Loss: 44.79394, Total Training Time: 6.02s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 17/50 -> Loss: 43.94604, Total Training Time: 7.31s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 18/50 -> Loss: 41.25477, Total Training Time: 6.19s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 19/50 -> Loss: 45.12185, Total Training Time: 6.36s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 20/50 -> Loss: 49.49166, Total Training Time: 5.94s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 21/50 -> Loss: 43.17090, Total Training Time: 6.21s\n",
      "  Accuracy: 0.9737, Sensitivity: 0.9615, Specificity: 1.0000\n",
      "\n",
      "Run 22/50 -> Loss: 46.90466, Total Training Time: 6.02s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 23/50 -> Loss: 49.42846, Total Training Time: 6.41s\n",
      "  Accuracy: 0.8684, Sensitivity: 1.0000, Specificity: 0.5833\n",
      "\n",
      "Run 24/50 -> Loss: 50.42635, Total Training Time: 6.06s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 25/50 -> Loss: 44.35759, Total Training Time: 6.00s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 26/50 -> Loss: 48.38189, Total Training Time: 6.09s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 27/50 -> Loss: 45.09466, Total Training Time: 6.35s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 28/50 -> Loss: 47.11416, Total Training Time: 4.93s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 29/50 -> Loss: 46.01610, Total Training Time: 5.96s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 30/50 -> Loss: 44.12857, Total Training Time: 5.83s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 31/50 -> Loss: 45.55265, Total Training Time: 6.33s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 32/50 -> Loss: 49.08465, Total Training Time: 5.96s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 33/50 -> Loss: 51.05661, Total Training Time: 6.82s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 34/50 -> Loss: 53.36249, Total Training Time: 6.19s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 35/50 -> Loss: 51.57400, Total Training Time: 6.83s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 36/50 -> Loss: 48.09792, Total Training Time: 6.37s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 37/50 -> Loss: 47.30153, Total Training Time: 6.16s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 38/50 -> Loss: 53.22647, Total Training Time: 6.17s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 39/50 -> Loss: 50.95966, Total Training Time: 6.56s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 40/50 -> Loss: 52.31464, Total Training Time: 6.28s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 41/50 -> Loss: 53.95556, Total Training Time: 6.42s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 42/50 -> Loss: 59.23635, Total Training Time: 5.99s\n",
      "  Accuracy: 0.8158, Sensitivity: 1.0000, Specificity: 0.4167\n",
      "\n",
      "Run 43/50 -> Loss: 52.36452, Total Training Time: 5.94s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 44/50 -> Loss: 48.45270, Total Training Time: 6.27s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9615, Specificity: 0.5833\n",
      "\n",
      "Run 45/50 -> Loss: 46.21378, Total Training Time: 1.56s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 46/50 -> Loss: 45.35703, Total Training Time: 6.12s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 47/50 -> Loss: 39.00267, Total Training Time: 6.11s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 48/50 -> Loss: 40.75977, Total Training Time: 6.59s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 49/50 -> Loss: 57.44911, Total Training Time: 6.90s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 50/50 -> Loss: 50.78466, Total Training Time: 6.29s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "Best model saved to models/best_gcesn_2l_mutag.pth with accuracy 0.9737\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.8732 ± 0.06, Avg Sensitivity: 0.9215 ± 0.03, Avg Specificity: 0.7683 ± 0.18\n",
      "  Max Accuracy: 0.9737, Max Sensitivity: 1.0000, Max Specificity: 1.0000\n",
      "  Avg Num Epoch: 260.46, Avg Training Time: 6.18s, Avg Epoch Time: 0.02s\n",
      "  Avg CPU Usage: 20.51%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.25GB\n",
      "  Avg Max CPU Usage: 56.92%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.25GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag_2 = GCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=1)\n",
    "print(gcesn_mutag_2)\n",
    "print(f\"Total number of trainable parameters: {gcesn_mutag_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_mutag_2, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_gcesn_2l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6842\n",
      "Average Sensitivity (Recall): 1.0000\n",
      "Average Specificity: 0.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0020s\n",
      "Average CPU Usage: 19.00%\n",
      "Average Memory Usage: 3.25GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 5.00%\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag_2 = GCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "gcesn_mutag_2.initialize_weights()\n",
    "gcesn_mutag_2.load_state_dict(torch.load('models/best_gcesn_2l_mutag.pth'))\n",
    "single_test(gcesn_mutag_2.to(device), mutag_test_loader)\n",
    "inference_performance(gcesn_mutag_2.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_2layer(\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (ridge_layer_2): RidgeLayer(\n",
      "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1106\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 1922.00609, Total Training Time: 1.16s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 2/50 -> Loss: 4224.90617, Total Training Time: 0.90s\n",
      "  Accuracy: 0.4815, Sensitivity: 1.0000, Specificity: 0.1250\n",
      "\n",
      "Run 3/50 -> Loss: 3489.46293, Total Training Time: 0.91s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.1818, Specificity: 0.8125\n",
      "\n",
      "Run 4/50 -> Loss: 2826.00755, Total Training Time: 2.00s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0909, Specificity: 0.9375\n",
      "\n",
      "Run 5/50 -> Loss: 1290.85168, Total Training Time: 2.58s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.1818, Specificity: 0.8125\n",
      "\n",
      "Run 6/50 -> Loss: 251.10470, Total Training Time: 1.54s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.7273, Specificity: 0.1250\n",
      "\n",
      "Run 7/50 -> Loss: 864.47903, Total Training Time: 2.36s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 8/50 -> Loss: 2768.04416, Total Training Time: 0.84s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 9/50 -> Loss: 1074.79335, Total Training Time: 0.49s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.3636, Specificity: 0.6250\n",
      "\n",
      "Run 10/50 -> Loss: 936.15384, Total Training Time: 0.97s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.8182, Specificity: 0.2500\n",
      "\n",
      "Run 11/50 -> Loss: 1029.24206, Total Training Time: 0.96s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 12/50 -> Loss: 100.56806, Total Training Time: 1.26s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.2727, Specificity: 0.6250\n",
      "\n",
      "Run 13/50 -> Loss: 172.07189, Total Training Time: 1.06s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.1818, Specificity: 0.5000\n",
      "\n",
      "Run 14/50 -> Loss: 729.44653, Total Training Time: 1.59s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.3636, Specificity: 0.5625\n",
      "\n",
      "Run 15/50 -> Loss: 238.24924, Total Training Time: 0.63s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.0909, Specificity: 0.8750\n",
      "\n",
      "Run 16/50 -> Loss: 753.37105, Total Training Time: 0.71s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.0000, Specificity: 0.9375\n",
      "\n",
      "Run 17/50 -> Loss: 600.51989, Total Training Time: 0.73s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 18/50 -> Loss: 198.20217, Total Training Time: 0.96s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 19/50 -> Loss: 297.35617, Total Training Time: 0.87s\n",
      "  Accuracy: 0.6296, Sensitivity: 0.0909, Specificity: 1.0000\n",
      "\n",
      "Run 20/50 -> Loss: 100.91951, Total Training Time: 1.39s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.0909, Specificity: 0.6250\n",
      "\n",
      "Run 21/50 -> Loss: 216.59845, Total Training Time: 0.71s\n",
      "  Accuracy: 0.4444, Sensitivity: 1.0000, Specificity: 0.0625\n",
      "\n",
      "Run 22/50 -> Loss: 187.06058, Total Training Time: 0.94s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 23/50 -> Loss: 71.09155, Total Training Time: 1.94s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.2727, Specificity: 0.8125\n",
      "\n",
      "Run 24/50 -> Loss: 327.46223, Total Training Time: 1.67s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.8182, Specificity: 0.0000\n",
      "\n",
      "Run 25/50 -> Loss: 113.55853, Total Training Time: 1.02s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.0000, Specificity: 0.8125\n",
      "\n",
      "Run 26/50 -> Loss: 91.26365, Total Training Time: 0.83s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.3636, Specificity: 0.6250\n",
      "\n",
      "Run 27/50 -> Loss: 67.55783, Total Training Time: 0.91s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.2727, Specificity: 0.6250\n",
      "\n",
      "Run 28/50 -> Loss: 66.34602, Total Training Time: 0.68s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.2727, Specificity: 0.6250\n",
      "\n",
      "Run 29/50 -> Loss: 85.91907, Total Training Time: 0.75s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 30/50 -> Loss: 67.21927, Total Training Time: 2.11s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.4545, Specificity: 0.2500\n",
      "\n",
      "Run 31/50 -> Loss: 73.72485, Total Training Time: 2.42s\n",
      "  Accuracy: 0.6667, Sensitivity: 0.1818, Specificity: 1.0000\n",
      "\n",
      "Run 32/50 -> Loss: 66.74887, Total Training Time: 1.06s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.7273, Specificity: 0.2500\n",
      "\n",
      "Run 33/50 -> Loss: 53.00342, Total Training Time: 13.33s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.5455, Specificity: 0.3125\n",
      "\n",
      "Run 34/50 -> Loss: 78.54688, Total Training Time: 0.83s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.0000, Specificity: 0.8750\n",
      "\n",
      "Run 35/50 -> Loss: 71.27843, Total Training Time: 0.57s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 36/50 -> Loss: 58.19730, Total Training Time: 1.73s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3636, Specificity: 0.3125\n",
      "\n",
      "Run 37/50 -> Loss: 85.35583, Total Training Time: 0.64s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.2727, Specificity: 0.3750\n",
      "\n",
      "Run 38/50 -> Loss: 73.92588, Total Training Time: 0.63s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 39/50 -> Loss: 71.32227, Total Training Time: 0.51s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.0000, Specificity: 0.7500\n",
      "\n",
      "Run 40/50 -> Loss: 74.60904, Total Training Time: 0.61s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 41/50 -> Loss: 84.12724, Total Training Time: 1.02s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Run 42/50 -> Loss: 68.74088, Total Training Time: 0.62s\n",
      "  Accuracy: 0.6296, Sensitivity: 0.3636, Specificity: 0.8125\n",
      "\n",
      "Run 43/50 -> Loss: 71.45517, Total Training Time: 0.62s\n",
      "  Accuracy: 0.6296, Sensitivity: 0.3636, Specificity: 0.8125\n",
      "\n",
      "Run 44/50 -> Loss: 69.64112, Total Training Time: 0.58s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.2727, Specificity: 0.6875\n",
      "\n",
      "Run 45/50 -> Loss: 63.74169, Total Training Time: 0.86s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.3636, Specificity: 0.5625\n",
      "\n",
      "Run 46/50 -> Loss: 64.87670, Total Training Time: 1.96s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.8182, Specificity: 0.3750\n",
      "\n",
      "Run 47/50 -> Loss: 70.44252, Total Training Time: 2.31s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.5455, Specificity: 0.3125\n",
      "\n",
      "Run 48/50 -> Loss: 64.28366, Total Training Time: 1.50s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.8182, Specificity: 0.1875\n",
      "\n",
      "Run 49/50 -> Loss: 86.07514, Total Training Time: 2.04s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.5455, Specificity: 0.1875\n",
      "\n",
      "Run 50/50 -> Loss: 66.24321, Total Training Time: 1.85s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.1818, Specificity: 0.5625\n",
      "Best model saved to models/best_gcesn_2l_emci.pth with accuracy 0.6667\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.5037 ± 0.09, Avg Sensitivity: 0.3055 ± 0.31, Avg Specificity: 0.6400 ± 0.32\n",
      "  Max Accuracy: 0.6667, Max Sensitivity: 1.0000, Max Specificity: 1.0000\n",
      "  Avg Num Epoch: 28.82, Avg Training Time: 1.42s, Avg Epoch Time: 0.05s\n",
      "  Avg CPU Usage: 19.12%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.25GB\n",
      "  Avg Max CPU Usage: 33.05%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.25GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_emci_2 = GCESN_2layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=0.7, num_iterations=3, ridge_alpha=0.9)\n",
    "print(gcesn_emci_2)\n",
    "print(f\"Total number of trainable parameters: {gcesn_emci_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_emci_2, emci_train_loader, emci_val_loader, emci_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_gcesn_2l_emci.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5926\n",
      "Average Sensitivity (Recall): 0.0000\n",
      "Average Specificity: 1.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0016s\n",
      "Average CPU Usage: 19.97%\n",
      "Average Memory Usage: 3.25GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 5.00%\n"
     ]
    }
   ],
   "source": [
    "gcesn_emci_2 = GCESN_2layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "gcesn_emci_2.initialize_weights()\n",
    "gcesn_emci_2.load_state_dict(torch.load('models/best_gcesn_2l_emci.pth'))\n",
    "single_test(gcesn_emci_2.to(device), emci_test_loader)\n",
    "inference_performance(gcesn_emci_2.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_2layer(\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (ridge_layer_2): RidgeLayer(\n",
      "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1123\n",
      "\n",
      "\n",
      "Run 1/10 -> Loss: 220.32367, Total Training Time: 35.57s\n",
      "  Accuracy: 0.6000, Sensitivity: 0.5000, Specificity: 0.6875\n",
      "\n",
      "Run 2/10 -> Loss: 222.05566, Total Training Time: 38.33s\n",
      "  Accuracy: 0.6429, Sensitivity: 0.6000, Specificity: 0.6667\n",
      "\n",
      "Run 3/10 -> Loss: 237.57963, Total Training Time: 67.27s\n",
      "  Accuracy: 0.5714, Sensitivity: 0.4444, Specificity: 0.6667\n",
      "\n",
      "Run 4/10 -> Loss: 232.60611, Total Training Time: 56.87s\n",
      "  Accuracy: 0.6538, Sensitivity: 0.5833, Specificity: 0.7143\n",
      "\n",
      "Run 5/10 -> Loss: 226.13063, Total Training Time: 50.87s\n",
      "  Accuracy: 0.5789, Sensitivity: 0.4000, Specificity: 0.6429\n",
      "\n",
      "Run 6/10 -> Loss: 217.02539, Total Training Time: 67.51s\n",
      "  Accuracy: 0.5714, Sensitivity: 0.4286, Specificity: 0.6429\n",
      "\n",
      "Run 7/10 -> Loss: 227.36563, Total Training Time: 78.71s\n",
      "  Accuracy: 0.6000, Sensitivity: 0.3333, Specificity: 0.7500\n",
      "\n",
      "Run 8/10 -> Loss: 225.53379, Total Training Time: 18.37s\n",
      "  Accuracy: 0.6154, Sensitivity: 0.3636, Specificity: 0.8000\n",
      "\n",
      "Run 9/10 -> Loss: 222.15370, Total Training Time: 97.22s\n",
      "  Accuracy: 0.5385, Sensitivity: 0.4000, Specificity: 0.6250\n",
      "\n",
      "Run 10/10 -> Loss: 214.96983, Total Training Time: 61.17s\n",
      "  Accuracy: 0.6452, Sensitivity: 0.3077, Specificity: 0.8889\n",
      "Best model saved to models/best_gcesn_2l_slim160.pth with accuracy 0.6538\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.6018 ± 0.04, Avg Sensitivity: 0.4361 ± 0.09, Avg Specificity: 0.7085 ± 0.08\n",
      "  Max Accuracy: 0.6538, Max Sensitivity: 0.6000, Max Specificity: 0.8889\n",
      "  Avg Num Epoch: 243.10, Avg Training Time: 57.19s, Avg Epoch Time: 0.24s\n",
      "  Avg CPU Usage: 58.89%, Avg GPU Usage: 0.06%, Avg Memory Usage: 3.29GB\n",
      "  Avg Max CPU Usage: 91.50%, Avg Max GPU Usage: 0.06GB, Avg Max Memory Usage: 3.30GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_slim160_2 = GCESN_2layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=1, num_iterations=1)\n",
    "print(gcesn_slim160_2)\n",
    "print(f\"Total number of trainable parameters: {gcesn_slim160_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_slim160_2, slim160_train_loader, slim160_val_loader, slim160_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=10, binary_classification=True, best_model_path='models/best_gcesn_2l_slim160.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: nan\n",
      "Average Sensitivity (Recall): 0.0000\n",
      "Average Specificity: 0.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0064s\n",
      "Average CPU Usage: 18.85%\n",
      "Average Memory Usage: 3.30GB\n",
      "Average GPU Usage: 0.09GB\n",
      "Average GPU Utilization: 4.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37/3672718425.py:155: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n"
     ]
    }
   ],
   "source": [
    "gcesn_slim160_2 = GCESN_2layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "gcesn_slim160_2.initialize_weights()\n",
    "gcesn_slim160_2.load_state_dict(torch.load('models/best_gcesn_2l_slim160.pth'))\n",
    "single_test(gcesn_slim160_2.to(device), slim160_test_loader)\n",
    "inference_performance(gcesn_slim160_2.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoupled GCESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_1layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 352\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 70.56559, Total Training Time: 6.96s\n",
      "  Accuracy: 0.7105, Sensitivity: 0.9615, Specificity: 0.1667\n",
      "\n",
      "Run 2/50 -> Loss: 71.67577, Total Training Time: 6.31s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 3/50 -> Loss: 72.48537, Total Training Time: 6.61s\n",
      "  Accuracy: 0.7105, Sensitivity: 0.9615, Specificity: 0.1667\n",
      "\n",
      "Run 4/50 -> Loss: 69.26342, Total Training Time: 6.60s\n",
      "  Accuracy: 0.7632, Sensitivity: 1.0000, Specificity: 0.2500\n",
      "\n",
      "Run 5/50 -> Loss: 75.93545, Total Training Time: 6.92s\n",
      "  Accuracy: 0.7105, Sensitivity: 0.9231, Specificity: 0.2500\n",
      "\n",
      "Run 6/50 -> Loss: 66.12871, Total Training Time: 6.40s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 7/50 -> Loss: 67.54536, Total Training Time: 6.49s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 8/50 -> Loss: 68.99481, Total Training Time: 6.55s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 9/50 -> Loss: 71.58659, Total Training Time: 6.74s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9615, Specificity: 0.3333\n",
      "\n",
      "Run 10/50 -> Loss: 70.57770, Total Training Time: 6.71s\n",
      "  Accuracy: 0.6842, Sensitivity: 0.9615, Specificity: 0.0833\n",
      "\n",
      "Run 11/50 -> Loss: 69.83149, Total Training Time: 6.79s\n",
      "  Accuracy: 0.6842, Sensitivity: 0.9615, Specificity: 0.0833\n",
      "\n",
      "Run 12/50 -> Loss: 66.92408, Total Training Time: 6.51s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 13/50 -> Loss: 66.86728, Total Training Time: 6.54s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 14/50 -> Loss: 66.84173, Total Training Time: 6.35s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 15/50 -> Loss: 65.86311, Total Training Time: 0.43s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 16/50 -> Loss: 75.27487, Total Training Time: 6.91s\n",
      "  Accuracy: 0.6842, Sensitivity: 0.9615, Specificity: 0.0833\n",
      "\n",
      "Run 17/50 -> Loss: 64.47852, Total Training Time: 6.73s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 18/50 -> Loss: 65.18140, Total Training Time: 0.48s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 19/50 -> Loss: 66.18107, Total Training Time: 6.49s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 20/50 -> Loss: 67.75954, Total Training Time: 6.55s\n",
      "  Accuracy: 0.7632, Sensitivity: 1.0000, Specificity: 0.2500\n",
      "\n",
      "Run 21/50 -> Loss: 63.72594, Total Training Time: 6.22s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 22/50 -> Loss: 68.10106, Total Training Time: 6.52s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 23/50 -> Loss: 73.89106, Total Training Time: 6.62s\n",
      "  Accuracy: 0.6842, Sensitivity: 0.9231, Specificity: 0.1667\n",
      "\n",
      "Run 24/50 -> Loss: 62.42668, Total Training Time: 6.34s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 25/50 -> Loss: 68.58616, Total Training Time: 6.45s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9231, Specificity: 0.3333\n",
      "\n",
      "Run 26/50 -> Loss: 63.75181, Total Training Time: 6.41s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9615, Specificity: 0.5833\n",
      "\n",
      "Run 27/50 -> Loss: 67.65488, Total Training Time: 6.78s\n",
      "  Accuracy: 0.7105, Sensitivity: 1.0000, Specificity: 0.0833\n",
      "\n",
      "Run 28/50 -> Loss: 65.76569, Total Training Time: 0.47s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9231, Specificity: 0.3333\n",
      "\n",
      "Run 29/50 -> Loss: 66.30837, Total Training Time: 6.52s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 30/50 -> Loss: 80.32955, Total Training Time: 7.66s\n",
      "  Accuracy: 0.6842, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 31/50 -> Loss: 66.01290, Total Training Time: 6.39s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 32/50 -> Loss: 62.37525, Total Training Time: 6.48s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 33/50 -> Loss: 60.09990, Total Training Time: 6.57s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8462, Specificity: 0.5833\n",
      "\n",
      "Run 34/50 -> Loss: 63.90464, Total Training Time: 6.94s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 35/50 -> Loss: 66.37256, Total Training Time: 6.49s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 36/50 -> Loss: 59.48818, Total Training Time: 6.39s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 37/50 -> Loss: 85.00058, Total Training Time: 7.85s\n",
      "  Accuracy: 0.7105, Sensitivity: 1.0000, Specificity: 0.0833\n",
      "\n",
      "Run 38/50 -> Loss: 65.37932, Total Training Time: 6.99s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 39/50 -> Loss: 57.54145, Total Training Time: 6.08s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 40/50 -> Loss: 65.09430, Total Training Time: 6.33s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 41/50 -> Loss: 58.90322, Total Training Time: 6.54s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 42/50 -> Loss: 69.10241, Total Training Time: 7.93s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9615, Specificity: 0.4167\n",
      "\n",
      "Run 43/50 -> Loss: 61.51349, Total Training Time: 6.38s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8462, Specificity: 0.5833\n",
      "\n",
      "Run 44/50 -> Loss: 72.79213, Total Training Time: 6.82s\n",
      "  Accuracy: 0.7105, Sensitivity: 1.0000, Specificity: 0.0833\n",
      "\n",
      "Run 45/50 -> Loss: 64.20056, Total Training Time: 0.44s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8462, Specificity: 0.5833\n",
      "\n",
      "Run 46/50 -> Loss: 67.76274, Total Training Time: 0.45s\n",
      "  Accuracy: 0.8158, Sensitivity: 1.0000, Specificity: 0.4167\n",
      "\n",
      "Run 47/50 -> Loss: 62.09536, Total Training Time: 5.27s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8462, Specificity: 0.5833\n",
      "\n",
      "Run 48/50 -> Loss: 66.15325, Total Training Time: 6.49s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9231, Specificity: 0.4167\n",
      "\n",
      "Run 49/50 -> Loss: 66.36495, Total Training Time: 6.39s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 50/50 -> Loss: 62.57899, Total Training Time: 6.69s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.8462, Specificity: 0.5833\n",
      "Best model saved to models/best_decoupledgcesn_1l_mutag.pth with accuracy 0.8421\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.7658 ± 0.04, Avg Sensitivity: 0.9238 ± 0.04, Avg Specificity: 0.4233 ± 0.19\n",
      "  Max Accuracy: 0.8421, Max Sensitivity: 1.0000, Max Specificity: 0.5833\n",
      "  Avg Num Epoch: 239.52, Avg Training Time: 6.00s, Avg Epoch Time: 0.03s\n",
      "  Avg CPU Usage: 19.66%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.27GB\n",
      "  Avg Max CPU Usage: 54.52%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.27GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag = decoupledGCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "print(decoupled_gcesn_mutag)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_mutag.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_mutag, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_decoupledgcesn_1l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3158\n",
      "Average Sensitivity (Recall): 0.0000\n",
      "Average Specificity: 1.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0008s\n",
      "Average CPU Usage: 44.08%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag = decoupledGCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_mutag.initialize_weights()\n",
    "decoupled_gcesn_mutag.load_state_dict(torch.load('models/best_decoupledgcesn_1l_mutag.pth'))\n",
    "single_test(decoupled_gcesn_mutag.to(device), mutag_test_loader)\n",
    "inference_performance(decoupled_gcesn_mutag.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_1layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 450\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 41.66671, Total Training Time: 10.69s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.5455, Specificity: 0.5000\n",
      "\n",
      "Run 2/50 -> Loss: 49.73309, Total Training Time: 11.02s\n",
      "  Accuracy: 0.2963, Sensitivity: 0.4545, Specificity: 0.1875\n",
      "\n",
      "Run 3/50 -> Loss: 43.67198, Total Training Time: 10.97s\n",
      "  Accuracy: 0.2593, Sensitivity: 0.2727, Specificity: 0.2500\n",
      "\n",
      "Run 4/50 -> Loss: 51.25417, Total Training Time: 10.62s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.4545, Specificity: 0.4375\n",
      "\n",
      "Run 5/50 -> Loss: 48.27971, Total Training Time: 10.74s\n",
      "  Accuracy: 0.2593, Sensitivity: 0.2727, Specificity: 0.2500\n",
      "\n",
      "Run 6/50 -> Loss: 54.26389, Total Training Time: 1.80s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.2727, Specificity: 0.6875\n",
      "\n",
      "Run 7/50 -> Loss: 54.29724, Total Training Time: 2.41s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.4545, Specificity: 0.2500\n",
      "\n",
      "Run 8/50 -> Loss: 39.13849, Total Training Time: 11.51s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.2727, Specificity: 0.3750\n",
      "\n",
      "Run 9/50 -> Loss: 52.25730, Total Training Time: 10.99s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.4545, Specificity: 0.4375\n",
      "\n",
      "Run 10/50 -> Loss: 58.30635, Total Training Time: 11.21s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.3636, Specificity: 0.4375\n",
      "\n",
      "Run 11/50 -> Loss: 47.63602, Total Training Time: 10.80s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.4545, Specificity: 0.5000\n",
      "\n",
      "Run 12/50 -> Loss: 50.32857, Total Training Time: 10.92s\n",
      "  Accuracy: 0.2222, Sensitivity: 0.2727, Specificity: 0.1875\n",
      "\n",
      "Run 13/50 -> Loss: 51.71798, Total Training Time: 10.54s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.5455, Specificity: 0.5625\n",
      "\n",
      "Run 14/50 -> Loss: 43.66884, Total Training Time: 11.85s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.4545, Specificity: 0.4375\n",
      "\n",
      "Run 15/50 -> Loss: 52.30631, Total Training Time: 11.20s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 16/50 -> Loss: 44.78250, Total Training Time: 10.92s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.3636, Specificity: 0.3750\n",
      "\n",
      "Run 17/50 -> Loss: 51.84828, Total Training Time: 2.36s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.3636, Specificity: 0.3750\n",
      "\n",
      "Run 18/50 -> Loss: 44.26674, Total Training Time: 10.89s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.3636, Specificity: 0.4375\n",
      "\n",
      "Run 19/50 -> Loss: 42.69522, Total Training Time: 11.18s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.4545, Specificity: 0.2500\n",
      "\n",
      "Run 20/50 -> Loss: 51.25608, Total Training Time: 2.32s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.3636, Specificity: 0.6875\n",
      "\n",
      "Run 21/50 -> Loss: 53.06762, Total Training Time: 10.98s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.6364, Specificity: 0.3750\n",
      "\n",
      "Run 22/50 -> Loss: 44.96263, Total Training Time: 10.66s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.5455, Specificity: 0.3125\n",
      "\n",
      "Run 23/50 -> Loss: 58.00636, Total Training Time: 10.60s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.3636, Specificity: 0.5625\n",
      "\n",
      "Run 24/50 -> Loss: 48.84482, Total Training Time: 11.00s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.3636, Specificity: 0.4375\n",
      "\n",
      "Run 25/50 -> Loss: 43.97242, Total Training Time: 10.89s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.2727, Specificity: 0.3750\n",
      "\n",
      "Run 26/50 -> Loss: 44.35918, Total Training Time: 10.68s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.4545, Specificity: 0.5000\n",
      "\n",
      "Run 27/50 -> Loss: 43.46925, Total Training Time: 10.49s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3636, Specificity: 0.3125\n",
      "\n",
      "Run 28/50 -> Loss: 41.30496, Total Training Time: 11.58s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.3636, Specificity: 0.3750\n",
      "\n",
      "Run 29/50 -> Loss: 39.72529, Total Training Time: 10.98s\n",
      "  Accuracy: 0.2963, Sensitivity: 0.3636, Specificity: 0.2500\n",
      "\n",
      "Run 30/50 -> Loss: 42.87666, Total Training Time: 2.42s\n",
      "  Accuracy: 0.2963, Sensitivity: 0.3636, Specificity: 0.2500\n",
      "\n",
      "Run 31/50 -> Loss: 44.12257, Total Training Time: 2.39s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 32/50 -> Loss: 45.46803, Total Training Time: 10.89s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.3636, Specificity: 0.4375\n",
      "\n",
      "Run 33/50 -> Loss: 41.88809, Total Training Time: 10.94s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.2727, Specificity: 0.5000\n",
      "\n",
      "Run 34/50 -> Loss: 46.42155, Total Training Time: 11.36s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.4545, Specificity: 0.3125\n",
      "\n",
      "Run 35/50 -> Loss: 45.82631, Total Training Time: 10.55s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.3636, Specificity: 0.5000\n",
      "\n",
      "Run 36/50 -> Loss: 39.22026, Total Training Time: 10.79s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 37/50 -> Loss: 47.07300, Total Training Time: 2.45s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.1818, Specificity: 0.5000\n",
      "\n",
      "Run 38/50 -> Loss: 49.39408, Total Training Time: 11.11s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.3636, Specificity: 0.5000\n",
      "\n",
      "Run 39/50 -> Loss: 50.78454, Total Training Time: 10.82s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.5455, Specificity: 0.4375\n",
      "\n",
      "Run 40/50 -> Loss: 41.66963, Total Training Time: 10.48s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.3636, Specificity: 0.5625\n",
      "\n",
      "Run 41/50 -> Loss: 36.17146, Total Training Time: 10.64s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.1818, Specificity: 0.4375\n",
      "\n",
      "Run 42/50 -> Loss: 38.71564, Total Training Time: 11.07s\n",
      "  Accuracy: 0.3704, Sensitivity: 0.3636, Specificity: 0.3750\n",
      "\n",
      "Run 43/50 -> Loss: 47.00041, Total Training Time: 10.86s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.6364, Specificity: 0.3750\n",
      "\n",
      "Run 44/50 -> Loss: 48.56292, Total Training Time: 2.63s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.5455, Specificity: 0.5000\n",
      "\n",
      "Run 45/50 -> Loss: 39.91356, Total Training Time: 11.50s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.3636, Specificity: 0.3125\n",
      "\n",
      "Run 46/50 -> Loss: 34.38821, Total Training Time: 10.66s\n",
      "  Accuracy: 0.3333, Sensitivity: 0.4545, Specificity: 0.2500\n",
      "\n",
      "Run 47/50 -> Loss: 44.06955, Total Training Time: 10.73s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.5455, Specificity: 0.4375\n",
      "\n",
      "Run 48/50 -> Loss: 35.39868, Total Training Time: 2.44s\n",
      "  Accuracy: 0.5185, Sensitivity: 0.4545, Specificity: 0.5625\n",
      "\n",
      "Run 49/50 -> Loss: 42.20156, Total Training Time: 2.38s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.4545, Specificity: 0.5000\n",
      "\n",
      "Run 50/50 -> Loss: 37.61938, Total Training Time: 10.71s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "Best model saved to models/best_decoupledgcesn_1l_emci.pth with accuracy 0.5556\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.4067 ± 0.08, Avg Sensitivity: 0.4055 ± 0.10, Avg Specificity: 0.4075 ± 0.11\n",
      "  Max Accuracy: 0.5556, Max Sensitivity: 0.6364, Max Specificity: 0.6875\n",
      "  Avg Num Epoch: 221.26, Avg Training Time: 9.21s, Avg Epoch Time: 0.04s\n",
      "  Avg CPU Usage: 21.32%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.27GB\n",
      "  Avg Max CPU Usage: 50.39%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.27GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_emci = decoupledGCESN_1layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=1, num_iterations=1)\n",
    "print(decoupled_gcesn_emci)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_emci.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_emci, emci_train_loader, emci_val_loader, emci_test_loader,\n",
    "                lr=0.01, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_decoupledgcesn_1l_emci.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4074\n",
      "Average Sensitivity (Recall): 1.0000\n",
      "Average Specificity: 0.0000\n",
      "\n",
      "Average Inference Time per Batch: 0.0009s\n",
      "Average CPU Usage: 9.12%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_emci = decoupledGCESN_1layer(emci_num_features, 2*emci_num_features, emci_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_emci.initialize_weights()\n",
    "decoupled_gcesn_emci.load_state_dict(torch.load('models/best_decoupledgcesn_1l_emci.pth'))\n",
    "single_test(decoupled_gcesn_emci.to(device), emci_test_loader)\n",
    "inference_performance(decoupled_gcesn_emci.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_1layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 467\n",
      "\n",
      "\n",
      "Run 1/10 -> Loss: 235.58989, Total Training Time: 44.69s\n",
      "  Accuracy: 0.5000, Sensitivity: 0.2857, Specificity: 0.6154\n",
      "\n",
      "Run 2/10 -> Loss: 228.54427, Total Training Time: 60.16s\n",
      "  Accuracy: 0.6087, Sensitivity: 0.1429, Specificity: 0.8125\n",
      "\n",
      "Run 3/10 -> Loss: 234.29372, Total Training Time: 50.72s\n",
      "  Accuracy: 0.7000, Sensitivity: 0.1667, Specificity: 0.9286\n",
      "\n",
      "Run 4/10 -> Loss: 236.59297, Total Training Time: 34.34s\n",
      "  Accuracy: 0.6552, Sensitivity: 0.4167, Specificity: 0.8235\n",
      "\n",
      "Run 5/10 -> Loss: 225.60957, Total Training Time: 70.10s\n",
      "  Accuracy: 0.6250, Sensitivity: 0.5000, Specificity: 0.6875\n",
      "\n",
      "Run 6/10 -> Loss: 221.92148, Total Training Time: 51.15s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.3000, Specificity: 0.7647\n",
      "\n",
      "Run 7/10 -> Loss: 218.74750, Total Training Time: 55.02s\n",
      "  Accuracy: 0.6538, Sensitivity: 0.4000, Specificity: 0.8125\n",
      "\n",
      "Run 8/10 -> Loss: 221.53723, Total Training Time: 42.92s\n",
      "  Accuracy: 0.6071, Sensitivity: 0.3750, Specificity: 0.7000\n",
      "\n",
      "Run 9/10 -> Loss: 222.16288, Total Training Time: 40.76s\n",
      "  Accuracy: 0.5357, Sensitivity: 0.3000, Specificity: 0.6667\n",
      "\n",
      "Run 10/10 -> Loss: 223.69826, Total Training Time: 47.79s\n",
      "  Accuracy: 0.6296, Sensitivity: 0.3333, Specificity: 0.7778\n",
      "Best model saved to models/best_decoupledgcesn_1l_slim160.pth with accuracy 0.7000\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.6108 ± 0.06, Avg Sensitivity: 0.3220 ± 0.10, Avg Specificity: 0.7589 ± 0.09\n",
      "  Max Accuracy: 0.7000, Max Sensitivity: 0.5000, Max Specificity: 0.9286\n",
      "  Avg Num Epoch: 261.50, Avg Training Time: 49.77s, Avg Epoch Time: 0.19s\n",
      "  Avg CPU Usage: 66.54%, Avg GPU Usage: 0.06%, Avg Memory Usage: 3.28GB\n",
      "  Avg Max CPU Usage: 90.98%, Avg Max GPU Usage: 0.06GB, Avg Max Memory Usage: 3.30GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_slim160 = decoupledGCESN_1layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=0.9, num_iterations=2)\n",
    "print(decoupled_gcesn_slim160)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_slim160.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_slim160, slim160_train_loader, slim160_val_loader, slim160_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=10, binary_classification=True, best_model_path='models/best_decoupledgcesn_1l_slim160.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4762\n",
      "Average Sensitivity (Recall): 1.0000\n",
      "Average Specificity: 0.0435\n",
      "\n",
      "Average Inference Time per Batch: 0.0012s\n",
      "Average CPU Usage: 37.88%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.09GB\n",
      "Average GPU Utilization: 2.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_slim160 = decoupledGCESN_1layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_slim160.initialize_weights()\n",
    "decoupled_gcesn_slim160.load_state_dict(torch.load('models/best_decoupledgcesn_1l_slim160.pth'))\n",
    "single_test(decoupled_gcesn_slim160.to(device), slim160_test_loader)\n",
    "inference_performance(decoupled_gcesn_slim160.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_2layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=28, bias=True)\n",
      "  )\n",
      "  (gcn2): GCN (14 -> 28)\n",
      "  (fc): Linear(in_features=28, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1010\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 67.67875, Total Training Time: 6.59s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 2/50 -> Loss: 64.16983, Total Training Time: 6.42s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 3/50 -> Loss: 60.70878, Total Training Time: 6.51s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 4/50 -> Loss: 57.59285, Total Training Time: 6.40s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 5/50 -> Loss: 57.21628, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 6/50 -> Loss: 56.81608, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 7/50 -> Loss: 56.43107, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 8/50 -> Loss: 56.06303, Total Training Time: 0.19s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 9/50 -> Loss: 55.69059, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 10/50 -> Loss: 55.33027, Total Training Time: 0.21s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 11/50 -> Loss: 54.97465, Total Training Time: 0.25s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 12/50 -> Loss: 54.61679, Total Training Time: 0.26s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 13/50 -> Loss: 54.27058, Total Training Time: 0.26s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 14/50 -> Loss: 53.92298, Total Training Time: 0.22s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 15/50 -> Loss: 53.56959, Total Training Time: 0.23s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 16/50 -> Loss: 53.22609, Total Training Time: 0.24s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 17/50 -> Loss: 52.72015, Total Training Time: 0.22s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 18/50 -> Loss: 52.41990, Total Training Time: 0.18s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 19/50 -> Loss: 52.11421, Total Training Time: 0.18s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 20/50 -> Loss: 51.81556, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 21/50 -> Loss: 51.51345, Total Training Time: 0.17s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 22/50 -> Loss: 51.22368, Total Training Time: 0.18s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 23/50 -> Loss: 48.57334, Total Training Time: 6.54s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 24/50 -> Loss: 46.10546, Total Training Time: 6.43s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 25/50 -> Loss: 43.91505, Total Training Time: 6.61s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 26/50 -> Loss: 42.11450, Total Training Time: 6.47s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 27/50 -> Loss: 40.68311, Total Training Time: 6.54s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 28/50 -> Loss: 40.55140, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 29/50 -> Loss: 39.39565, Total Training Time: 5.46s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 30/50 -> Loss: 39.46050, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 31/50 -> Loss: 39.28992, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 32/50 -> Loss: 39.16496, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 33/50 -> Loss: 39.04438, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 34/50 -> Loss: 38.91974, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 35/50 -> Loss: 38.85313, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 36/50 -> Loss: 38.74127, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 37/50 -> Loss: 38.63717, Total Training Time: 0.27s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 38/50 -> Loss: 38.53826, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 39/50 -> Loss: 38.44313, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 40/50 -> Loss: 38.34868, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 41/50 -> Loss: 38.25723, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 42/50 -> Loss: 38.16646, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 43/50 -> Loss: 38.07829, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 44/50 -> Loss: 37.99156, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 45/50 -> Loss: 37.90932, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 46/50 -> Loss: 37.82303, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 47/50 -> Loss: 37.74322, Total Training Time: 0.26s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 48/50 -> Loss: 37.66187, Total Training Time: 0.26s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 49/50 -> Loss: 37.44146, Total Training Time: 0.34s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 50/50 -> Loss: 37.34498, Total Training Time: 0.29s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "Best model saved to models/best_decoupledgcesn_2l_mutag.pth with accuracy 0.9211\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.8811 ± 0.04, Avg Sensitivity: 0.9115 ± 0.02, Avg Specificity: 0.8150 ± 0.11\n",
      "  Max Accuracy: 0.9211, Max Sensitivity: 0.9231, Max Specificity: 0.9167\n",
      "  Avg Num Epoch: 57.40, Avg Training Time: 1.46s, Avg Epoch Time: 0.03s\n",
      "  Avg CPU Usage: 19.34%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.28GB\n",
      "  Avg Max CPU Usage: 35.88%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.28GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag_2 = decoupledGCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "print(decoupled_gcesn_mutag_2)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_mutag_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_mutag_2, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_decoupledgcesn_2l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211\n",
      "Average Sensitivity (Recall): 0.9231\n",
      "Average Specificity: 0.9167\n",
      "\n",
      "Average Inference Time per Batch: 0.0011s\n",
      "Average CPU Usage: 54.38%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 8.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag_2 = decoupledGCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_mutag_2.initialize_weights()\n",
    "decoupled_gcesn_mutag_2.load_state_dict(torch.load('models/best_decoupledgcesn_2l_mutag.pth'))\n",
    "single_test(decoupled_gcesn_mutag_2.to(device), mutag_test_loader)\n",
    "inference_performance(decoupled_gcesn_mutag_2.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_2layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1298\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 66.44632, Total Training Time: 0.30s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 2/50 -> Loss: 66.38443, Total Training Time: 0.31s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 3/50 -> Loss: 66.29881, Total Training Time: 0.31s\n",
      "  Accuracy: 0.4074, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "\n",
      "Run 4/50 -> Loss: 64.11977, Total Training Time: 11.66s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.6364, Specificity: 0.5625\n",
      "\n",
      "Run 5/50 -> Loss: 61.30729, Total Training Time: 11.54s\n",
      "  Accuracy: 0.5556, Sensitivity: 0.6364, Specificity: 0.5000\n",
      "\n",
      "Run 6/50 -> Loss: 56.90317, Total Training Time: 11.56s\n",
      "  Accuracy: 0.4815, Sensitivity: 0.6364, Specificity: 0.3750\n",
      "\n",
      "Run 7/50 -> Loss: 52.06110, Total Training Time: 11.61s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 8/50 -> Loss: 47.86638, Total Training Time: 11.26s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 9/50 -> Loss: 44.21390, Total Training Time: 9.07s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 10/50 -> Loss: 40.84012, Total Training Time: 9.51s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 11/50 -> Loss: 38.82384, Total Training Time: 2.57s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 12/50 -> Loss: 37.02069, Total Training Time: 2.65s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 13/50 -> Loss: 35.38808, Total Training Time: 2.42s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 14/50 -> Loss: 35.79930, Total Training Time: 0.55s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 15/50 -> Loss: 35.54057, Total Training Time: 0.46s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 16/50 -> Loss: 35.35708, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 17/50 -> Loss: 35.19296, Total Training Time: 0.44s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 18/50 -> Loss: 35.02877, Total Training Time: 0.45s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 19/50 -> Loss: 34.88505, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 20/50 -> Loss: 34.74684, Total Training Time: 0.47s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 21/50 -> Loss: 34.60498, Total Training Time: 0.51s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 22/50 -> Loss: 34.45550, Total Training Time: 0.49s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 23/50 -> Loss: 34.31216, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 24/50 -> Loss: 34.16036, Total Training Time: 0.49s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 25/50 -> Loss: 34.03816, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 26/50 -> Loss: 33.91822, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 27/50 -> Loss: 33.80956, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 28/50 -> Loss: 33.70740, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 29/50 -> Loss: 33.63149, Total Training Time: 0.51s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 30/50 -> Loss: 33.51182, Total Training Time: 0.58s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 31/50 -> Loss: 33.29646, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 32/50 -> Loss: 33.19642, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 33/50 -> Loss: 33.10735, Total Training Time: 0.47s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 34/50 -> Loss: 33.02314, Total Training Time: 0.50s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 35/50 -> Loss: 32.92581, Total Training Time: 0.44s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 36/50 -> Loss: 32.83831, Total Training Time: 0.45s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 37/50 -> Loss: 32.77683, Total Training Time: 0.45s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 38/50 -> Loss: 32.68112, Total Training Time: 0.48s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 39/50 -> Loss: 32.72616, Total Training Time: 0.63s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 40/50 -> Loss: 32.57393, Total Training Time: 0.66s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 41/50 -> Loss: 32.39788, Total Training Time: 0.47s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 42/50 -> Loss: 32.42968, Total Training Time: 0.68s\n",
      "  Accuracy: 0.4444, Sensitivity: 0.5455, Specificity: 0.3750\n",
      "\n",
      "Run 43/50 -> Loss: 32.38783, Total Training Time: 1.06s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 44/50 -> Loss: 32.09117, Total Training Time: 0.51s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 45/50 -> Loss: 32.13872, Total Training Time: 0.69s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 46/50 -> Loss: 32.07422, Total Training Time: 0.65s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 47/50 -> Loss: 31.98915, Total Training Time: 0.70s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 48/50 -> Loss: 31.99120, Total Training Time: 0.98s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 49/50 -> Loss: 31.81127, Total Training Time: 0.64s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "\n",
      "Run 50/50 -> Loss: 31.78859, Total Training Time: 0.96s\n",
      "  Accuracy: 0.4074, Sensitivity: 0.4545, Specificity: 0.3750\n",
      "Best model saved to models/best_decoupledgcesn_2l_emci.pth with accuracy 0.5926\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.4200 ± 0.04, Avg Sensitivity: 0.5091 ± 0.13, Avg Specificity: 0.3588 ± 0.10\n",
      "  Max Accuracy: 0.5926, Max Sensitivity: 1.0000, Max Specificity: 0.5625\n",
      "  Avg Num Epoch: 47.98, Avg Training Time: 2.11s, Avg Epoch Time: 0.04s\n",
      "  Avg CPU Usage: 24.21%, Avg GPU Usage: 0.02%, Avg Memory Usage: 3.28GB\n",
      "  Avg Max CPU Usage: 38.05%, Avg Max GPU Usage: 0.02GB, Avg Max Memory Usage: 3.28GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_emci_2 = decoupledGCESN_2layer(emci_num_features, 2*emci_num_features, emci_num_classes, leaky_rate=0.7, num_iterations=1, ridge_alpha=0.9)\n",
    "print(decoupled_gcesn_emci_2)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_emci_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_emci_2, emci_train_loader, emci_val_loader, emci_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_decoupledgcesn_2l_emci.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4074\n",
      "Average Sensitivity (Recall): 0.4545\n",
      "Average Specificity: 0.3750\n",
      "\n",
      "Average Inference Time per Batch: 0.0009s\n",
      "Average CPU Usage: 10.15%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 7.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_emci_2 = decoupledGCESN_2layer(emci_num_features, 2*emci_num_features, emci_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_emci_2.initialize_weights()\n",
    "decoupled_gcesn_emci_2.load_state_dict(torch.load('models/best_decoupledgcesn_2l_emci.pth'))\n",
    "single_test(decoupled_gcesn_emci_2.to(device), emci_test_loader)\n",
    "inference_performance(decoupled_gcesn_emci_2.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_2layer(\n",
      "  (gcn1): GCN (8 -> 16)\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=16, out_features=32, bias=True)\n",
      "  )\n",
      "  (gcn2): GCN (16 -> 32)\n",
      "  (fc): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1331\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 199.30466, Total Training Time: 43.22s\n",
      "  Accuracy: 0.5333, Sensitivity: 0.3077, Specificity: 0.7059\n",
      "\n",
      "Run 2/50 -> Loss: 160.21583, Total Training Time: 56.91s\n",
      "  Accuracy: 0.5161, Sensitivity: 0.4615, Specificity: 0.5556\n",
      "\n",
      "Run 3/50 -> Loss: 132.13584, Total Training Time: 56.69s\n",
      "  Accuracy: 0.6452, Sensitivity: 0.6429, Specificity: 0.6471\n",
      "\n",
      "Run 4/50 -> Loss: 109.93535, Total Training Time: 50.70s\n",
      "  Accuracy: 0.6129, Sensitivity: 0.6429, Specificity: 0.5882\n",
      "\n",
      "Run 5/50 -> Loss: 110.27690, Total Training Time: 1.34s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.6429, Specificity: 0.5294\n",
      "\n",
      "Run 6/50 -> Loss: 107.12594, Total Training Time: 1.63s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.6429, Specificity: 0.5294\n",
      "\n",
      "Run 7/50 -> Loss: 103.95535, Total Training Time: 1.40s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.6429, Specificity: 0.5294\n",
      "\n",
      "Run 8/50 -> Loss: 100.83120, Total Training Time: 4.12s\n",
      "  Accuracy: 0.6129, Sensitivity: 0.6429, Specificity: 0.5882\n",
      "\n",
      "Run 9/50 -> Loss: 97.88982, Total Training Time: 1.44s\n",
      "  Accuracy: 0.6129, Sensitivity: 0.6429, Specificity: 0.5882\n",
      "\n",
      "Run 10/50 -> Loss: 94.94650, Total Training Time: 1.47s\n",
      "  Accuracy: 0.6000, Sensitivity: 0.6429, Specificity: 0.5625\n",
      "\n",
      "Run 11/50 -> Loss: 92.08071, Total Training Time: 1.44s\n",
      "  Accuracy: 0.6000, Sensitivity: 0.6429, Specificity: 0.5625\n",
      "\n",
      "Run 12/50 -> Loss: 68.84185, Total Training Time: 45.99s\n",
      "  Accuracy: 0.6207, Sensitivity: 0.6154, Specificity: 0.6250\n",
      "\n",
      "Run 13/50 -> Loss: 64.10188, Total Training Time: 5.70s\n",
      "  Accuracy: 0.6061, Sensitivity: 0.6667, Specificity: 0.5556\n",
      "\n",
      "Run 14/50 -> Loss: 41.39237, Total Training Time: 61.58s\n",
      "  Accuracy: 0.6667, Sensitivity: 0.5833, Specificity: 0.7333\n",
      "\n",
      "Run 15/50 -> Loss: 46.00323, Total Training Time: 4.06s\n",
      "  Accuracy: 0.6667, Sensitivity: 0.5833, Specificity: 0.7333\n",
      "\n",
      "Run 16/50 -> Loss: 37.09146, Total Training Time: 3.73s\n",
      "  Accuracy: 0.6364, Sensitivity: 0.6429, Specificity: 0.6316\n",
      "\n",
      "Run 17/50 -> Loss: 31.64980, Total Training Time: 20.40s\n",
      "  Accuracy: 0.6250, Sensitivity: 0.6154, Specificity: 0.6316\n",
      "\n",
      "Run 18/50 -> Loss: 27.63491, Total Training Time: 4.28s\n",
      "  Accuracy: 0.6667, Sensitivity: 0.5833, Specificity: 0.7333\n",
      "\n",
      "Run 19/50 -> Loss: 29.03707, Total Training Time: 14.19s\n",
      "  Accuracy: 0.6957, Sensitivity: 0.5556, Specificity: 0.7857\n",
      "\n",
      "Run 20/50 -> Loss: 74.53248, Total Training Time: 10.44s\n",
      "  Accuracy: 0.5926, Sensitivity: 0.8182, Specificity: 0.4375\n",
      "\n",
      "Run 21/50 -> Loss: 20.10253, Total Training Time: 6.85s\n",
      "  Accuracy: 0.6129, Sensitivity: 0.5385, Specificity: 0.6667\n",
      "\n",
      "Run 22/50 -> Loss: 14.05014, Total Training Time: 8.02s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.5385, Specificity: 0.6111\n",
      "\n",
      "Run 23/50 -> Loss: 52.07829, Total Training Time: 6.04s\n",
      "  Accuracy: 0.6250, Sensitivity: 0.5000, Specificity: 0.7143\n",
      "\n",
      "Run 24/50 -> Loss: 14.06901, Total Training Time: 4.06s\n",
      "  Accuracy: 0.5600, Sensitivity: 0.3000, Specificity: 0.7333\n",
      "\n",
      "Run 25/50 -> Loss: 9.97542, Total Training Time: 8.68s\n",
      "  Accuracy: 0.6452, Sensitivity: 0.6923, Specificity: 0.6111\n",
      "\n",
      "Run 26/50 -> Loss: 19.86038, Total Training Time: 4.63s\n",
      "  Accuracy: 0.6087, Sensitivity: 0.3333, Specificity: 0.7857\n",
      "\n",
      "Run 27/50 -> Loss: 6.06934, Total Training Time: 47.64s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.5385, Specificity: 0.6111\n",
      "\n",
      "Run 28/50 -> Loss: 14.09211, Total Training Time: 12.10s\n",
      "  Accuracy: 0.6087, Sensitivity: 0.3333, Specificity: 0.7857\n",
      "\n",
      "Run 29/50 -> Loss: 4.21247, Total Training Time: 57.09s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.5385, Specificity: 0.6111\n",
      "\n",
      "Run 30/50 -> Loss: 4.24959, Total Training Time: 3.00s\n",
      "  Accuracy: 0.5806, Sensitivity: 0.5385, Specificity: 0.6111\n",
      "\n",
      "Run 31/50 -> Loss: 4.08655, Total Training Time: 15.37s\n",
      "  Accuracy: 0.5667, Sensitivity: 0.4615, Specificity: 0.6471\n",
      "\n",
      "Run 32/50 -> Loss: 3.02792, Total Training Time: 106.39s\n",
      "  Accuracy: 0.6207, Sensitivity: 0.5385, Specificity: 0.6875\n",
      "\n",
      "Run 33/50 -> Loss: 2.51957, Total Training Time: 75.81s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 34/50 -> Loss: 2.16114, Total Training Time: 66.89s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 35/50 -> Loss: 1.88828, Total Training Time: 41.32s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 36/50 -> Loss: 1.67036, Total Training Time: 38.33s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 37/50 -> Loss: 1.48430, Total Training Time: 51.48s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 38/50 -> Loss: 1.32755, Total Training Time: 88.10s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 39/50 -> Loss: 1.19624, Total Training Time: 37.56s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 40/50 -> Loss: 1.08599, Total Training Time: 64.98s\n",
      "  Accuracy: 0.5862, Sensitivity: 0.4615, Specificity: 0.6875\n",
      "\n",
      "Run 41/50 -> Loss: 0.98485, Total Training Time: 79.37s\n",
      "  Accuracy: 0.5667, Sensitivity: 0.4615, Specificity: 0.6471\n",
      "\n",
      "Run 42/50 -> Loss: 0.89841, Total Training Time: 39.32s\n",
      "  Accuracy: 0.5667, Sensitivity: 0.4615, Specificity: 0.6471\n",
      "\n",
      "Run 43/50 -> Loss: 0.82551, Total Training Time: 43.53s\n",
      "  Accuracy: 0.5667, Sensitivity: 0.4615, Specificity: 0.6471\n",
      "\n",
      "Run 44/50 -> Loss: 0.76172, Total Training Time: 44.96s\n",
      "  Accuracy: 0.5667, Sensitivity: 0.4615, Specificity: 0.6471\n",
      "\n",
      "Run 45/50 -> Loss: 0.70388, Total Training Time: 61.58s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "\n",
      "Run 46/50 -> Loss: 0.64852, Total Training Time: 52.49s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "\n",
      "Run 47/50 -> Loss: 0.60547, Total Training Time: 51.10s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "\n",
      "Run 48/50 -> Loss: 0.55035, Total Training Time: 64.44s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "\n",
      "Run 49/50 -> Loss: 0.51661, Total Training Time: 32.24s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "\n",
      "Run 50/50 -> Loss: 0.47331, Total Training Time: 75.66s\n",
      "  Accuracy: 0.5517, Sensitivity: 0.4615, Specificity: 0.6250\n",
      "Best model saved to models/best_decoupledgcesn_2l_slim160.pth with accuracy 0.6957\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.5938 ± 0.04, Avg Sensitivity: 0.5276 ± 0.11, Avg Specificity: 0.6433 ± 0.07\n",
      "  Max Accuracy: 0.6957, Max Sensitivity: 0.8182, Max Specificity: 0.7857\n",
      "  Avg Num Epoch: 137.06, Avg Training Time: 33.60s, Avg Epoch Time: 0.25s\n",
      "  Avg CPU Usage: 66.60%, Avg GPU Usage: 0.06%, Avg Memory Usage: 3.28GB\n",
      "  Avg Max CPU Usage: 86.97%, Avg Max GPU Usage: 0.06GB, Avg Max Memory Usage: 3.30GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_slim160_2 = decoupledGCESN_2layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, leaky_rate=1, num_iterations=1)\n",
    "print(decoupled_gcesn_slim160_2)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_slim160_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_slim160_2, slim160_train_loader, slim160_val_loader, slim160_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True, best_model_path='models/best_decoupledgcesn_2l_slim160.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5517\n",
      "Average Sensitivity (Recall): 0.4615\n",
      "Average Specificity: 0.6250\n",
      "\n",
      "Average Inference Time per Batch: 0.0030s\n",
      "Average CPU Usage: 75.30%\n",
      "Average Memory Usage: 3.28GB\n",
      "Average GPU Usage: 0.09GB\n",
      "Average GPU Utilization: 2.00%\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_slim160_2 = decoupledGCESN_2layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, num_iterations=2)\n",
    "decoupled_gcesn_slim160_2.initialize_weights()\n",
    "decoupled_gcesn_slim160_2.load_state_dict(torch.load('models/best_decoupledgcesn_2l_slim160.pth'))\n",
    "single_test(decoupled_gcesn_slim160_2.to(device), slim160_test_loader)\n",
    "inference_performance(decoupled_gcesn_slim160_2.to(device), slim160_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable GCESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 534\n",
      "\n",
      "Epoch 1, Train Loss: 127.88738405704498, Val Loss: 20.463053584098816\n",
      "Time: 0.44s, CPU: 17.25%, Memory: 3.30GB, GPU: 0.02GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 96.2626564502716, Val Loss: 14.868319630622864\n",
      "Time: 0.05s, CPU: 28.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 0.00%\n",
      "Epoch 3, Train Loss: 77.71163195371628, Val Loss: 11.740573346614838\n",
      "Time: 0.03s, CPU: 29.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 2.00%\n",
      "Epoch 4, Train Loss: 73.72892814874649, Val Loss: 10.775383114814758\n",
      "Time: 0.03s, CPU: 27.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 5, Train Loss: 74.98957723379135, Val Loss: 10.51710158586502\n",
      "Time: 0.04s, CPU: 22.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 6, Train Loss: 74.14107310771942, Val Loss: 10.471301078796387\n",
      "Time: 0.03s, CPU: 24.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 4.00%\n",
      "Epoch 7, Train Loss: 73.71823626756668, Val Loss: 10.526301562786102\n",
      "Time: 0.03s, CPU: 25.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 8, Train Loss: 72.77032274007797, Val Loss: 10.667933821678162\n",
      "Time: 0.03s, CPU: 31.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 9, Train Loss: 71.7225998044014, Val Loss: 10.798599421977997\n",
      "Time: 0.04s, CPU: 19.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 10, Train Loss: 70.91532278060913, Val Loss: 10.814155340194702\n",
      "Time: 0.04s, CPU: 26.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 11, Train Loss: 70.38741427659988, Val Loss: 10.738265812397003\n",
      "Time: 0.04s, CPU: 22.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 12, Train Loss: 69.77368980646133, Val Loss: 10.616628527641296\n",
      "Time: 0.04s, CPU: 24.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 13, Train Loss: 69.05609422922134, Val Loss: 10.477105379104614\n",
      "Time: 0.04s, CPU: 32.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 14, Train Loss: 68.2811188697815, Val Loss: 10.326927602291107\n",
      "Time: 0.03s, CPU: 22.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 15, Train Loss: 67.56787353754044, Val Loss: 10.138109922409058\n",
      "Time: 0.03s, CPU: 22.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 16, Train Loss: 66.99574548006058, Val Loss: 9.96815800666809\n",
      "Time: 0.04s, CPU: 25.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 17, Train Loss: 66.61031776666641, Val Loss: 9.909550845623016\n",
      "Time: 0.03s, CPU: 29.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 18, Train Loss: 66.22903937101364, Val Loss: 9.90735948085785\n",
      "Time: 0.04s, CPU: 33.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 19, Train Loss: 65.78957170248032, Val Loss: 9.80807214975357\n",
      "Time: 0.04s, CPU: 22.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 20, Train Loss: 65.4032735824585, Val Loss: 9.662035703659058\n",
      "Time: 0.03s, CPU: 29.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 21, Train Loss: 65.1058378815651, Val Loss: 9.569377899169922\n",
      "Time: 0.03s, CPU: 23.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 22, Train Loss: 64.8931457400322, Val Loss: 9.567065834999084\n",
      "Time: 0.04s, CPU: 31.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 23, Train Loss: 64.62597745656967, Val Loss: 9.536010324954987\n",
      "Time: 0.03s, CPU: 27.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 24, Train Loss: 64.46253621578217, Val Loss: 9.432619214057922\n",
      "Time: 0.04s, CPU: 13.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 25, Train Loss: 64.38304221630096, Val Loss: 9.374612867832184\n",
      "Time: 0.04s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 26, Train Loss: 64.29955923557281, Val Loss: 9.380089044570923\n",
      "Time: 0.04s, CPU: 25.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 27, Train Loss: 64.1706468462944, Val Loss: 9.368370473384857\n",
      "Time: 0.05s, CPU: 41.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 28, Train Loss: 64.07276636362076, Val Loss: 9.32757943868637\n",
      "Time: 0.04s, CPU: 32.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 29, Train Loss: 64.01870852708817, Val Loss: 9.31637316942215\n",
      "Time: 0.04s, CPU: 12.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 30, Train Loss: 63.94611829519272, Val Loss: 9.308000206947327\n",
      "Time: 0.04s, CPU: 23.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 31, Train Loss: 63.8767471909523, Val Loss: 9.280266165733337\n",
      "Time: 0.04s, CPU: 29.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 32, Train Loss: 63.84580743312836, Val Loss: 9.263933300971985\n",
      "Time: 0.03s, CPU: 27.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 33, Train Loss: 63.7987095117569, Val Loss: 9.259185791015625\n",
      "Time: 0.04s, CPU: 30.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 34, Train Loss: 63.74500286579132, Val Loss: 9.219282567501068\n",
      "Time: 0.03s, CPU: 22.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 35, Train Loss: 63.74682277441025, Val Loss: 9.211277067661285\n",
      "Time: 0.04s, CPU: 21.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 36, Train Loss: 63.71689313650131, Val Loss: 9.22035276889801\n",
      "Time: 0.03s, CPU: 25.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 37, Train Loss: 63.67377930879593, Val Loss: 9.208299815654755\n",
      "Time: 0.04s, CPU: 25.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 38, Train Loss: 63.64562773704529, Val Loss: 9.17560189962387\n",
      "Time: 0.04s, CPU: 20.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 39, Train Loss: 63.641088128089905, Val Loss: 9.164737164974213\n",
      "Time: 0.03s, CPU: 20.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 40, Train Loss: 63.619561553001404, Val Loss: 9.170030951499939\n",
      "Time: 0.03s, CPU: 27.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 41, Train Loss: 63.58352476358414, Val Loss: 9.16601300239563\n",
      "Time: 0.03s, CPU: 29.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 42, Train Loss: 63.55326026678085, Val Loss: 9.14345920085907\n",
      "Time: 0.03s, CPU: 21.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 43, Train Loss: 63.53693413734436, Val Loss: 9.137162268161774\n",
      "Time: 0.03s, CPU: 23.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 44, Train Loss: 63.511390805244446, Val Loss: 9.138635694980621\n",
      "Time: 0.04s, CPU: 22.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 45, Train Loss: 63.47720456123352, Val Loss: 9.10462349653244\n",
      "Time: 0.04s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 46, Train Loss: 63.47305464744568, Val Loss: 9.104913175106049\n",
      "Time: 0.03s, CPU: 27.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 47, Train Loss: 63.450322926044464, Val Loss: 9.110429584980011\n",
      "Time: 0.03s, CPU: 12.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 48, Train Loss: 63.41894990205765, Val Loss: 9.09842848777771\n",
      "Time: 0.04s, CPU: 31.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 49, Train Loss: 63.404580652713776, Val Loss: 9.08859372138977\n",
      "Time: 0.03s, CPU: 20.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 50, Train Loss: 63.38476610183716, Val Loss: 9.088680446147919\n",
      "Time: 0.04s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 51, Train Loss: 63.35578262805939, Val Loss: 9.08610463142395\n",
      "Time: 0.03s, CPU: 25.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 52, Train Loss: 63.327414870262146, Val Loss: 9.071536660194397\n",
      "Time: 0.04s, CPU: 23.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.00%\n",
      "Epoch 53, Train Loss: 63.308590948581696, Val Loss: 9.071421325206757\n",
      "Time: 0.03s, CPU: 19.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 5.50%\n",
      "Epoch 54, Train Loss: 63.28200250864029, Val Loss: 9.066300094127655\n",
      "Time: 0.04s, CPU: 25.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 55, Train Loss: 63.25414431095123, Val Loss: 9.054189026355743\n",
      "Time: 0.04s, CPU: 27.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 56, Train Loss: 63.22900873422623, Val Loss: 9.066219627857208\n",
      "Time: 0.04s, CPU: 19.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 57, Train Loss: 63.19079542160034, Val Loss: 9.062836468219757\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 58, Train Loss: 63.170403093099594, Val Loss: 9.043964445590973\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 59, Train Loss: 63.15230020880699, Val Loss: 9.050479531288147\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 60, Train Loss: 63.1083919107914, Val Loss: 9.053311049938202\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 61, Train Loss: 63.07181906700134, Val Loss: 9.04747098684311\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 62, Train Loss: 63.040933310985565, Val Loss: 9.04587596654892\n",
      "Time: 0.03s, CPU: 10.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 63, Train Loss: 63.012034088373184, Val Loss: 9.048883616924286\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 64, Train Loss: 62.97984963655472, Val Loss: 9.040419459342957\n",
      "Time: 0.04s, CPU: 6.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 65, Train Loss: 62.95985221862793, Val Loss: 9.035872220993042\n",
      "Time: 0.04s, CPU: 17.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 66, Train Loss: 62.931831538677216, Val Loss: 9.029671847820282\n",
      "Time: 0.04s, CPU: 13.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 67, Train Loss: 62.90522140264511, Val Loss: 9.034454226493835\n",
      "Time: 0.03s, CPU: 15.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 68, Train Loss: 62.87683993577957, Val Loss: 9.045544266700745\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 69, Train Loss: 62.83748671412468, Val Loss: 9.03465449810028\n",
      "Time: 0.03s, CPU: 8.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 70, Train Loss: 62.81399583816528, Val Loss: 9.028944075107574\n",
      "Time: 0.03s, CPU: 11.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 71, Train Loss: 62.785093039274216, Val Loss: 9.02829498052597\n",
      "Time: 0.03s, CPU: 16.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 72, Train Loss: 62.754150450229645, Val Loss: 9.030847549438477\n",
      "Time: 0.03s, CPU: 10.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 73, Train Loss: 62.71612906455994, Val Loss: 9.04316246509552\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 74, Train Loss: 62.666275680065155, Val Loss: 9.050697684288025\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 75, Train Loss: 62.62212038040161, Val Loss: 9.041959047317505\n",
      "Time: 0.03s, CPU: 8.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 76, Train Loss: 62.589734107255936, Val Loss: 9.031619131565094\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 77, Train Loss: 62.56188902258873, Val Loss: 9.035899043083191\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 78, Train Loss: 62.513861417770386, Val Loss: 9.020212590694427\n",
      "Time: 0.04s, CPU: 8.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 79, Train Loss: 62.49052181839943, Val Loss: 9.037211537361145\n",
      "Time: 0.04s, CPU: 20.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 80, Train Loss: 62.43380305171013, Val Loss: 9.045303761959076\n",
      "Time: 0.04s, CPU: 16.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 81, Train Loss: 62.38977962732315, Val Loss: 9.03238445520401\n",
      "Time: 0.04s, CPU: 14.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 82, Train Loss: 62.35613697767258, Val Loss: 9.042105674743652\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 83, Train Loss: 62.2682686150074, Val Loss: 9.02639240026474\n",
      "Time: 0.03s, CPU: 9.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 84, Train Loss: 62.22722485661507, Val Loss: 9.026565849781036\n",
      "Time: 0.04s, CPU: 15.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 85, Train Loss: 62.15244480967522, Val Loss: 9.047435224056244\n",
      "Time: 0.04s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 86, Train Loss: 62.074478685855865, Val Loss: 9.058303534984589\n",
      "Time: 0.04s, CPU: 20.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 87, Train Loss: 61.98983946442604, Val Loss: 9.042567908763885\n",
      "Time: 0.03s, CPU: 18.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 88, Train Loss: 61.9034429192543, Val Loss: 9.060072004795074\n",
      "Time: 0.03s, CPU: 8.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 89, Train Loss: 61.801799952983856, Val Loss: 9.047042727470398\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 90, Train Loss: 61.69224292039871, Val Loss: 9.056978523731232\n",
      "Time: 0.03s, CPU: 15.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 91, Train Loss: 61.54455849528313, Val Loss: 9.071899652481079\n",
      "Time: 0.04s, CPU: 13.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 92, Train Loss: 61.38912209868431, Val Loss: 9.095033705234528\n",
      "Time: 0.04s, CPU: 14.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 93, Train Loss: 61.19416955113411, Val Loss: 9.045149087905884\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 94, Train Loss: 61.09970313310623, Val Loss: 9.003902077674866\n",
      "Time: 0.03s, CPU: 8.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 95, Train Loss: 61.07473322749138, Val Loss: 9.150337278842926\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 96, Train Loss: 60.75485756993294, Val Loss: 9.110110402107239\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 97, Train Loss: 60.68245095014572, Val Loss: 9.103112518787384\n",
      "Time: 0.03s, CPU: 19.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 98, Train Loss: 60.66246536374092, Val Loss: 9.23536241054535\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 99, Train Loss: 60.41547539830208, Val Loss: 9.214193522930145\n",
      "Time: 0.03s, CPU: 20.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 100, Train Loss: 60.35033303499222, Val Loss: 9.250345230102539\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 101, Train Loss: 60.290037363767624, Val Loss: 9.249722063541412\n",
      "Time: 0.03s, CPU: 46.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 102, Train Loss: 60.25856611132622, Val Loss: 9.252046644687653\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 103, Train Loss: 60.206113666296005, Val Loss: 9.24473762512207\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 104, Train Loss: 60.17276105284691, Val Loss: 9.24166738986969\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 105, Train Loss: 60.16004613041878, Val Loss: 9.247765839099884\n",
      "Time: 0.04s, CPU: 26.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 106, Train Loss: 60.158954322338104, Val Loss: 9.264573454856873\n",
      "Time: 0.04s, CPU: 31.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 107, Train Loss: 60.14759197831154, Val Loss: 9.273093044757843\n",
      "Time: 0.04s, CPU: 26.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 108, Train Loss: 60.144789189100266, Val Loss: 9.273193180561066\n",
      "Time: 0.04s, CPU: 35.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 109, Train Loss: 60.14068126678467, Val Loss: 9.269959330558777\n",
      "Time: 0.04s, CPU: 20.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 110, Train Loss: 60.13426002860069, Val Loss: 9.271774291992188\n",
      "Time: 0.03s, CPU: 22.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 111, Train Loss: 60.11719134449959, Val Loss: 9.272238314151764\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 112, Train Loss: 60.107237845659256, Val Loss: 9.270874857902527\n",
      "Time: 0.03s, CPU: 8.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 113, Train Loss: 60.10637637972832, Val Loss: 9.272402822971344\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 114, Train Loss: 60.09962645173073, Val Loss: 9.27545964717865\n",
      "Time: 0.03s, CPU: 22.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 115, Train Loss: 60.090533286333084, Val Loss: 9.276304543018341\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 116, Train Loss: 60.08409506082535, Val Loss: 9.276419878005981\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 117, Train Loss: 60.07888478040695, Val Loss: 9.279885292053223\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 118, Train Loss: 60.06901177763939, Val Loss: 9.280118644237518\n",
      "Time: 0.04s, CPU: 12.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 119, Train Loss: 60.062253415584564, Val Loss: 9.28158849477768\n",
      "Time: 0.03s, CPU: 10.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 120, Train Loss: 60.0576257109642, Val Loss: 9.28486704826355\n",
      "Time: 0.04s, CPU: 6.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 121, Train Loss: 60.049270421266556, Val Loss: 9.285792410373688\n",
      "Time: 0.03s, CPU: 14.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 122, Train Loss: 60.04304710030556, Val Loss: 9.288236796855927\n",
      "Time: 0.03s, CPU: 11.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 123, Train Loss: 60.03788188099861, Val Loss: 9.290780425071716\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 124, Train Loss: 60.02976205945015, Val Loss: 9.293014705181122\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 125, Train Loss: 60.02185755968094, Val Loss: 9.295015633106232\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 126, Train Loss: 60.01481342315674, Val Loss: 9.297646880149841\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 127, Train Loss: 60.00542455911636, Val Loss: 9.29962545633316\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 128, Train Loss: 59.99590650200844, Val Loss: 9.301759600639343\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 129, Train Loss: 59.98803648352623, Val Loss: 9.304158389568329\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 130, Train Loss: 59.97942993044853, Val Loss: 9.30380791425705\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 131, Train Loss: 59.97428876161575, Val Loss: 9.306453466415405\n",
      "Time: 0.03s, CPU: 11.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 132, Train Loss: 59.965599954128265, Val Loss: 9.306119978427887\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 133, Train Loss: 59.961019933223724, Val Loss: 9.305537939071655\n",
      "Time: 0.04s, CPU: 16.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 134, Train Loss: 59.95791104435921, Val Loss: 9.307138323783875\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 135, Train Loss: 59.9498430788517, Val Loss: 9.307824075222015\n",
      "Time: 0.03s, CPU: 22.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 136, Train Loss: 59.94273942708969, Val Loss: 9.307110607624054\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 137, Train Loss: 59.93589785695076, Val Loss: 9.308278262615204\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 138, Train Loss: 59.93032842874527, Val Loss: 9.310637712478638\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 139, Train Loss: 59.92315426468849, Val Loss: 9.312158524990082\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 140, Train Loss: 59.91544923186302, Val Loss: 9.313640892505646\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 141, Train Loss: 59.91017982363701, Val Loss: 9.31485503911972\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 142, Train Loss: 59.904961347579956, Val Loss: 9.316309690475464\n",
      "Time: 0.03s, CPU: 17.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 143, Train Loss: 59.89938345551491, Val Loss: 9.316947162151337\n",
      "Time: 0.04s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 144, Train Loss: 59.89360809326172, Val Loss: 9.31882917881012\n",
      "Time: 0.03s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 145, Train Loss: 59.88705724477768, Val Loss: 9.31929498910904\n",
      "Time: 0.04s, CPU: 18.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 146, Train Loss: 59.88299176096916, Val Loss: 9.317311942577362\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 147, Train Loss: 59.87839886546135, Val Loss: 9.32094007730484\n",
      "Time: 0.04s, CPU: 21.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 148, Train Loss: 59.86963373422623, Val Loss: 9.321368336677551\n",
      "Time: 0.03s, CPU: 16.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 149, Train Loss: 59.86565691232681, Val Loss: 9.319676756858826\n",
      "Time: 0.04s, CPU: 23.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 150, Train Loss: 59.86365655064583, Val Loss: 9.321427345275879\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 151, Train Loss: 59.85778072476387, Val Loss: 9.323974549770355\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 152, Train Loss: 59.85173651576042, Val Loss: 9.325596392154694\n",
      "Time: 0.04s, CPU: 16.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 153, Train Loss: 59.84467989206314, Val Loss: 9.327009916305542\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 154, Train Loss: 59.83859986066818, Val Loss: 9.328257143497467\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 155, Train Loss: 59.83503943681717, Val Loss: 9.328871369361877\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 156, Train Loss: 59.83022007346153, Val Loss: 9.331128001213074\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 157, Train Loss: 59.823092848062515, Val Loss: 9.331787824630737\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 158, Train Loss: 59.816109865903854, Val Loss: 9.331765472888947\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 159, Train Loss: 59.80912923812866, Val Loss: 9.331152141094208\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 160, Train Loss: 59.80412515997887, Val Loss: 9.331480264663696\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 161, Train Loss: 59.798491805791855, Val Loss: 9.331653714179993\n",
      "Time: 0.03s, CPU: 25.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 162, Train Loss: 59.79257184267044, Val Loss: 9.332908987998962\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 163, Train Loss: 59.7852640748024, Val Loss: 9.33337390422821\n",
      "Time: 0.03s, CPU: 14.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 164, Train Loss: 59.78007510304451, Val Loss: 9.333171844482422\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 165, Train Loss: 59.77604427933693, Val Loss: 9.33439314365387\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 166, Train Loss: 59.77091035246849, Val Loss: 9.335973858833313\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 167, Train Loss: 59.76608684659004, Val Loss: 9.336424469947815\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 168, Train Loss: 59.76057979464531, Val Loss: 9.338502287864685\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 169, Train Loss: 59.754274398088455, Val Loss: 9.338755309581757\n",
      "Time: 0.04s, CPU: 27.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 170, Train Loss: 59.75000113248825, Val Loss: 9.33918982744217\n",
      "Time: 0.03s, CPU: 12.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 171, Train Loss: 59.74570471048355, Val Loss: 9.341055750846863\n",
      "Time: 0.03s, CPU: 10.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 172, Train Loss: 59.73932012915611, Val Loss: 9.341055750846863\n",
      "Time: 0.03s, CPU: 10.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 173, Train Loss: 59.734494626522064, Val Loss: 9.340400397777557\n",
      "Time: 0.03s, CPU: 9.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 174, Train Loss: 59.73194107413292, Val Loss: 9.341908693313599\n",
      "Time: 0.03s, CPU: 8.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 175, Train Loss: 59.724108546972275, Val Loss: 9.342705309391022\n",
      "Time: 0.03s, CPU: 11.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 176, Train Loss: 59.71912768483162, Val Loss: 9.342378079891205\n",
      "Time: 0.03s, CPU: 20.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 177, Train Loss: 59.715007334947586, Val Loss: 9.34377372264862\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 178, Train Loss: 59.70823812484741, Val Loss: 9.344932436943054\n",
      "Time: 0.03s, CPU: 14.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 179, Train Loss: 59.702578872442245, Val Loss: 9.345393776893616\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 180, Train Loss: 59.69711849093437, Val Loss: 9.346626698970795\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 181, Train Loss: 59.693985879421234, Val Loss: 9.34775859117508\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 182, Train Loss: 59.688847571611404, Val Loss: 9.349459111690521\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 183, Train Loss: 59.684794187545776, Val Loss: 9.350114464759827\n",
      "Time: 0.03s, CPU: 12.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 184, Train Loss: 59.68082031607628, Val Loss: 9.351016581058502\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 185, Train Loss: 59.676899433135986, Val Loss: 9.352997839450836\n",
      "Time: 0.03s, CPU: 18.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 186, Train Loss: 59.6715804040432, Val Loss: 9.353128373622894\n",
      "Time: 0.03s, CPU: 24.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 187, Train Loss: 59.66665601730347, Val Loss: 9.353707730770111\n",
      "Time: 0.04s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 188, Train Loss: 59.66278380155563, Val Loss: 9.356732368469238\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 189, Train Loss: 59.6577750146389, Val Loss: 9.357986748218536\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 190, Train Loss: 59.653265953063965, Val Loss: 9.356962144374847\n",
      "Time: 0.03s, CPU: 11.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 191, Train Loss: 59.65033459663391, Val Loss: 9.357887506484985\n",
      "Time: 0.03s, CPU: 8.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 192, Train Loss: 59.64495778083801, Val Loss: 9.359559416770935\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 193, Train Loss: 59.63850551843643, Val Loss: 9.360567927360535\n",
      "Time: 0.03s, CPU: 23.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 194, Train Loss: 59.633286237716675, Val Loss: 9.361183941364288\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 195, Train Loss: 59.63074257969856, Val Loss: 9.361065030097961\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 196, Train Loss: 59.62700578570366, Val Loss: 9.363167881965637\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 197, Train Loss: 59.619297474622726, Val Loss: 9.36335563659668\n",
      "Time: 0.03s, CPU: 16.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 198, Train Loss: 59.61469241976738, Val Loss: 9.363283216953278\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 199, Train Loss: 59.61266008019447, Val Loss: 9.364170134067535\n",
      "Time: 0.03s, CPU: 9.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 200, Train Loss: 59.607485204935074, Val Loss: 9.365327060222626\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 201, Train Loss: 59.59752815961838, Val Loss: 9.365442395210266\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 202, Train Loss: 59.597062438726425, Val Loss: 9.365467429161072\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 203, Train Loss: 59.5964729487896, Val Loss: 9.3655264377594\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 204, Train Loss: 59.595824629068375, Val Loss: 9.36561495065689\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 205, Train Loss: 59.595063865184784, Val Loss: 9.365709722042084\n",
      "Time: 0.03s, CPU: 16.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 206, Train Loss: 59.59428051114082, Val Loss: 9.365787506103516\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 207, Train Loss: 59.59351509809494, Val Loss: 9.365890324115753\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 208, Train Loss: 59.59280923008919, Val Loss: 9.366068243980408\n",
      "Time: 0.03s, CPU: 11.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 209, Train Loss: 59.592298060655594, Val Loss: 9.366205930709839\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 210, Train Loss: 59.59184458851814, Val Loss: 9.366330206394196\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 211, Train Loss: 59.591311395168304, Val Loss: 9.366440176963806\n",
      "Time: 0.03s, CPU: 11.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 212, Train Loss: 59.59066891670227, Val Loss: 9.36665564775467\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 213, Train Loss: 59.59024375677109, Val Loss: 9.366728067398071\n",
      "Time: 0.04s, CPU: 10.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 214, Train Loss: 59.58967366814613, Val Loss: 9.366792440414429\n",
      "Time: 0.03s, CPU: 17.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 215, Train Loss: 59.58911591768265, Val Loss: 9.366980195045471\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 216, Train Loss: 59.58872762322426, Val Loss: 9.367003440856934\n",
      "Time: 0.04s, CPU: 16.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 217, Train Loss: 59.5881364941597, Val Loss: 9.3670454621315\n",
      "Time: 0.03s, CPU: 50.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 218, Train Loss: 59.587650537490845, Val Loss: 9.36717689037323\n",
      "Time: 0.03s, CPU: 11.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 219, Train Loss: 59.58733752369881, Val Loss: 9.367251098155975\n",
      "Time: 0.03s, CPU: 16.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 220, Train Loss: 59.586808294057846, Val Loss: 9.367340505123138\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 221, Train Loss: 59.586165219545364, Val Loss: 9.367436170578003\n",
      "Time: 0.03s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 222, Train Loss: 59.58554142713547, Val Loss: 9.367607831954956\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 223, Train Loss: 59.585014164447784, Val Loss: 9.367676675319672\n",
      "Time: 0.03s, CPU: 35.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 224, Train Loss: 59.584291368722916, Val Loss: 9.367799162864685\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 225, Train Loss: 59.583776861429214, Val Loss: 9.367834031581879\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 226, Train Loss: 59.58338052034378, Val Loss: 9.367879629135132\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 227, Train Loss: 59.58288234472275, Val Loss: 9.367978870868683\n",
      "Time: 0.04s, CPU: 25.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 228, Train Loss: 59.58245399594307, Val Loss: 9.36802089214325\n",
      "Time: 0.04s, CPU: 22.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 229, Train Loss: 59.582044929265976, Val Loss: 9.368127286434174\n",
      "Time: 0.04s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 230, Train Loss: 59.58146661520004, Val Loss: 9.368193447589874\n",
      "Time: 0.04s, CPU: 35.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 231, Train Loss: 59.58091393113136, Val Loss: 9.36827927827835\n",
      "Time: 0.03s, CPU: 28.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 232, Train Loss: 59.580470353364944, Val Loss: 9.368351697921753\n",
      "Time: 0.04s, CPU: 23.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 233, Train Loss: 59.58001157641411, Val Loss: 9.368499219417572\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 234, Train Loss: 59.57953101396561, Val Loss: 9.368535876274109\n",
      "Time: 0.04s, CPU: 9.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.50%\n",
      "Epoch 235, Train Loss: 59.578870952129364, Val Loss: 9.36859130859375\n",
      "Time: 0.04s, CPU: 6.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 236, Train Loss: 59.57834219932556, Val Loss: 9.36869502067566\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 237, Train Loss: 59.57789948582649, Val Loss: 9.36876654624939\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 238, Train Loss: 59.577399253845215, Val Loss: 9.36884880065918\n",
      "Time: 0.04s, CPU: 11.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 239, Train Loss: 59.576891243457794, Val Loss: 9.368893504142761\n",
      "Time: 0.03s, CPU: 10.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 240, Train Loss: 59.57641598582268, Val Loss: 9.368994534015656\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 241, Train Loss: 59.576027631759644, Val Loss: 9.369022250175476\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 242, Train Loss: 59.57539874315262, Val Loss: 9.369072318077087\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 243, Train Loss: 59.57489022612572, Val Loss: 9.369164407253265\n",
      "Time: 0.03s, CPU: 15.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 244, Train Loss: 59.574503630399704, Val Loss: 9.36920017004013\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 245, Train Loss: 59.57407206296921, Val Loss: 9.369234144687653\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 246, Train Loss: 59.573554664850235, Val Loss: 9.369365572929382\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 247, Train Loss: 59.573174238204956, Val Loss: 9.369431734085083\n",
      "Time: 0.04s, CPU: 19.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 248, Train Loss: 59.57260710000992, Val Loss: 9.369444251060486\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 249, Train Loss: 59.572138875722885, Val Loss: 9.36952918767929\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 250, Train Loss: 59.57179546356201, Val Loss: 9.369552433490753\n",
      "Time: 0.03s, CPU: 8.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 251, Train Loss: 59.57126632332802, Val Loss: 9.369574785232544\n",
      "Time: 0.03s, CPU: 12.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 252, Train Loss: 59.57075089216232, Val Loss: 9.369654357433319\n",
      "Time: 0.03s, CPU: 13.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 253, Train Loss: 59.57031089067459, Val Loss: 9.369647204875946\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 254, Train Loss: 59.569703847169876, Val Loss: 9.369708895683289\n",
      "Time: 0.03s, CPU: 9.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 255, Train Loss: 59.56920075416565, Val Loss: 9.369881451129913\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 256, Train Loss: 59.56881436705589, Val Loss: 9.369882345199585\n",
      "Time: 0.03s, CPU: 5.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 257, Train Loss: 59.56842088699341, Val Loss: 9.369850158691406\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 258, Train Loss: 59.56788930296898, Val Loss: 9.369979798793793\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 259, Train Loss: 59.56743097305298, Val Loss: 9.370042383670807\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 260, Train Loss: 59.56693431735039, Val Loss: 9.370018243789673\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 261, Train Loss: 59.56643858551979, Val Loss: 9.370120167732239\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 262, Train Loss: 59.56596916913986, Val Loss: 9.370199739933014\n",
      "Time: 0.03s, CPU: 10.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 263, Train Loss: 59.56551194190979, Val Loss: 9.370231926441193\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 264, Train Loss: 59.56505271792412, Val Loss: 9.370276629924774\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 265, Train Loss: 59.56460663676262, Val Loss: 9.37031865119934\n",
      "Time: 0.03s, CPU: 15.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 266, Train Loss: 59.564072757959366, Val Loss: 9.370401799678802\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 267, Train Loss: 59.563689172267914, Val Loss: 9.370409846305847\n",
      "Time: 0.03s, CPU: 10.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 268, Train Loss: 59.56314614415169, Val Loss: 9.370528757572174\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 269, Train Loss: 59.56266286969185, Val Loss: 9.3706414103508\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 270, Train Loss: 59.5622541308403, Val Loss: 9.370654821395874\n",
      "Time: 0.03s, CPU: 16.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 271, Train Loss: 59.56172361969948, Val Loss: 9.370627999305725\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 272, Train Loss: 59.56129524111748, Val Loss: 9.370704889297485\n",
      "Time: 0.03s, CPU: 12.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 273, Train Loss: 59.56082281470299, Val Loss: 9.370818436145782\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 274, Train Loss: 59.56038120388985, Val Loss: 9.370848834514618\n",
      "Time: 0.03s, CPU: 10.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 275, Train Loss: 59.559803545475006, Val Loss: 9.370938241481781\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 276, Train Loss: 59.559262186288834, Val Loss: 9.371135830879211\n",
      "Time: 0.03s, CPU: 9.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 277, Train Loss: 59.55888041853905, Val Loss: 9.371083080768585\n",
      "Time: 0.03s, CPU: 10.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 278, Train Loss: 59.55843633413315, Val Loss: 9.371071457862854\n",
      "Time: 0.03s, CPU: 10.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 279, Train Loss: 59.557952880859375, Val Loss: 9.371200203895569\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 280, Train Loss: 59.55756902694702, Val Loss: 9.371391534805298\n",
      "Time: 0.04s, CPU: 7.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.50%\n",
      "Epoch 281, Train Loss: 59.557244032621384, Val Loss: 9.371422827243805\n",
      "Time: 0.04s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 282, Train Loss: 59.55663323402405, Val Loss: 9.371490776538849\n",
      "Time: 0.03s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 283, Train Loss: 59.55606487393379, Val Loss: 9.371638298034668\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 284, Train Loss: 59.55565771460533, Val Loss: 9.371683895587921\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 285, Train Loss: 59.55527925491333, Val Loss: 9.371727705001831\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 286, Train Loss: 59.554780185222626, Val Loss: 9.371876120567322\n",
      "Time: 0.03s, CPU: 26.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 287, Train Loss: 59.55442827939987, Val Loss: 9.371892213821411\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 288, Train Loss: 59.55397242307663, Val Loss: 9.372040629386902\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 289, Train Loss: 59.55353540182114, Val Loss: 9.372142553329468\n",
      "Time: 0.03s, CPU: 20.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 290, Train Loss: 59.55294418334961, Val Loss: 9.372158646583557\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 291, Train Loss: 59.55242258310318, Val Loss: 9.372362494468689\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 292, Train Loss: 59.55206444859505, Val Loss: 9.372441172599792\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 293, Train Loss: 59.55149483680725, Val Loss: 9.372600317001343\n",
      "Time: 0.03s, CPU: 12.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 294, Train Loss: 59.55114808678627, Val Loss: 9.372622668743134\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 295, Train Loss: 59.5506531894207, Val Loss: 9.372740685939789\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 296, Train Loss: 59.55019974708557, Val Loss: 9.372859597206116\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 297, Train Loss: 59.549832344055176, Val Loss: 9.372932016849518\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 298, Train Loss: 59.54924872517586, Val Loss: 9.373054504394531\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 299, Train Loss: 59.54881164431572, Val Loss: 9.37316358089447\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 300, Train Loss: 59.548486560583115, Val Loss: 9.373194873332977\n",
      "Time: 0.03s, CPU: 11.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 301, Train Loss: 59.547547340393066, Val Loss: 9.373222589492798\n",
      "Time: 0.03s, CPU: 11.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 302, Train Loss: 59.547475188970566, Val Loss: 9.373216331005096\n",
      "Time: 0.03s, CPU: 9.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 303, Train Loss: 59.54741469025612, Val Loss: 9.373222589492798\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 304, Train Loss: 59.54734769463539, Val Loss: 9.373225271701813\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 305, Train Loss: 59.54730024933815, Val Loss: 9.373250305652618\n",
      "Time: 0.03s, CPU: 10.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 306, Train Loss: 59.54722982645035, Val Loss: 9.373253881931305\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 307, Train Loss: 59.547192603349686, Val Loss: 9.373277127742767\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 308, Train Loss: 59.54713898897171, Val Loss: 9.373269081115723\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 309, Train Loss: 59.54711151123047, Val Loss: 9.3732950091362\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 310, Train Loss: 59.547055929899216, Val Loss: 9.3732950091362\n",
      "Time: 0.03s, CPU: 9.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 311, Train Loss: 59.547002017498016, Val Loss: 9.373298585414886\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 312, Train Loss: 59.5469451546669, Val Loss: 9.373309314250946\n",
      "Time: 0.03s, CPU: 13.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 313, Train Loss: 59.54690578579903, Val Loss: 9.373321831226349\n",
      "Time: 0.03s, CPU: 12.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 314, Train Loss: 59.5468587577343, Val Loss: 9.37333881855011\n",
      "Time: 0.03s, CPU: 18.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 315, Train Loss: 59.54681649804115, Val Loss: 9.373340606689453\n",
      "Time: 0.03s, CPU: 15.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 316, Train Loss: 59.54677173495293, Val Loss: 9.37332808971405\n",
      "Time: 0.04s, CPU: 16.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 317, Train Loss: 59.54673698544502, Val Loss: 9.373351335525513\n",
      "Time: 0.03s, CPU: 12.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 318, Train Loss: 59.54668569564819, Val Loss: 9.373379945755005\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 319, Train Loss: 59.54664358496666, Val Loss: 9.373384416103363\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 320, Train Loss: 59.54658940434456, Val Loss: 9.373380839824677\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 321, Train Loss: 59.54654532670975, Val Loss: 9.373385310173035\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 322, Train Loss: 59.546503603458405, Val Loss: 9.373411238193512\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 323, Train Loss: 59.54645675420761, Val Loss: 9.373442530632019\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 324, Train Loss: 59.54640793800354, Val Loss: 9.373427331447601\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 325, Train Loss: 59.54634138941765, Val Loss: 9.373444318771362\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 326, Train Loss: 59.54628726840019, Val Loss: 9.373449683189392\n",
      "Time: 0.03s, CPU: 21.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 327, Train Loss: 59.54624533653259, Val Loss: 9.373472034931183\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 328, Train Loss: 59.54620045423508, Val Loss: 9.373473823070526\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 329, Train Loss: 59.54615768790245, Val Loss: 9.373480081558228\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 330, Train Loss: 59.546109944581985, Val Loss: 9.373494386672974\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 331, Train Loss: 59.546066612005234, Val Loss: 9.373500645160675\n",
      "Time: 0.04s, CPU: 24.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 332, Train Loss: 59.54602241516113, Val Loss: 9.373526573181152\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 333, Train Loss: 59.54597917199135, Val Loss: 9.373532831668854\n",
      "Time: 0.03s, CPU: 8.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 334, Train Loss: 59.54593950510025, Val Loss: 9.373539984226227\n",
      "Time: 0.03s, CPU: 21.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 335, Train Loss: 59.54587608575821, Val Loss: 9.373546242713928\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 336, Train Loss: 59.54583552479744, Val Loss: 9.3735471367836\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 337, Train Loss: 59.545789152383804, Val Loss: 9.373565018177032\n",
      "Time: 0.04s, CPU: 18.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 338, Train Loss: 59.54573628306389, Val Loss: 9.373575747013092\n",
      "Time: 0.04s, CPU: 10.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 339, Train Loss: 59.54569283127785, Val Loss: 9.373572170734406\n",
      "Time: 0.04s, CPU: 15.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 340, Train Loss: 59.545650631189346, Val Loss: 9.373575747013092\n",
      "Time: 0.04s, CPU: 15.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 341, Train Loss: 59.54561021924019, Val Loss: 9.373611509799957\n",
      "Time: 0.03s, CPU: 19.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 342, Train Loss: 59.54555568099022, Val Loss: 9.373603463172913\n",
      "Time: 0.03s, CPU: 15.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 343, Train Loss: 59.54551914334297, Val Loss: 9.37362402677536\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 344, Train Loss: 59.545462280511856, Val Loss: 9.373641014099121\n",
      "Time: 0.03s, CPU: 31.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 345, Train Loss: 59.54540964961052, Val Loss: 9.37363475561142\n",
      "Time: 0.03s, CPU: 16.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 346, Train Loss: 59.545371145009995, Val Loss: 9.373652637004852\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 347, Train Loss: 59.5453380048275, Val Loss: 9.37366783618927\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 348, Train Loss: 59.545269161462784, Val Loss: 9.373659789562225\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 349, Train Loss: 59.54522642493248, Val Loss: 9.373683035373688\n",
      "Time: 0.03s, CPU: 13.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 350, Train Loss: 59.545178920030594, Val Loss: 9.37370628118515\n",
      "Time: 0.04s, CPU: 30.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 351, Train Loss: 59.54512384533882, Val Loss: 9.373708069324493\n",
      "Time: 0.03s, CPU: 13.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 352, Train Loss: 59.545073717832565, Val Loss: 9.37371164560318\n",
      "Time: 0.03s, CPU: 25.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 353, Train Loss: 59.54502406716347, Val Loss: 9.373736679553986\n",
      "Time: 0.04s, CPU: 44.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 354, Train Loss: 59.54497814178467, Val Loss: 9.37376081943512\n",
      "Time: 0.04s, CPU: 25.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 355, Train Loss: 59.544929534196854, Val Loss: 9.37375009059906\n",
      "Time: 0.04s, CPU: 20.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 356, Train Loss: 59.54489278793335, Val Loss: 9.37375009059906\n",
      "Time: 0.04s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 357, Train Loss: 59.544845044612885, Val Loss: 9.373775124549866\n",
      "Time: 0.03s, CPU: 25.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 358, Train Loss: 59.544794738292694, Val Loss: 9.373786747455597\n",
      "Time: 0.04s, CPU: 28.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 359, Train Loss: 59.5447493493557, Val Loss: 9.37380999326706\n",
      "Time: 0.03s, CPU: 19.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 360, Train Loss: 59.54468768835068, Val Loss: 9.373807311058044\n",
      "Time: 0.04s, CPU: 24.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 361, Train Loss: 59.54465064406395, Val Loss: 9.373813569545746\n",
      "Time: 0.03s, CPU: 13.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 362, Train Loss: 59.54460549354553, Val Loss: 9.373828768730164\n",
      "Time: 0.04s, CPU: 6.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 363, Train Loss: 59.544551104307175, Val Loss: 9.373843967914581\n",
      "Time: 0.03s, CPU: 11.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 364, Train Loss: 59.544514030218124, Val Loss: 9.373845756053925\n",
      "Time: 0.03s, CPU: 12.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 365, Train Loss: 59.544459760189056, Val Loss: 9.373845756053925\n",
      "Time: 0.03s, CPU: 16.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 366, Train Loss: 59.544430673122406, Val Loss: 9.373860955238342\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 367, Train Loss: 59.54438200592995, Val Loss: 9.373872578144073\n",
      "Time: 0.04s, CPU: 6.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 368, Train Loss: 59.54432511329651, Val Loss: 9.373879730701447\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 369, Train Loss: 59.54428222775459, Val Loss: 9.373894929885864\n",
      "Time: 0.04s, CPU: 16.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 370, Train Loss: 59.54422378540039, Val Loss: 9.373904764652252\n",
      "Time: 0.03s, CPU: 11.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 371, Train Loss: 59.54419180750847, Val Loss: 9.373923540115356\n",
      "Time: 0.03s, CPU: 11.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 372, Train Loss: 59.54414984583855, Val Loss: 9.373931586742401\n",
      "Time: 0.04s, CPU: 8.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 373, Train Loss: 59.544092416763306, Val Loss: 9.373938739299774\n",
      "Time: 0.03s, CPU: 24.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 374, Train Loss: 59.54403758049011, Val Loss: 9.373944103717804\n",
      "Time: 0.03s, CPU: 9.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.50%\n",
      "Epoch 375, Train Loss: 59.54398888349533, Val Loss: 9.37395840883255\n",
      "Time: 0.03s, CPU: 8.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 376, Train Loss: 59.543941885232925, Val Loss: 9.373971819877625\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 377, Train Loss: 59.54389053583145, Val Loss: 9.373976290225983\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 378, Train Loss: 59.54386228322983, Val Loss: 9.373998641967773\n",
      "Time: 0.03s, CPU: 11.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 379, Train Loss: 59.54381704330444, Val Loss: 9.37401831150055\n",
      "Time: 0.04s, CPU: 24.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 380, Train Loss: 59.54375320672989, Val Loss: 9.374030828475952\n",
      "Time: 0.04s, CPU: 15.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 381, Train Loss: 59.54370737075806, Val Loss: 9.37400758266449\n",
      "Time: 0.04s, CPU: 23.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 382, Train Loss: 59.54366374015808, Val Loss: 9.374015629291534\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 383, Train Loss: 59.543641269207, Val Loss: 9.374033510684967\n",
      "Time: 0.03s, CPU: 9.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 384, Train Loss: 59.543575555086136, Val Loss: 9.374043345451355\n",
      "Time: 0.03s, CPU: 10.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 385, Train Loss: 59.543537586927414, Val Loss: 9.374038875102997\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 386, Train Loss: 59.54348799586296, Val Loss: 9.374049603939056\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 387, Train Loss: 59.54344826936722, Val Loss: 9.374078214168549\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 388, Train Loss: 59.54338297247887, Val Loss: 9.374088048934937\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 389, Train Loss: 59.543328791856766, Val Loss: 9.374076426029205\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 390, Train Loss: 59.543287098407745, Val Loss: 9.374096989631653\n",
      "Time: 0.03s, CPU: 12.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 391, Train Loss: 59.5432505607605, Val Loss: 9.374127388000488\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 392, Train Loss: 59.543188393116, Val Loss: 9.374136328697205\n",
      "Time: 0.03s, CPU: 14.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 393, Train Loss: 59.543158769607544, Val Loss: 9.374145269393921\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 394, Train Loss: 59.543120354413986, Val Loss: 9.37413901090622\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 395, Train Loss: 59.543083280324936, Val Loss: 9.374147057533264\n",
      "Time: 0.03s, CPU: 18.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 396, Train Loss: 59.5430203974247, Val Loss: 9.374149739742279\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 397, Train Loss: 59.54297450184822, Val Loss: 9.374172985553741\n",
      "Time: 0.03s, CPU: 11.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 398, Train Loss: 59.54292565584183, Val Loss: 9.374167621135712\n",
      "Time: 0.03s, CPU: 10.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 399, Train Loss: 59.54289412498474, Val Loss: 9.374187290668488\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 400, Train Loss: 59.542838633060455, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 401, Train Loss: 59.54274320602417, Val Loss: 9.374206960201263\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 402, Train Loss: 59.54273575544357, Val Loss: 9.374201595783234\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 403, Train Loss: 59.54272285103798, Val Loss: 9.374214112758636\n",
      "Time: 0.03s, CPU: 11.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 404, Train Loss: 59.5427288711071, Val Loss: 9.374198913574219\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 405, Train Loss: 59.54272964596748, Val Loss: 9.374181926250458\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 406, Train Loss: 59.542724609375, Val Loss: 9.374201595783234\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 407, Train Loss: 59.54272270202637, Val Loss: 9.374185502529144\n",
      "Time: 0.03s, CPU: 18.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 408, Train Loss: 59.54272684454918, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 409, Train Loss: 59.54271849989891, Val Loss: 9.374169409275055\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 410, Train Loss: 59.54271271824837, Val Loss: 9.374179244041443\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 411, Train Loss: 59.542712450027466, Val Loss: 9.374179244041443\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 412, Train Loss: 59.54271379113197, Val Loss: 9.374175667762756\n",
      "Time: 0.03s, CPU: 16.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 413, Train Loss: 59.542695730924606, Val Loss: 9.374193549156189\n",
      "Time: 0.03s, CPU: 8.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 414, Train Loss: 59.542702466249466, Val Loss: 9.374196231365204\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 415, Train Loss: 59.54270023107529, Val Loss: 9.374185502529144\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 416, Train Loss: 59.54269817471504, Val Loss: 9.374181032180786\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 417, Train Loss: 59.54269367456436, Val Loss: 9.374180138111115\n",
      "Time: 0.03s, CPU: 19.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 418, Train Loss: 59.54269555211067, Val Loss: 9.374179244041443\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 419, Train Loss: 59.54268738627434, Val Loss: 9.374187290668488\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 420, Train Loss: 59.542682111263275, Val Loss: 9.374189972877502\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 421, Train Loss: 59.54268476366997, Val Loss: 9.37418907880783\n",
      "Time: 0.03s, CPU: 16.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 422, Train Loss: 59.54267904162407, Val Loss: 9.37418907880783\n",
      "Time: 0.03s, CPU: 12.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 423, Train Loss: 59.542673885822296, Val Loss: 9.374169409275055\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 424, Train Loss: 59.54267677664757, Val Loss: 9.374183714389801\n",
      "Time: 0.03s, CPU: 12.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 425, Train Loss: 59.54266685247421, Val Loss: 9.374180138111115\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 426, Train Loss: 59.54265934228897, Val Loss: 9.37418818473816\n",
      "Time: 0.03s, CPU: 21.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 427, Train Loss: 59.54265159368515, Val Loss: 9.374186396598816\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 428, Train Loss: 59.54265213012695, Val Loss: 9.374190866947174\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 429, Train Loss: 59.54265138506889, Val Loss: 9.374198019504547\n",
      "Time: 0.04s, CPU: 35.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 430, Train Loss: 59.5426464676857, Val Loss: 9.374184608459473\n",
      "Time: 0.03s, CPU: 11.30%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 431, Train Loss: 59.54264298081398, Val Loss: 9.374204277992249\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 432, Train Loss: 59.54264011979103, Val Loss: 9.374186396598816\n",
      "Time: 0.03s, CPU: 14.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 433, Train Loss: 59.54264688491821, Val Loss: 9.37419980764389\n",
      "Time: 0.04s, CPU: 9.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 434, Train Loss: 59.54262736439705, Val Loss: 9.374173879623413\n",
      "Time: 0.03s, CPU: 18.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 435, Train Loss: 59.54263296723366, Val Loss: 9.37418907880783\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 436, Train Loss: 59.54262122511864, Val Loss: 9.374173879623413\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 437, Train Loss: 59.54261967539787, Val Loss: 9.37418282032013\n",
      "Time: 0.03s, CPU: 15.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 438, Train Loss: 59.542615830898285, Val Loss: 9.37419444322586\n",
      "Time: 0.03s, CPU: 12.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 439, Train Loss: 59.542614072561264, Val Loss: 9.374192655086517\n",
      "Time: 0.03s, CPU: 13.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 440, Train Loss: 59.542604863643646, Val Loss: 9.374186396598816\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 441, Train Loss: 59.5425980091095, Val Loss: 9.37418282032013\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 442, Train Loss: 59.542605340480804, Val Loss: 9.37418818473816\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 443, Train Loss: 59.54260587692261, Val Loss: 9.374193549156189\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 444, Train Loss: 59.54259151220322, Val Loss: 9.374175667762756\n",
      "Time: 0.03s, CPU: 32.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 445, Train Loss: 59.54259571433067, Val Loss: 9.374200701713562\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 446, Train Loss: 59.54258653521538, Val Loss: 9.374192655086517\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 447, Train Loss: 59.54258355498314, Val Loss: 9.374185502529144\n",
      "Time: 0.03s, CPU: 16.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 448, Train Loss: 59.54258370399475, Val Loss: 9.374211430549622\n",
      "Time: 0.04s, CPU: 15.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 449, Train Loss: 59.5425751209259, Val Loss: 9.374201595783234\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 450, Train Loss: 59.54257267713547, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 14.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 451, Train Loss: 59.5425583422184, Val Loss: 9.374193549156189\n",
      "Time: 0.04s, CPU: 19.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 452, Train Loss: 59.54256910085678, Val Loss: 9.374200701713562\n",
      "Time: 0.03s, CPU: 16.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 453, Train Loss: 59.54256200790405, Val Loss: 9.374193549156189\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 454, Train Loss: 59.54255414009094, Val Loss: 9.374208748340607\n",
      "Time: 0.03s, CPU: 17.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 455, Train Loss: 59.542553186416626, Val Loss: 9.37419444322586\n",
      "Time: 0.03s, CPU: 13.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 456, Train Loss: 59.542543679475784, Val Loss: 9.374187290668488\n",
      "Time: 0.03s, CPU: 7.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 457, Train Loss: 59.5425446331501, Val Loss: 9.374184608459473\n",
      "Time: 0.03s, CPU: 14.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 458, Train Loss: 59.542553037405014, Val Loss: 9.374196231365204\n",
      "Time: 0.03s, CPU: 22.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 459, Train Loss: 59.54253587126732, Val Loss: 9.374187290668488\n",
      "Time: 0.03s, CPU: 18.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 460, Train Loss: 59.54254648089409, Val Loss: 9.37418818473816\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 461, Train Loss: 59.54254561662674, Val Loss: 9.374193549156189\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 462, Train Loss: 59.54254350066185, Val Loss: 9.374200701713562\n",
      "Time: 0.03s, CPU: 6.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 463, Train Loss: 59.5425218641758, Val Loss: 9.374202489852905\n",
      "Time: 0.03s, CPU: 14.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 464, Train Loss: 59.54252567887306, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 12.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 465, Train Loss: 59.54251402616501, Val Loss: 9.374168515205383\n",
      "Time: 0.03s, CPU: 18.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 466, Train Loss: 59.54253268241882, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 467, Train Loss: 59.54251056909561, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 468, Train Loss: 59.54251131415367, Val Loss: 9.374198913574219\n",
      "Time: 0.03s, CPU: 5.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 469, Train Loss: 59.54251155257225, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 24.35%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 470, Train Loss: 59.54248946905136, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 9.60%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 471, Train Loss: 59.54249835014343, Val Loss: 9.374183714389801\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 472, Train Loss: 59.54249230027199, Val Loss: 9.374208748340607\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 473, Train Loss: 59.54248768091202, Val Loss: 9.374187290668488\n",
      "Time: 0.03s, CPU: 21.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 474, Train Loss: 59.54249086976051, Val Loss: 9.374196231365204\n",
      "Time: 0.03s, CPU: 17.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 475, Train Loss: 59.54248094558716, Val Loss: 9.374176561832428\n",
      "Time: 0.03s, CPU: 20.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 476, Train Loss: 59.54248958826065, Val Loss: 9.374172985553741\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 477, Train Loss: 59.54249006509781, Val Loss: 9.374185502529144\n",
      "Time: 0.03s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 478, Train Loss: 59.54247719049454, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 479, Train Loss: 59.54247909784317, Val Loss: 9.374200701713562\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 480, Train Loss: 59.5424670279026, Val Loss: 9.374197125434875\n",
      "Time: 0.03s, CPU: 22.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 481, Train Loss: 59.542474031448364, Val Loss: 9.374192655086517\n",
      "Time: 0.04s, CPU: 25.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 482, Train Loss: 59.542455822229385, Val Loss: 9.37418282032013\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 483, Train Loss: 59.5424627661705, Val Loss: 9.37418818473816\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 484, Train Loss: 59.54245528578758, Val Loss: 9.37419444322586\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 485, Train Loss: 59.54244902729988, Val Loss: 9.374203383922577\n",
      "Time: 0.03s, CPU: 22.90%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 486, Train Loss: 59.54244849085808, Val Loss: 9.374197125434875\n",
      "Time: 0.04s, CPU: 25.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 487, Train Loss: 59.5424447953701, Val Loss: 9.374183714389801\n",
      "Time: 0.03s, CPU: 25.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 488, Train Loss: 59.54244723916054, Val Loss: 9.374180138111115\n",
      "Time: 0.04s, CPU: 28.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 489, Train Loss: 59.54244440793991, Val Loss: 9.374186396598816\n",
      "Time: 0.03s, CPU: 24.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 490, Train Loss: 59.542437732219696, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 22.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 491, Train Loss: 59.542429596185684, Val Loss: 9.37419444322586\n",
      "Time: 0.03s, CPU: 7.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 492, Train Loss: 59.54242938756943, Val Loss: 9.374198019504547\n",
      "Time: 0.03s, CPU: 13.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 493, Train Loss: 59.54242569208145, Val Loss: 9.374195337295532\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 494, Train Loss: 59.54242444038391, Val Loss: 9.374186396598816\n",
      "Time: 0.03s, CPU: 6.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 495, Train Loss: 59.54242783784866, Val Loss: 9.374198913574219\n",
      "Time: 0.03s, CPU: 12.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 496, Train Loss: 59.54242521524429, Val Loss: 9.374196231365204\n",
      "Time: 0.03s, CPU: 8.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 497, Train Loss: 59.54241409897804, Val Loss: 9.374193549156189\n",
      "Time: 0.03s, CPU: 6.00%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.50%\n",
      "Epoch 498, Train Loss: 59.54241594672203, Val Loss: 9.374202489852905\n",
      "Time: 0.03s, CPU: 13.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 499, Train Loss: 59.54239907860756, Val Loss: 9.374201595783234\n",
      "Time: 0.04s, CPU: 32.85%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 500, Train Loss: 59.542405903339386, Val Loss: 9.37418907880783\n",
      "Time: 0.03s, CPU: 13.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/trainable_gcesn_1l_mutag.pth\n",
      "Average Time per Epoch: 0.03s\n",
      "Average CPU Usage: 14.39%\n",
      "Average Memory Usage: 3.32GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 7.52%\n",
      "\n",
      "Total Training Time: 17.11s\n",
      "Max CPU Usage: 50.90%\n",
      "Max Memory Usage: 3.32GB\n",
      "Max GPU Usage: 0.02GB\n",
      "Max GPU Utilization: 9.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "trainable_gcesn_mutag = TrainableGCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=6)\n",
    "print(f\"Total number of trainable parameters: {trainable_gcesn_mutag.count_parameters()}\\n\")\n",
    "                \n",
    "single_train(trainable_gcesn_mutag, mutag_train_loader, mutag_val_loader,\n",
    "                lr=0.001, num_epochs=500, patience=5, step_size=100, gamma=0.1, \n",
    "                save_path='models/trainable_gcesn_1l_mutag.pth',\n",
    "                binary_classification=True, is_esn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8421\n",
      "Average Sensitivity (Recall): 0.8846\n",
      "Average Specificity: 0.7500\n",
      "\n",
      "Average Inference Time per Batch: 0.0012s\n",
      "Average CPU Usage: 14.00%\n",
      "Average Memory Usage: 3.32GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "trainable_gcesn_mutag = TrainableGCESN(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=6)\n",
    "trainable_gcesn_mutag.load_state_dict(torch.load('models/trainable_gcesn_1l_mutag.pth'))\n",
    "single_test(trainable_gcesn_mutag.to(device), mutag_test_loader)\n",
    "inference_performance(trainable_gcesn_mutag.to(device), mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMCI-AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 690\n",
      "\n",
      "Epoch 1, Train Loss: 848060.4930419922, Val Loss: 30282.50244140625\n",
      "Time: 0.06s, CPU: 17.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 292123.59375, Val Loss: 5871.326904296875\n",
      "Time: 0.06s, CPU: 26.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 3.00%\n",
      "Epoch 3, Train Loss: 100376.08856201172, Val Loss: 6966.4278564453125\n",
      "Time: 0.06s, CPU: 19.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 4, Train Loss: 47102.153579711914, Val Loss: 3504.4424438476562\n",
      "Time: 0.06s, CPU: 26.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 5, Train Loss: 19642.681915283203, Val Loss: 1901.0745239257812\n",
      "Time: 0.06s, CPU: 26.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 7.00%\n",
      "Epoch 6, Train Loss: 13036.037582397461, Val Loss: 1472.3210754394531\n",
      "Time: 0.06s, CPU: 30.95%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 7, Train Loss: 12837.297637939453, Val Loss: 2024.5742797851562\n",
      "Time: 0.06s, CPU: 21.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 8, Train Loss: 16230.435348510742, Val Loss: 1657.61181640625\n",
      "Time: 0.06s, CPU: 24.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 9, Train Loss: 10352.709510803223, Val Loss: 1475.95458984375\n",
      "Time: 0.06s, CPU: 29.15%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 10, Train Loss: 11975.475242614746, Val Loss: 2083.916778564453\n",
      "Time: 0.07s, CPU: 30.20%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 11, Train Loss: 13600.531845092773, Val Loss: 2017.2677612304688\n",
      "Time: 0.07s, CPU: 38.50%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.50%\n",
      "Epoch 12, Train Loss: 15106.22484588623, Val Loss: 1182.257797241211\n",
      "Time: 0.07s, CPU: 45.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 13, Train Loss: 9397.132484436035, Val Loss: 1228.8981628417969\n",
      "Time: 0.06s, CPU: 32.65%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 14, Train Loss: 10096.42741394043, Val Loss: 1071.253433227539\n",
      "Time: 0.07s, CPU: 20.70%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 15, Train Loss: 16051.775756835938, Val Loss: 1272.5053405761719\n",
      "Time: 0.06s, CPU: 22.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 16, Train Loss: 12386.60065460205, Val Loss: 977.976318359375\n",
      "Time: 0.06s, CPU: 37.75%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 17, Train Loss: 8187.908432006836, Val Loss: 1319.2520141601562\n",
      "Time: 0.06s, CPU: 26.80%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 18, Train Loss: 10841.279012151179, Val Loss: 895.0649261474609\n",
      "Time: 0.06s, CPU: 34.45%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 19, Train Loss: 17120.564514160156, Val Loss: 1055.9358978271484\n",
      "Time: 0.06s, CPU: 30.25%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 20, Train Loss: 9875.048713684082, Val Loss: 961.5510559082031\n",
      "Time: 0.06s, CPU: 33.55%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 8.00%\n",
      "Epoch 21, Train Loss: 10767.465378761292, Val Loss: 1332.3356628417969\n",
      "Time: 0.06s, CPU: 26.05%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Epoch 22, Train Loss: 10525.854314804077, Val Loss: 1862.6362609863281\n",
      "Time: 0.06s, CPU: 29.40%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 9.00%\n",
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/trainable_gcesn_1l_emci.pth\n",
      "Average Time per Epoch: 0.06s\n",
      "Average CPU Usage: 28.42%\n",
      "Average Memory Usage: 3.32GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 7.43%\n",
      "\n",
      "Total Training Time: 1.45s\n",
      "Max CPU Usage: 45.10%\n",
      "Max Memory Usage: 3.32GB\n",
      "Max GPU Usage: 0.02GB\n",
      "Max GPU Utilization: 9.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "trainable_gcesn_emci = TrainableGCESN_1layer(emci_num_features, 2*emci_num_features, emci_num_classes, num_iterations=6)\n",
    "print(f\"Total number of trainable parameters: {trainable_gcesn_emci.count_parameters()}\\n\")\n",
    "                \n",
    "single_train(trainable_gcesn_emci, emci_train_loader, emci_val_loader,\n",
    "                lr=0.001, num_epochs=500, patience=5, step_size=100, gamma=0.1, \n",
    "                save_path='models/trainable_gcesn_1l_emci.pth',\n",
    "                binary_classification=True, is_esn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5556\n",
      "Average Sensitivity (Recall): 0.3636\n",
      "Average Specificity: 0.6875\n",
      "\n",
      "Average Inference Time per Batch: 0.0010s\n",
      "Average CPU Usage: 10.02%\n",
      "Average Memory Usage: 3.32GB\n",
      "Average GPU Usage: 0.02GB\n",
      "Average GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "trainable_gcesn_emci = TrainableGCESN(emci_num_features, 2*emci_num_features, emci_num_classes, num_iterations=6)\n",
    "trainable_gcesn_emci.load_state_dict(torch.load('models/trainable_gcesn_1l_emci.pth'))\n",
    "single_test(trainable_gcesn_emci.to(device), emci_test_loader)\n",
    "inference_performance(trainable_gcesn_emci.to(device), emci_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLIM160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 707\n",
      "\n",
      "Epoch 1, Train Loss: 30175033040.0, Val Loss: 1848747456.0\n",
      "Time: 0.18s, CPU: 25.10%, Memory: 3.32GB, GPU: 0.02GB, GPU Util: 6.00%\n",
      "Epoch 2, Train Loss: 10975893320.0, Val Loss: 773108118.0\n",
      "Time: 0.94s, CPU: 66.55%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 8.50%\n",
      "Epoch 3, Train Loss: 3757652516.0, Val Loss: 471202380.0\n",
      "Time: 1.24s, CPU: 82.20%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 6.50%\n",
      "Epoch 4, Train Loss: 4390581598.0, Val Loss: 356241753.0\n",
      "Time: 0.96s, CPU: 83.95%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 8.50%\n",
      "Epoch 5, Train Loss: 2991954338.0, Val Loss: 215771458.5\n",
      "Time: 0.98s, CPU: 65.10%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 8.00%\n",
      "Epoch 6, Train Loss: 2142898174.0, Val Loss: 172875087.0\n",
      "Time: 0.23s, CPU: 61.60%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 22.50%\n",
      "Epoch 7, Train Loss: 1995084944.0, Val Loss: 197384715.0\n",
      "Time: 1.28s, CPU: 88.95%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 3.50%\n",
      "Epoch 8, Train Loss: 1610074958.0, Val Loss: 158783773.5\n",
      "Time: 0.68s, CPU: 85.30%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 9.50%\n",
      "Epoch 9, Train Loss: 1564336448.0, Val Loss: 167875537.5\n",
      "Time: 0.34s, CPU: 55.65%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 16.50%\n",
      "Epoch 10, Train Loss: 1319804274.0, Val Loss: 157517217.0\n",
      "Time: 0.21s, CPU: 49.05%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 28.00%\n",
      "Epoch 11, Train Loss: 1326334358.0, Val Loss: 144918909.0\n",
      "Time: 0.17s, CPU: 39.60%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 36.50%\n",
      "Epoch 12, Train Loss: 1175305753.0, Val Loss: 150876931.5\n",
      "Time: 0.16s, CPU: 50.70%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 13, Train Loss: 1074317256.5, Val Loss: 147110877.0\n",
      "Time: 0.16s, CPU: 40.50%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Epoch 14, Train Loss: 1062344466.0, Val Loss: 139827357.0\n",
      "Time: 0.17s, CPU: 35.70%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 38.50%\n",
      "Epoch 15, Train Loss: 1006799795.0, Val Loss: 148609188.0\n",
      "Time: 0.16s, CPU: 43.10%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 38.50%\n",
      "Epoch 16, Train Loss: 917519016.0, Val Loss: 134418906.0\n",
      "Time: 0.17s, CPU: 38.00%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 17, Train Loss: 900896873.0, Val Loss: 132625849.5\n",
      "Time: 0.24s, CPU: 50.50%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 30.50%\n",
      "Epoch 18, Train Loss: 849165823.0, Val Loss: 133226734.5\n",
      "Time: 0.16s, CPU: 33.85%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 31.50%\n",
      "Epoch 19, Train Loss: 827662008.0, Val Loss: 129052453.5\n",
      "Time: 0.17s, CPU: 43.10%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 39.00%\n",
      "Epoch 20, Train Loss: 797253332.25, Val Loss: 130208080.5\n",
      "Time: 0.17s, CPU: 30.65%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 38.00%\n",
      "Epoch 21, Train Loss: 782516036.0, Val Loss: 129047620.5\n",
      "Time: 0.17s, CPU: 39.55%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 38.50%\n",
      "Epoch 22, Train Loss: 765147484.5, Val Loss: 126714537.0\n",
      "Time: 0.17s, CPU: 37.10%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 38.00%\n",
      "Epoch 23, Train Loss: 688613871.0, Val Loss: 122452060.5\n",
      "Time: 0.23s, CPU: 45.00%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 33.50%\n",
      "Epoch 24, Train Loss: 645345852.25, Val Loss: 109144840.5\n",
      "Time: 0.17s, CPU: 34.70%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 32.00%\n",
      "Epoch 25, Train Loss: 622029504.0, Val Loss: 105993279.0\n",
      "Time: 0.16s, CPU: 36.10%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 38.00%\n",
      "Epoch 26, Train Loss: 600527795.5, Val Loss: 106043688.0\n",
      "Time: 0.17s, CPU: 34.80%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 38.00%\n",
      "Epoch 27, Train Loss: 584392531.5, Val Loss: 102864012.75\n",
      "Time: 0.21s, CPU: 45.90%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 36.00%\n",
      "Epoch 28, Train Loss: 587309979.5, Val Loss: 100929550.5\n",
      "Time: 0.16s, CPU: 41.55%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 36.00%\n",
      "Epoch 29, Train Loss: 571649687.75, Val Loss: 102452809.5\n",
      "Time: 0.16s, CPU: 42.30%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 40.00%\n",
      "Epoch 30, Train Loss: 530237880.25, Val Loss: 97183678.5\n",
      "Time: 0.17s, CPU: 44.95%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Epoch 31, Train Loss: 498866212.0, Val Loss: 93680840.25\n",
      "Time: 0.16s, CPU: 42.60%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 32, Train Loss: 498051876.25, Val Loss: 93946014.0\n",
      "Time: 0.24s, CPU: 56.35%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 35.50%\n",
      "Epoch 33, Train Loss: 487705388.25, Val Loss: 96535874.25\n",
      "Time: 0.15s, CPU: 32.95%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 36.50%\n",
      "Epoch 34, Train Loss: 444715499.5, Val Loss: 88075053.0\n",
      "Time: 0.16s, CPU: 35.90%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.50%\n",
      "Epoch 35, Train Loss: 416880583.75, Val Loss: 81916110.0\n",
      "Time: 0.24s, CPU: 53.00%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 37.50%\n",
      "Epoch 36, Train Loss: 438698800.5, Val Loss: 82076706.0\n",
      "Time: 0.21s, CPU: 49.85%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 33.00%\n",
      "Epoch 37, Train Loss: 445195002.0, Val Loss: 79713942.75\n",
      "Time: 0.16s, CPU: 41.15%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 37.50%\n",
      "Epoch 38, Train Loss: 414156720.25, Val Loss: 79304001.75\n",
      "Time: 0.16s, CPU: 39.75%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 40.50%\n",
      "Epoch 39, Train Loss: 418264494.0, Val Loss: 83936958.75\n",
      "Time: 0.16s, CPU: 56.10%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 38.50%\n",
      "Epoch 40, Train Loss: 412121857.5, Val Loss: 80130660.75\n",
      "Time: 0.16s, CPU: 42.05%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 39.00%\n",
      "Epoch 41, Train Loss: 387339011.5, Val Loss: 74651287.5\n",
      "Time: 0.16s, CPU: 40.70%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 42, Train Loss: 404256726.0, Val Loss: 83016346.5\n",
      "Time: 0.16s, CPU: 40.05%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 40.50%\n",
      "Epoch 43, Train Loss: 383191653.5, Val Loss: 71904118.5\n",
      "Time: 0.16s, CPU: 47.60%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 41.50%\n",
      "Epoch 44, Train Loss: 345320308.0, Val Loss: 69298463.25\n",
      "Time: 0.16s, CPU: 33.50%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Epoch 45, Train Loss: 376002969.5, Val Loss: 78707949.75\n",
      "Time: 0.16s, CPU: 39.95%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 46, Train Loss: 371547000.5, Val Loss: 87648345.0\n",
      "Time: 0.16s, CPU: 40.95%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 43.00%\n",
      "Epoch 47, Train Loss: 313652549.5, Val Loss: 71869754.25\n",
      "Time: 0.19s, CPU: 48.75%, Memory: 3.34GB, GPU: 0.06GB, GPU Util: 39.50%\n",
      "Epoch 48, Train Loss: 267220882.0, Val Loss: 58098350.25\n",
      "Time: 0.19s, CPU: 38.30%, Memory: 3.32GB, GPU: 0.06GB, GPU Util: 36.00%\n",
      "Epoch 49, Train Loss: 259594299.9375, Val Loss: 63216564.75\n",
      "Time: 0.16s, CPU: 41.65%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 35.00%\n",
      "Epoch 50, Train Loss: 281538826.0625, Val Loss: 67635121.5\n",
      "Time: 0.16s, CPU: 35.80%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 39.00%\n",
      "Epoch 51, Train Loss: 289961435.875, Val Loss: 69407502.75\n",
      "Time: 0.16s, CPU: 47.75%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Epoch 52, Train Loss: 312078222.25, Val Loss: 62795081.25\n",
      "Time: 0.16s, CPU: 40.30%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Epoch 53, Train Loss: 366833100.25, Val Loss: 63997080.75\n",
      "Time: 0.16s, CPU: 41.40%, Memory: 3.33GB, GPU: 0.06GB, GPU Util: 41.00%\n",
      "Early stopping at epoch 54\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/trainable_gcesn_1l_slim160.pth\n",
      "Average Time per Epoch: 0.27s\n",
      "Average CPU Usage: 46.24%\n",
      "Average Memory Usage: 3.33GB\n",
      "Average GPU Usage: 0.06GB\n",
      "Average GPU Utilization: 33.31%\n",
      "\n",
      "Total Training Time: 14.68s\n",
      "Max CPU Usage: 88.95%\n",
      "Max Memory Usage: 3.34GB\n",
      "Max GPU Usage: 0.06GB\n",
      "Max GPU Utilization: 43.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "trainable_gcesn_slim160 = TrainableGCESN_1layer(slim160_num_features, 2*slim160_num_features, slim160_num_classes, num_iterations=6)\n",
    "print(f\"Total number of trainable parameters: {trainable_gcesn_slim160.count_parameters()}\\n\")\n",
    "                \n",
    "single_train(trainable_gcesn_slim160, slim160_train_loader, slim160_val_loader,\n",
    "                lr=0.001, num_epochs=500, patience=5, step_size=100, gamma=0.1, \n",
    "                save_path='models/trainable_gcesn_1l_slim160.pth',\n",
    "                binary_classification=True, is_esn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5405\n",
      "Average Sensitivity (Recall): 0.8235\n",
      "Average Specificity: 0.3000\n",
      "\n",
      "Average Inference Time per Batch: 0.0034s\n",
      "Average CPU Usage: 30.75%\n",
      "Average Memory Usage: 3.33GB\n",
      "Average GPU Usage: 0.09GB\n",
      "Average GPU Utilization: 1.00%\n"
     ]
    }
   ],
   "source": [
    "trainable_gcesn_slim160 = TrainableGCESN(slim160_num_features, 2*slim160_num_features, slim160_num_classes, num_iterations=6)\n",
    "trainable_gcesn_slim160.load_state_dict(torch.load('models/trainable_gcesn_1l_slim160.pth'))\n",
    "single_test(trainable_gcesn_slim160.to(device), slim160_test_loader)\n",
    "inference_performance(trainable_gcesn_slim160.to(device), slim160_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
