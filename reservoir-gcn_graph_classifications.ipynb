{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'using {device}')\n",
    "\n",
    "def set_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "Num Features = 7, Num Classes = 2\n"
     ]
    }
   ],
   "source": [
    "# Load the MUTAG dataset\n",
    "mutag_data = TUDataset(root='/tmp/MUTAG', name='MUTAG')\n",
    "print(mutag_data[0])\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_val_data, test_data = train_test_split(mutag_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "mutag_train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "mutag_val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "mutag_test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get the number of features and classes from the dataset\n",
    "mutag_num_features = mutag_data.num_features\n",
    "mutag_num_classes = mutag_data.num_classes\n",
    "print(f'Num Features = {mutag_num_features}, Num Classes = {mutag_num_classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMCI-AD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLIM160 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maturation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters() \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "## GCN Models\n",
    "class GCN1Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN1Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.fc = torch.nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.gcn1(x, adj))\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class GCN2Layer(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_features, num_classes):\n",
    "        super(GCN2Layer, self).__init__()\n",
    "        self.gcn1 = GCN(num_features, hidden_features)\n",
    "        self.gcn2 = GCN(hidden_features, hidden_features*2)\n",
    "        self.fc = torch.nn.Linear(hidden_features*2, num_classes)\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        x = F.relu(self.gcn1(x, adj))\n",
    "        x = F.relu(self.gcn2(x, adj))\n",
    "        x = global_mean_pool(x, batch)  # Global mean pooling\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "## GCESN Model\n",
    "class RidgeLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, alpha=1):\n",
    "        super(RidgeLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def ridge_regularization(self):\n",
    "        return self.alpha * torch.sum(self.linear.weight ** 2)\n",
    "\n",
    "class GCESN_1layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(GCESN_1layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.ridge_layer = RidgeLayer(hidden_features, hidden_features, ridge_alpha)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / rhoW\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W))))\n",
    "        h = self.ridge_layer(h)\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class GCESN_2layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(GCESN_2layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.ridge_layer_1 = RidgeLayer(hidden_features, 2*hidden_features, ridge_alpha)\n",
    "        self.ridge_layer_2 = RidgeLayer(2*hidden_features, hidden_features, 0.7)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win_1 = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_1 = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_2 = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues_1, _ = torch.linalg.eig(self.W_1)\n",
    "        rhoW_1 = max(abs(eigenvalues_1))\n",
    "        self.W_1 *= self.spectral_radius / rhoW_1\n",
    "\n",
    "        eigenvalues_2, _ = torch.linalg.eig(self.W_2)\n",
    "        rhoW_2 = max(abs(eigenvalues_2))\n",
    "        self.W_2 *= self.spectral_radius / rhoW_2\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win_1)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W_1))))\n",
    "        h = self.ridge_layer_1(h)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            h = (1-self.leaky_rate)*h + self.leaky_rate*(F.relu(torch.mm(adj, torch.mm(h, self.W_2))))\n",
    "        h = self.ridge_layer_2(h)\n",
    "\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "## Decoupled GCESN Model\n",
    "class decoupledGCESN_1layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=1, ridge_alpha=0.9):\n",
    "        super(decoupledGCESN_1layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.spectral_radius = 0.9\n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.gcn1 = GCN(in_features, hidden_features)\n",
    "        self.ridge_layer = RidgeLayer(hidden_features, hidden_features, ridge_alpha)\n",
    "        self.fc = nn.Linear(hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W   = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / rhoW\n",
    "        \n",
    "    def forward(self, x, adj, batch):\n",
    "        n_node, _ = x.shape\n",
    "        # set_seed(42) \n",
    "        state = torch.zeros(n_node, self.hidden_features).to(device)\n",
    "        h = x\n",
    "\n",
    "        h = F.relu(self.gcn1(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win) + torch.mm(state, self.W)))\n",
    "        \n",
    "        x = self.ridge_layer(state)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class decoupledGCESN_2layer(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=1, ridge_alpha=0.9):\n",
    "        super(decoupledGCESN_2layer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.spectral_radius = 0.9\n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.gcn1 = GCN(in_features, hidden_features)\n",
    "        self.ridge_layer_1 = RidgeLayer(hidden_features, 2*hidden_features, ridge_alpha)\n",
    "        \n",
    "        self.gcn2 = GCN(hidden_features, 2*hidden_features)\n",
    "        # self.ridge_layer_2 = RidgeLayer(2*hidden_features, hidden_features, 0.7)\n",
    "        self.fc = nn.Linear(2*hidden_features, num_classes)\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.Win_1 = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_1   = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.Win_2 = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.W_2   = nn.Parameter(torch.rand((2*self.hidden_features, 2*self.hidden_features)) * 2 - 1, requires_grad=False)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues_1, _ = torch.linalg.eig(self.W_1)\n",
    "        rhoW_1 = max(abs(eigenvalues_1))\n",
    "        self.W_1 *= self.spectral_radius / rhoW_1\n",
    "\n",
    "        eigenvalues_2, _ = torch.linalg.eig(self.W_2)\n",
    "        rhoW_2 = max(abs(eigenvalues_2))\n",
    "        self.W_2 *= self.spectral_radius / rhoW_2\n",
    "        \n",
    "    def forward(self, x, adj, batch):\n",
    "        n_node, _ = x.shape\n",
    "        state = torch.zeros(n_node, self.hidden_features).to(device)\n",
    "        h = x\n",
    "\n",
    "        h = F.relu(self.gcn1(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win_1) + torch.mm(state, self.W_1)))\n",
    "        state = self.ridge_layer_1(state)\n",
    "\n",
    "        h = F.relu(self.gcn2(h, adj))\n",
    "        for _ in range(self.num_iterations):\n",
    "            state = (1-self.leaky_rate)*state + self.leaky_rate*(torch.relu(torch.mm(h, self.Win_2) + torch.mm(state, self.W_2)))\n",
    "        # h = self.ridge_layer_2(state)\n",
    "        \n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings & Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single Functions\n",
    "def single_train(model, loader, val_loader, lr=0.001, num_epochs=100, patience=5, step_size=50, gamma=0.1, save_path='models/gcn_x.pth', binary_classification=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage before the epoch\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        for data in loader:\n",
    "            x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        \n",
    "        # Measure CPU and GPU usage after the epoch\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during the epoch\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                x, edge_index, batch, y = val_data.x.to(device), val_data.edge_index.to(device), val_data.batch.to(device), val_data.y.to(device)\n",
    "                adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "                val_output = model(x, adj_matrix, batch)\n",
    "                val_loss += criterion(val_output, y).item() * val_data.num_graphs\n",
    "\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Early stopping logic considering both training and validation loss\n",
    "        if val_loss < best_val_loss or epoch_loss < best_train_loss:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            total_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), save_path)  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                total_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {epoch_loss}, Val Loss: {val_loss}')\n",
    "        print(f'Time: {epoch_time[-1]:.2f}s, CPU: {cpu_usage_percent[-1]:.2f}%, Memory: {memory_usage[-1]:.2f}GB, GPU: {gpu_usage[-1]:.2f}GB, GPU Util: {gpu_usage_percent[-1]:.2f}%')\n",
    "\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "    avg_epoch_time = np.mean(epoch_time)\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_gpu_usage_percent = np.mean(gpu_usage_percent)\n",
    "    total_training_time = np.sum(epoch_time)\n",
    "    max_cpu_usage_percent = np.max(cpu_usage_percent)\n",
    "    max_memory_usage = np.max(memory_usage)\n",
    "    max_gpu_usage = np.max(gpu_usage)\n",
    "    max_gpu_usage_percent = np.max(gpu_usage_percent)\n",
    "\n",
    "    print(f'Average Time per Epoch: {avg_epoch_time:.2f}s')\n",
    "    print(f'Average CPU Usage: {avg_cpu_usage_percent:.2f}%')\n",
    "    print(f'Average Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'Average GPU Usage: {avg_gpu_usage:.2f}GB')\n",
    "    print(f'Average GPU Utilization: {avg_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    print(f'\\nTotal Training Time: {total_training_time:.2f}s')\n",
    "    print(f'Max CPU Usage: {max_cpu_usage_percent:.2f}%')\n",
    "    print(f'Max Memory Usage: {max_memory_usage:.2f}GB')\n",
    "    print(f'Max GPU Usage: {max_gpu_usage:.2f}GB')\n",
    "    print(f'Max GPU Utilization: {max_gpu_usage_percent:.2f}%')\n",
    "\n",
    "    return\n",
    "\n",
    "def binary_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate True Positives, True Negatives, False Positives, False Negatives\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    \n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "def multiclass_evaluation(y_true, y_pred):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f'Confusion Matrix:\\n{cm}')\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    sensitivity = np.zeros(cm.shape[0])\n",
    "    specificity = np.zeros(cm.shape[0])\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        TP = cm[i, i]\n",
    "        FN = np.sum(cm[i, :]) - TP\n",
    "        FP = np.sum(cm[:, i]) - TP\n",
    "        TN = np.sum(cm) - (TP + FN + FP)\n",
    "\n",
    "        sensitivity[i] = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity[i] = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    avg_sensitivity = np.mean(sensitivity)\n",
    "    avg_specificity = np.mean(specificity)\n",
    "\n",
    "    return accuracy, avg_sensitivity, avg_specificity\n",
    "    \n",
    "def single_test(model, loader, binary_classification=True):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            pred = output.argmax(dim=1)\n",
    "            y_true.extend(data.y.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    if binary_classification:\n",
    "        accuracy, sensitivity, specificity = binary_evaluation(y_true, y_pred)\n",
    "    else:\n",
    "        accuracy, sensitivity, specificity = multiclass_evaluation(y_true, y_pred)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Average Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Average Specificity: {specificity:.4f}')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Runs\n",
    "def multi_train(model, loader, val_loader, lr=0.001, num_epochs=100, patience=5, step_size=50, gamma=0.1, binary_classification=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage_percent = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    gpu_usage_percent = []\n",
    "\n",
    "    best_train_loss = float('inf')\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Get the process object for the current process\n",
    "    process = psutil.Process()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        set_seed(42)\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Measure CPU and GPU usage before the epoch\n",
    "        cpu_usage_before = psutil.cpu_percent(interval=None)\n",
    "        memory_before = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_before = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_before = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_before = 0\n",
    "            gpu_util_before = 0\n",
    "\n",
    "        for data in loader:\n",
    "            x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * data.num_graphs\n",
    "        \n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        \n",
    "        # Measure CPU and GPU usage after the epoch\n",
    "        cpu_usage_after = psutil.cpu_percent(interval=None)\n",
    "        memory_after = process.memory_info().rss\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_usage_after = torch.cuda.memory_allocated(device)\n",
    "            gpu_util_after = torch.cuda.utilization(device)\n",
    "        else:\n",
    "            gpu_usage_after = 0\n",
    "            gpu_util_after = 0\n",
    "\n",
    "        # Calculate average CPU and GPU usage during the epoch\n",
    "        cpu_usage_percent.append((cpu_usage_before + cpu_usage_after) / 2)\n",
    "        memory_usage.append((memory_before + memory_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage.append((gpu_usage_before + gpu_usage_after) / 2 / (1024**3))  # Convert to GB\n",
    "        gpu_usage_percent.append((gpu_util_before + gpu_util_after) / 2)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                x, edge_index, batch, y = val_data.x.to(device), val_data.edge_index.to(device), val_data.batch.to(device), val_data.y.to(device)\n",
    "                adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "                val_output = model(x, adj_matrix, batch)\n",
    "                val_loss += criterion(val_output, y).item() * val_data.num_graphs\n",
    "\n",
    "        validation_loss.append(val_loss)\n",
    "\n",
    "        # Early stopping logic considering both training and validation loss\n",
    "        if val_loss < best_val_loss or epoch_loss < best_train_loss:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "            epochs_no_improve = 0\n",
    "            total_epoch = epoch + 1\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                total_epoch = epoch + 1\n",
    "                break\n",
    "\n",
    "    total_training_time = np.sum(epoch_time)\n",
    "    avg_epoch_time = np.mean(epoch_time)\n",
    "\n",
    "    avg_cpu_usage_percent = np.mean(cpu_usage_percent)\n",
    "    avg_gpu_usage = np.mean(gpu_usage)\n",
    "    avg_memory_usage = np.mean(memory_usage)\n",
    "\n",
    "    max_cpu_usage_percent = np.max(cpu_usage_percent)\n",
    "    max_gpu_usage = np.max(gpu_usage)\n",
    "    max_memory_usage = np.max(memory_usage)\n",
    "\n",
    "    return training_loss[-1], total_epoch, total_training_time, avg_epoch_time, avg_cpu_usage_percent, avg_gpu_usage, avg_memory_usage, max_cpu_usage_percent, max_gpu_usage, max_memory_usage \n",
    "   \n",
    "def multi_test(model, loader, binary_classification=True):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)\n",
    "            adj_matrix = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "            output = model(x, adj_matrix, batch)\n",
    "            pred = output.argmax(dim=1)\n",
    "            y_true.extend(data.y.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "\n",
    "    if binary_classification:\n",
    "        accuracy, sensitivity, specificity = binary_evaluation(y_true, y_pred)\n",
    "    else:\n",
    "        accuracy, sensitivity, specificity = multiclass_evaluation(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "def multi_train_test(model, train_loader, val_loader, test_loader, num_runs=10, \n",
    "                    lr=0.001, num_epochs=100, patience=5, step_size=50, gamma=0.1, \n",
    "                    binary_classification=True):\n",
    "\n",
    "    all_accuracies = []\n",
    "    all_sensitivities = []\n",
    "    all_specificities = []\n",
    "\n",
    "    all_training_times = []\n",
    "    all_epoch_time = []\n",
    "    all_epoch = []\n",
    "\n",
    "    all_avg_cpu_usages = []\n",
    "    all_avg_gpu_usages = []\n",
    "    all_avg_memory_usages = []\n",
    "\n",
    "    all_max_cpu_usages = []\n",
    "    all_max_gpu_usages = []\n",
    "    all_max_memory_usages = []\n",
    "    \n",
    "\n",
    "    for run in range(num_runs):\n",
    "        set_seed(run)\n",
    "        model.initialize_weights()\n",
    "        loss, total_epoch, training_time, epoch_time, avg_cpu, avg_gpu, avg_memory, max_cpu, max_gpu, max_memory = multi_train(model, train_loader, val_loader, lr=0.01, num_epochs=num_epochs)\n",
    "\n",
    "        accuracy, sensitivity, specificity = multi_test(model, test_loader, binary_classification)\n",
    "\n",
    "        # Store results\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_sensitivities.append(sensitivity)\n",
    "        all_specificities.append(specificity)\n",
    "\n",
    "        all_training_times.append(training_time)\n",
    "        all_epoch_time.append(epoch_time)\n",
    "        all_epoch.append(total_epoch)\n",
    "\n",
    "        all_avg_cpu_usages.append(avg_cpu)\n",
    "        all_avg_gpu_usages.append(avg_gpu)\n",
    "        all_avg_memory_usages.append(avg_memory)\n",
    "\n",
    "        all_max_cpu_usages.append(max_cpu)\n",
    "        all_max_gpu_usages.append(max_gpu)\n",
    "        all_max_memory_usages.append(max_memory)\n",
    "        \n",
    "\n",
    "        print(f'\\nRun {run+1}/{num_runs} -> Loss: {loss:.5f}, Total Training Time: {training_time:.2f}s')\n",
    "        print(f'  Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    # Compute average values across all runs\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_sensitivity = np.mean(all_sensitivities)\n",
    "    avg_specificity = np.mean(all_specificities)\n",
    "\n",
    "    std_accuracy = np.std(all_accuracies)\n",
    "    std_sensitivity = np.std(all_sensitivities)\n",
    "    std_specificity = np.std(all_specificities)\n",
    "\n",
    "    max_accuracy = np.max(all_accuracies)\n",
    "    max_sensitivity = np.max(all_sensitivities)\n",
    "    max_specificity = np.max(all_specificities)\n",
    "\n",
    "    avg_training_time = np.mean(all_training_times)\n",
    "    avg_epoch_time = np.mean(all_epoch_time)\n",
    "    avg_num_epoch = np.mean(all_epoch)\n",
    "\n",
    "    avg_cpu_usage = np.mean(all_avg_cpu_usages)\n",
    "    avg_gpu_usage = np.mean(all_avg_gpu_usages)\n",
    "    avg_memory_usage = np.mean(all_avg_memory_usages)\n",
    "\n",
    "    avg_max_cpu_usage = np.mean(all_max_cpu_usages)\n",
    "    avg_max_gpu_usage = np.mean(all_max_gpu_usages)\n",
    "    avg_max_memory_usage = np.mean(all_max_memory_usages)\n",
    "    \n",
    "\n",
    "    print('Overall Results:')\n",
    "    print(f'  Avg Accuracy: {avg_accuracy:.4f} ± {std_accuracy:.2f}, Avg Sensitivity: {avg_sensitivity:.4f} ± {std_sensitivity:.2f}, Avg Specificity: {avg_specificity:.4f} ± {std_specificity:.2f}')\n",
    "    print(f'  Max Accuracy: {max_accuracy:.4f}, Max Sensitivity: {max_sensitivity:.4f}, Max Specificity: {max_specificity:.4f}')\n",
    "    print(f'  Avg Num Epoch:{avg_num_epoch:.2f}, Avg Training Time: {avg_training_time:.2f}s, Avg Epoch Time: {avg_epoch_time:.2f}s') \n",
    "    print(f'  Avg CPU Usage: {avg_cpu_usage:.2f}%, Avg GPU Usage: {avg_gpu_usage:.2f}%, Avg Memory Usage: {avg_memory_usage:.2f}GB')\n",
    "    print(f'  Avg Max CPU Usage: {avg_max_cpu_usage:.2f}%, Avg Max GPU Usage: {avg_max_gpu_usage:.2f}GB, Avg Max Memory Usage: {avg_max_memory_usage:.2f}GB')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN 1-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN1Layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 284\n",
      "\n",
      "Epoch 1, Train Loss: 94.14868241548538, Val Loss: 10.549120903015137\n",
      "Time: 0.10s, CPU: 20.65%, Memory: 0.12GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 84.44917887449265, Val Loss: 11.329365670681\n",
      "Time: 0.01s, CPU: 36.65%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 3, Train Loss: 81.23270785808563, Val Loss: 11.756115853786469\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 4, Train Loss: 80.24671220779419, Val Loss: 11.568961143493652\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 5, Train Loss: 79.62164026498795, Val Loss: 11.206330955028534\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 6, Train Loss: 79.45931816101074, Val Loss: 10.958997309207916\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 7, Train Loss: 79.3338468670845, Val Loss: 10.866145491600037\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 8, Train Loss: 78.86434942483902, Val Loss: 10.888813734054565\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 9, Train Loss: 78.12593364715576, Val Loss: 10.946557223796844\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 10, Train Loss: 77.37531220912933, Val Loss: 10.955286026000977\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 11, Train Loss: 76.73067849874496, Val Loss: 10.897073149681091\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 12, Train Loss: 76.18613165616989, Val Loss: 10.813261270523071\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 13, Train Loss: 75.69723242521286, Val Loss: 10.750460922718048\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 14, Train Loss: 75.17762440443039, Val Loss: 10.71726679801941\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 15, Train Loss: 74.62532091140747, Val Loss: 10.703313946723938\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 16, Train Loss: 74.05802303552628, Val Loss: 10.704942047595978\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 17, Train Loss: 73.52021569013596, Val Loss: 10.69248229265213\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 18, Train Loss: 73.03480327129364, Val Loss: 10.647140443325043\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 19, Train Loss: 72.61591893434525, Val Loss: 10.577105283737183\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 20, Train Loss: 72.24941766262054, Val Loss: 10.520010888576508\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 21, Train Loss: 71.88123792409897, Val Loss: 10.49483835697174\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 22, Train Loss: 71.47626513242722, Val Loss: 10.472457110881805\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 23, Train Loss: 71.08599764108658, Val Loss: 10.458587408065796\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 24, Train Loss: 70.69366735219955, Val Loss: 10.439586639404297\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 25, Train Loss: 70.3400274515152, Val Loss: 10.399213135242462\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 26, Train Loss: 70.03741747140884, Val Loss: 10.385197699069977\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 27, Train Loss: 69.71278101205826, Val Loss: 10.381034910678864\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 28, Train Loss: 69.3977597951889, Val Loss: 10.367452204227448\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 29, Train Loss: 69.11588460206985, Val Loss: 10.33398449420929\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 30, Train Loss: 68.88074570894241, Val Loss: 10.303735435009003\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 31, Train Loss: 68.6517681479454, Val Loss: 10.276626348495483\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 32, Train Loss: 68.42996215820312, Val Loss: 10.251536071300507\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 33, Train Loss: 68.21338933706284, Val Loss: 10.232345759868622\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 34, Train Loss: 67.99820727109909, Val Loss: 10.20680844783783\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 35, Train Loss: 67.8024138212204, Val Loss: 10.170038044452667\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 36, Train Loss: 67.62667167186737, Val Loss: 10.151338577270508\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 37, Train Loss: 67.4356861114502, Val Loss: 10.133848786354065\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 38, Train Loss: 67.25000590085983, Val Loss: 10.12489914894104\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 39, Train Loss: 67.06902742385864, Val Loss: 10.107819736003876\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 40, Train Loss: 66.91163903474808, Val Loss: 10.082689225673676\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 41, Train Loss: 66.7787275314331, Val Loss: 10.080616772174835\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 42, Train Loss: 66.62415301799774, Val Loss: 10.069080591201782\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 43, Train Loss: 66.48993980884552, Val Loss: 10.03538578748703\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 44, Train Loss: 66.3850885629654, Val Loss: 10.022538006305695\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 45, Train Loss: 66.25860965251923, Val Loss: 10.018356442451477\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 46, Train Loss: 66.1278373003006, Val Loss: 10.007299482822418\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 47, Train Loss: 66.01115018129349, Val Loss: 9.98768001794815\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 48, Train Loss: 65.90902727842331, Val Loss: 9.968271553516388\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 49, Train Loss: 65.81068056821823, Val Loss: 9.953933358192444\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 50, Train Loss: 65.71012717485428, Val Loss: 9.94335651397705\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 51, Train Loss: 65.60926914215088, Val Loss: 9.931636154651642\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 52, Train Loss: 65.51326411962509, Val Loss: 9.917327463626862\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 53, Train Loss: 65.42378014326096, Val Loss: 9.90279346704483\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 54, Train Loss: 65.33768367767334, Val Loss: 9.890027940273285\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 55, Train Loss: 65.25250500440598, Val Loss: 9.878521263599396\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 56, Train Loss: 65.16890633106232, Val Loss: 9.867084324359894\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 57, Train Loss: 65.08844411373138, Val Loss: 9.854975938796997\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 58, Train Loss: 65.0116394162178, Val Loss: 9.842909574508667\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 59, Train Loss: 64.93795150518417, Val Loss: 9.83139306306839\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 60, Train Loss: 64.86566716432571, Val Loss: 9.82005625963211\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 61, Train Loss: 64.79498702287674, Val Loss: 9.808136522769928\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 62, Train Loss: 64.72720110416412, Val Loss: 9.79850023984909\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 63, Train Loss: 64.66047257184982, Val Loss: 9.788969457149506\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 64, Train Loss: 64.59692353010178, Val Loss: 9.77939486503601\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 65, Train Loss: 64.53516376018524, Val Loss: 9.768785834312439\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 66, Train Loss: 64.47936487197876, Val Loss: 9.757941663265228\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 67, Train Loss: 64.42160528898239, Val Loss: 9.747105538845062\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 68, Train Loss: 64.36625862121582, Val Loss: 9.73660558462143\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 69, Train Loss: 64.31163507699966, Val Loss: 9.726475775241852\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 70, Train Loss: 64.25782126188278, Val Loss: 9.71640944480896\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 71, Train Loss: 64.20547384023666, Val Loss: 9.70660775899887\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 72, Train Loss: 64.15391743183136, Val Loss: 9.69696432352066\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 73, Train Loss: 64.10389626026154, Val Loss: 9.687154591083527\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 74, Train Loss: 64.05490678548813, Val Loss: 9.6774423122406\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 75, Train Loss: 64.00702029466629, Val Loss: 9.66776579618454\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 76, Train Loss: 63.959817826747894, Val Loss: 9.658613204956055\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 77, Train Loss: 63.913476288318634, Val Loss: 9.649261236190796\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 78, Train Loss: 63.868349850177765, Val Loss: 9.639765322208405\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 79, Train Loss: 63.82387167215347, Val Loss: 9.630285501480103\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 80, Train Loss: 63.779949367046356, Val Loss: 9.620513319969177\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 81, Train Loss: 63.736639976501465, Val Loss: 9.610052704811096\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 82, Train Loss: 63.694103360176086, Val Loss: 9.603572487831116\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 83, Train Loss: 63.651773393154144, Val Loss: 9.594095349311829\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 84, Train Loss: 63.61240714788437, Val Loss: 9.58432674407959\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 85, Train Loss: 63.572392642498016, Val Loss: 9.575238525867462\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 86, Train Loss: 63.5313395857811, Val Loss: 9.566179811954498\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 87, Train Loss: 63.49112832546234, Val Loss: 9.5563405752182\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 88, Train Loss: 63.452043414115906, Val Loss: 9.546104371547699\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 89, Train Loss: 63.41358882188797, Val Loss: 9.536247253417969\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 90, Train Loss: 63.37396901845932, Val Loss: 9.526623487472534\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 91, Train Loss: 63.33597218990326, Val Loss: 9.517236649990082\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 92, Train Loss: 63.29887992143631, Val Loss: 9.507869482040405\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 93, Train Loss: 63.26274073123932, Val Loss: 9.497865736484528\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 94, Train Loss: 63.2255522608757, Val Loss: 9.485034942626953\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 95, Train Loss: 63.19453191757202, Val Loss: 9.481392502784729\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 96, Train Loss: 63.15217882394791, Val Loss: 9.471277892589569\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 97, Train Loss: 63.119647324085236, Val Loss: 9.460289776325226\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 98, Train Loss: 63.086681485176086, Val Loss: 9.452395141124725\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 99, Train Loss: 63.052119851112366, Val Loss: 9.443503618240356\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 100, Train Loss: 63.01838308572769, Val Loss: 9.432616531848907\n",
      "Time: 0.01s, CPU: 11.10%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 101, Train Loss: 62.98462861776352, Val Loss: 9.42152202129364\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 102, Train Loss: 62.94966134428978, Val Loss: 9.410914778709412\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 103, Train Loss: 62.91418156027794, Val Loss: 9.401190876960754\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 104, Train Loss: 62.8782554268837, Val Loss: 9.391385614871979\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 105, Train Loss: 62.845503121614456, Val Loss: 9.38027411699295\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 106, Train Loss: 62.81181877851486, Val Loss: 9.371827840805054\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 107, Train Loss: 62.780343323946, Val Loss: 9.362472295761108\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 108, Train Loss: 62.74883362650871, Val Loss: 9.353111386299133\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 109, Train Loss: 62.71777078509331, Val Loss: 9.344042837619781\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 110, Train Loss: 62.6900572180748, Val Loss: 9.333562552928925\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 111, Train Loss: 62.65927082300186, Val Loss: 9.322536885738373\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 112, Train Loss: 62.62611213326454, Val Loss: 9.311007857322693\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 113, Train Loss: 62.59153217077255, Val Loss: 9.29744839668274\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 114, Train Loss: 62.5594844520092, Val Loss: 9.292680323123932\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 115, Train Loss: 62.53044167160988, Val Loss: 9.28249329328537\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 116, Train Loss: 62.502179861068726, Val Loss: 9.27137553691864\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 117, Train Loss: 62.47050791978836, Val Loss: 9.260353446006775\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 118, Train Loss: 62.43593010306358, Val Loss: 9.248276352882385\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 119, Train Loss: 62.40160450339317, Val Loss: 9.239408075809479\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 120, Train Loss: 62.37228661775589, Val Loss: 9.22891616821289\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 121, Train Loss: 62.34287542104721, Val Loss: 9.219450652599335\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 122, Train Loss: 62.316061943769455, Val Loss: 9.211000800132751\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 123, Train Loss: 62.2925783097744, Val Loss: 9.200119972229004\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 124, Train Loss: 62.26460236310959, Val Loss: 9.18813407421112\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 125, Train Loss: 62.233219891786575, Val Loss: 9.175711870193481\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 126, Train Loss: 62.199287712574005, Val Loss: 9.155940413475037\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 127, Train Loss: 62.18389290571213, Val Loss: 9.162333905696869\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 128, Train Loss: 62.14086413383484, Val Loss: 9.149520099163055\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 129, Train Loss: 62.12469032406807, Val Loss: 9.137946367263794\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 130, Train Loss: 62.10132598876953, Val Loss: 9.138131439685822\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 131, Train Loss: 62.05970785021782, Val Loss: 9.156762063503265\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 132, Train Loss: 61.99417972564697, Val Loss: 9.172351062297821\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 133, Train Loss: 61.93703866004944, Val Loss: 9.056243598461151\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 134, Train Loss: 62.02668645977974, Val Loss: 9.057866334915161\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 135, Train Loss: 61.97646242380142, Val Loss: 9.112517237663269\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 136, Train Loss: 61.911071836948395, Val Loss: 9.085846245288849\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 137, Train Loss: 61.91816997528076, Val Loss: 9.05137449502945\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 138, Train Loss: 61.921895921230316, Val Loss: 9.069034159183502\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 139, Train Loss: 61.85966822504997, Val Loss: 9.073492884635925\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 140, Train Loss: 61.8144513964653, Val Loss: 9.015443623065948\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 141, Train Loss: 61.830929428339005, Val Loss: 9.0021613240242\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 142, Train Loss: 61.79627487063408, Val Loss: 9.01637077331543\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 143, Train Loss: 61.73446899652481, Val Loss: 8.996100425720215\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 144, Train Loss: 61.718109369277954, Val Loss: 8.974083065986633\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 145, Train Loss: 61.69128116965294, Val Loss: 8.9745032787323\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 146, Train Loss: 61.66328293085098, Val Loss: 8.966073989868164\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 147, Train Loss: 61.64079564809799, Val Loss: 8.950618207454681\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 148, Train Loss: 61.618240267038345, Val Loss: 8.93987774848938\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 149, Train Loss: 61.58613941073418, Val Loss: 8.929573595523834\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 150, Train Loss: 61.55570459365845, Val Loss: 8.918589055538177\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 151, Train Loss: 61.52408555150032, Val Loss: 8.908325135707855\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 152, Train Loss: 61.50291299819946, Val Loss: 8.896726369857788\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 153, Train Loss: 61.500348180532455, Val Loss: 8.893181383609772\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 154, Train Loss: 61.447887659072876, Val Loss: 8.884986340999603\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 155, Train Loss: 61.43706366419792, Val Loss: 8.870899379253387\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 156, Train Loss: 61.42144700884819, Val Loss: 8.876759111881256\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 157, Train Loss: 61.373175889253616, Val Loss: 8.856068551540375\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 158, Train Loss: 61.3504356443882, Val Loss: 8.809178173542023\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 159, Train Loss: 61.3806155025959, Val Loss: 8.83877545595169\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 160, Train Loss: 61.31821832060814, Val Loss: 8.830117285251617\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 161, Train Loss: 61.31372791528702, Val Loss: 8.817963302135468\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 162, Train Loss: 61.30111864209175, Val Loss: 8.837732076644897\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 163, Train Loss: 61.240082412958145, Val Loss: 8.807953298091888\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 164, Train Loss: 61.228360801935196, Val Loss: 8.76639872789383\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 165, Train Loss: 61.227733075618744, Val Loss: 8.77103179693222\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 166, Train Loss: 61.1771522462368, Val Loss: 8.786773681640625\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 167, Train Loss: 61.115734338760376, Val Loss: 8.763601183891296\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 168, Train Loss: 61.10948038101196, Val Loss: 8.731360137462616\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 169, Train Loss: 61.11580416560173, Val Loss: 8.72360497713089\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 170, Train Loss: 61.092492282390594, Val Loss: 8.764244019985199\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 171, Train Loss: 61.04987046122551, Val Loss: 8.742390275001526\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 172, Train Loss: 61.045420706272125, Val Loss: 8.69470864534378\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 173, Train Loss: 61.06502032279968, Val Loss: 8.682660162448883\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 174, Train Loss: 61.037195682525635, Val Loss: 8.694088160991669\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 175, Train Loss: 60.98197340965271, Val Loss: 8.671385943889618\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 176, Train Loss: 60.960062474012375, Val Loss: 8.645737767219543\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 177, Train Loss: 60.946389973163605, Val Loss: 8.660605251789093\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 178, Train Loss: 60.92096334695816, Val Loss: 8.649331033229828\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 179, Train Loss: 60.91246369481087, Val Loss: 8.634180128574371\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 180, Train Loss: 60.89579465985298, Val Loss: 8.628940880298615\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 181, Train Loss: 60.87890511751175, Val Loss: 8.625617623329163\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 182, Train Loss: 60.843486070632935, Val Loss: 8.61964613199234\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 183, Train Loss: 60.84610790014267, Val Loss: 8.626116514205933\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 184, Train Loss: 60.816262274980545, Val Loss: 8.6128368973732\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 185, Train Loss: 60.797330766916275, Val Loss: 8.580984771251678\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 186, Train Loss: 60.79455915093422, Val Loss: 8.59616607427597\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 187, Train Loss: 60.73199859261513, Val Loss: 8.60610544681549\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 188, Train Loss: 60.70209077000618, Val Loss: 8.61964613199234\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 189, Train Loss: 60.63163712620735, Val Loss: 8.579073250293732\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 190, Train Loss: 60.669544249773026, Val Loss: 8.6199751496315\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 191, Train Loss: 60.614551931619644, Val Loss: 8.553802371025085\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 192, Train Loss: 60.664973735809326, Val Loss: 8.570189774036407\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 193, Train Loss: 60.61143299937248, Val Loss: 8.590039014816284\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 194, Train Loss: 60.54936361312866, Val Loss: 8.560990691184998\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 195, Train Loss: 60.54649552702904, Val Loss: 8.59723448753357\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 196, Train Loss: 60.51594164967537, Val Loss: 8.477285206317902\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 197, Train Loss: 60.62632131576538, Val Loss: 8.48825991153717\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 198, Train Loss: 60.59133955836296, Val Loss: 8.60112726688385\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 199, Train Loss: 60.45759290456772, Val Loss: 8.482615649700165\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 200, Train Loss: 60.55010133981705, Val Loss: 8.48101794719696\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 201, Train Loss: 60.55184218287468, Val Loss: 8.52477103471756\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 202, Train Loss: 60.497714936733246, Val Loss: 8.498629331588745\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 203, Train Loss: 60.49384728074074, Val Loss: 8.466333746910095\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 204, Train Loss: 60.49635657668114, Val Loss: 8.473839461803436\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 205, Train Loss: 60.448528945446014, Val Loss: 8.436840176582336\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 206, Train Loss: 60.45098063349724, Val Loss: 8.414645791053772\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 207, Train Loss: 60.45568412542343, Val Loss: 8.430300951004028\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 208, Train Loss: 60.3840229511261, Val Loss: 8.396116197109222\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 209, Train Loss: 60.39801353216171, Val Loss: 8.39375764131546\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 210, Train Loss: 60.39084079861641, Val Loss: 8.435886204242706\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 211, Train Loss: 60.333069294691086, Val Loss: 8.410532176494598\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 212, Train Loss: 60.33299449086189, Val Loss: 8.402304947376251\n",
      "Time: 0.01s, CPU: 11.80%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 213, Train Loss: 60.30907163023949, Val Loss: 8.421531915664673\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 214, Train Loss: 60.2622492313385, Val Loss: 8.370251655578613\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 215, Train Loss: 60.278254091739655, Val Loss: 8.351076543331146\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 216, Train Loss: 60.324553579092026, Val Loss: 8.379280865192413\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 217, Train Loss: 60.2946717441082, Val Loss: 8.355803489685059\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 218, Train Loss: 60.301179230213165, Val Loss: 8.368404507637024\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 219, Train Loss: 60.2632437646389, Val Loss: 8.361916244029999\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 220, Train Loss: 60.234087228775024, Val Loss: 8.329816460609436\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 221, Train Loss: 60.2380476295948, Val Loss: 8.433980941772461\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 222, Train Loss: 60.10259345173836, Val Loss: 8.417838513851166\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 223, Train Loss: 60.12721064686775, Val Loss: 8.385029733181\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 224, Train Loss: 60.16584366559982, Val Loss: 8.4027698636055\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 225, Train Loss: 60.12326470017433, Val Loss: 8.373867273330688\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 226, Train Loss: 60.1382671892643, Val Loss: 8.462559878826141\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 227, Train Loss: 60.02703994512558, Val Loss: 8.426468074321747\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 228, Train Loss: 60.03208723664284, Val Loss: 8.425878882408142\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 229, Train Loss: 60.02503004670143, Val Loss: 8.473318219184875\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 230, Train Loss: 59.96666067838669, Val Loss: 8.449561893939972\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 231, Train Loss: 59.97474026679993, Val Loss: 8.366890847682953\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 232, Train Loss: 60.0349859893322, Val Loss: 8.31600934267044\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 233, Train Loss: 60.07251042127609, Val Loss: 8.385515213012695\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 234, Train Loss: 60.00608345866203, Val Loss: 8.458980023860931\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 235, Train Loss: 59.90948298573494, Val Loss: 8.413020372390747\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 236, Train Loss: 59.938926219940186, Val Loss: 8.427513241767883\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 237, Train Loss: 59.923927158117294, Val Loss: 8.433597385883331\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 238, Train Loss: 59.8944878578186, Val Loss: 8.412445485591888\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 239, Train Loss: 59.922229677438736, Val Loss: 8.410026133060455\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 240, Train Loss: 59.910874634981155, Val Loss: 8.393069207668304\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 241, Train Loss: 59.96546509861946, Val Loss: 8.39416354894638\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 242, Train Loss: 59.99346098303795, Val Loss: 8.453210592269897\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Early stopping at epoch 243\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_1l_mutag.pth\n",
      "Average Time per Epoch: 0.01s\n",
      "Average CPU Usage: 0.33%\n",
      "Average Memory Usage: 0.15GB\n",
      "Average GPU Usage: 0.00GB\n",
      "Average GPU Utilization: 0.00%\n",
      "\n",
      "Total Training Time: 1.92s\n",
      "Max CPU Usage: 36.65%\n",
      "Max Memory Usage: 0.15GB\n",
      "Max GPU Usage: 0.00GB\n",
      "Max GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn1_mutag = GCN1Layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes)\n",
    "print(gcn1_mutag)\n",
    "print(f\"Total number of trainable parameters: {(gcn1_mutag.count_parameters())*2}\\n\")\n",
    "single_train(gcn1_mutag, mutag_train_loader, mutag_val_loader, lr=0.01, num_epochs=500, step_size=500, save_path='models/gcn_1l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8158\n",
      "Average Sensitivity (Recall): 0.8462\n",
      "Average Specificity: 0.7500\n"
     ]
    }
   ],
   "source": [
    "single_test(gcn1_mutag, mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN 2-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2Layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (gcn2): GCN (14 -> 28)\n",
      "  (fc): Linear(in_features=28, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1180\n",
      "\n",
      "Epoch 1, Train Loss: 86.46722114086151, Val Loss: 11.68528139591217\n",
      "Time: 0.03s, CPU: 36.00%, Memory: 0.12GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 2, Train Loss: 80.8584617972374, Val Loss: 11.765201389789581\n",
      "Time: 0.01s, CPU: 52.25%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 3, Train Loss: 79.94231444597244, Val Loss: 11.294539868831635\n",
      "Time: 0.01s, CPU: 16.65%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 4, Train Loss: 79.69838201999664, Val Loss: 10.979202389717102\n",
      "Time: 0.01s, CPU: 7.15%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 5, Train Loss: 78.92960119247437, Val Loss: 10.866539776325226\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 6, Train Loss: 77.34770423173904, Val Loss: 10.953080356121063\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 7, Train Loss: 75.19219690561295, Val Loss: 10.701700150966644\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 8, Train Loss: 73.11086839437485, Val Loss: 10.182013213634491\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 9, Train Loss: 71.41098129749298, Val Loss: 10.199012160301208\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 10, Train Loss: 68.82054024934769, Val Loss: 9.913854897022247\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 11, Train Loss: 67.19393283128738, Val Loss: 9.694614708423615\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 12, Train Loss: 66.07376915216446, Val Loss: 9.734602868556976\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 13, Train Loss: 65.23304122686386, Val Loss: 9.590409994125366\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.13GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 14, Train Loss: 65.05366963148117, Val Loss: 9.682472348213196\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 15, Train Loss: 64.67376166582108, Val Loss: 9.622152149677277\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 16, Train Loss: 64.49860513210297, Val Loss: 9.601138830184937\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 17, Train Loss: 64.21159464120865, Val Loss: 9.574737846851349\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 18, Train Loss: 63.906593799591064, Val Loss: 9.484460949897766\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 19, Train Loss: 63.64144492149353, Val Loss: 9.46673959493637\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 20, Train Loss: 63.29860711097717, Val Loss: 9.386911690235138\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 21, Train Loss: 63.05598929524422, Val Loss: 9.363631010055542\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 22, Train Loss: 62.759312868118286, Val Loss: 9.304143190383911\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 23, Train Loss: 62.484799563884735, Val Loss: 9.236420094966888\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 24, Train Loss: 62.21164044737816, Val Loss: 9.192310273647308\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 25, Train Loss: 61.885161608457565, Val Loss: 9.112432301044464\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 26, Train Loss: 61.60598999261856, Val Loss: 9.03768539428711\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 27, Train Loss: 61.35011675953865, Val Loss: 9.007077813148499\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 28, Train Loss: 60.98519802093506, Val Loss: 8.87515515089035\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 29, Train Loss: 60.75697049498558, Val Loss: 8.853981792926788\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 30, Train Loss: 60.371290266513824, Val Loss: 8.730494678020477\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 31, Train Loss: 60.10594180226326, Val Loss: 8.661073744297028\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 32, Train Loss: 59.79511094093323, Val Loss: 8.571542501449585\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 33, Train Loss: 59.472879230976105, Val Loss: 8.463415503501892\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 34, Train Loss: 59.24041646718979, Val Loss: 8.425772488117218\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 35, Train Loss: 58.94946926832199, Val Loss: 8.31486314535141\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 36, Train Loss: 58.763433426618576, Val Loss: 8.288188576698303\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 37, Train Loss: 58.45696586370468, Val Loss: 8.181408941745758\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 38, Train Loss: 58.29679623246193, Val Loss: 8.120955526828766\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 39, Train Loss: 58.10964983701706, Val Loss: 8.085702359676361\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 40, Train Loss: 58.00678992271423, Val Loss: 8.029155135154724\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 41, Train Loss: 57.92739620804787, Val Loss: 7.992702126502991\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 42, Train Loss: 57.78824672102928, Val Loss: 7.934552729129791\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 43, Train Loss: 57.63038006424904, Val Loss: 7.827681005001068\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 44, Train Loss: 57.70956587791443, Val Loss: 7.845370173454285\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 45, Train Loss: 57.3852678835392, Val Loss: 7.778116464614868\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 46, Train Loss: 57.15857791900635, Val Loss: 7.717840075492859\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 47, Train Loss: 57.0663161277771, Val Loss: 7.713669240474701\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 48, Train Loss: 56.86142811179161, Val Loss: 7.626273036003113\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 49, Train Loss: 56.78312706947327, Val Loss: 7.628555595874786\n",
      "Time: 0.02s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 50, Train Loss: 56.575373113155365, Val Loss: 7.55138486623764\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 51, Train Loss: 56.48557862639427, Val Loss: 7.542573809623718\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 52, Train Loss: 56.31242987513542, Val Loss: 7.421895861625671\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 53, Train Loss: 56.350100845098495, Val Loss: 7.466225177049637\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 54, Train Loss: 56.01695328950882, Val Loss: 7.3223210871219635\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 55, Train Loss: 55.940161526203156, Val Loss: 7.331505864858627\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 56, Train Loss: 55.67951864004135, Val Loss: 7.338292747735977\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 57, Train Loss: 55.431188493967056, Val Loss: 7.253133952617645\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 58, Train Loss: 55.389956682920456, Val Loss: 7.31349304318428\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 59, Train Loss: 55.130753099918365, Val Loss: 7.26283997297287\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 60, Train Loss: 55.017334669828415, Val Loss: 7.256031185388565\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 61, Train Loss: 54.85415343940258, Val Loss: 7.2028085589408875\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 62, Train Loss: 54.773783788084984, Val Loss: 7.241965234279633\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 63, Train Loss: 54.575663074851036, Val Loss: 7.099916338920593\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 64, Train Loss: 54.67502045631409, Val Loss: 7.171420007944107\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 65, Train Loss: 54.438199892640114, Val Loss: 7.019955664873123\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 66, Train Loss: 54.537415727972984, Val Loss: 7.17776745557785\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 67, Train Loss: 54.147451892495155, Val Loss: 7.012437880039215\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 68, Train Loss: 54.287270709872246, Val Loss: 7.102062106132507\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 69, Train Loss: 54.06876376271248, Val Loss: 7.070774137973785\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 70, Train Loss: 54.002353474497795, Val Loss: 6.997126936912537\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 71, Train Loss: 54.05959531664848, Val Loss: 7.075100988149643\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 72, Train Loss: 53.81935624778271, Val Loss: 6.899577230215073\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 73, Train Loss: 53.990944400429726, Val Loss: 7.108370214700699\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 74, Train Loss: 53.53627856075764, Val Loss: 6.917079538106918\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 75, Train Loss: 53.67545486986637, Val Loss: 6.995538175106049\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 76, Train Loss: 53.46638648211956, Val Loss: 6.966323554515839\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 77, Train Loss: 53.372981905937195, Val Loss: 6.921427845954895\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 78, Train Loss: 53.34107042849064, Val Loss: 7.038137465715408\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.14GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 79, Train Loss: 53.012523502111435, Val Loss: 6.90688356757164\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 80, Train Loss: 53.09581995010376, Val Loss: 7.008082419633865\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 81, Train Loss: 52.86898250877857, Val Loss: 6.905648857355118\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 82, Train Loss: 52.90329299867153, Val Loss: 6.955830752849579\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 83, Train Loss: 52.654892429709435, Val Loss: 6.962570697069168\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 84, Train Loss: 52.587674885988235, Val Loss: 6.9355930387973785\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 85, Train Loss: 52.55563573539257, Val Loss: 6.943780928850174\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 86, Train Loss: 52.4393277913332, Val Loss: 6.897483319044113\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 87, Train Loss: 52.40767374634743, Val Loss: 6.947690695524216\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 88, Train Loss: 52.24427318572998, Val Loss: 6.85929536819458\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 89, Train Loss: 52.286446049809456, Val Loss: 6.948160082101822\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 90, Train Loss: 52.10774251818657, Val Loss: 6.880514770746231\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 91, Train Loss: 52.072577223181725, Val Loss: 6.8875752389431\n",
      "Time: 0.01s, CPU: 12.90%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 92, Train Loss: 51.92413814365864, Val Loss: 6.904896050691605\n",
      "Time: 0.01s, CPU: 14.30%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 93, Train Loss: 51.79220685362816, Val Loss: 6.885988712310791\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 94, Train Loss: 51.690114334225655, Val Loss: 6.849117279052734\n",
      "Time: 0.01s, CPU: 14.30%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 95, Train Loss: 51.66445203125477, Val Loss: 6.912108510732651\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 96, Train Loss: 51.48285694420338, Val Loss: 6.875641196966171\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 97, Train Loss: 51.42009852826595, Val Loss: 6.8239787220954895\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 98, Train Loss: 51.360902696847916, Val Loss: 6.875013560056686\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 99, Train Loss: 51.19117024540901, Val Loss: 6.864261478185654\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 100, Train Loss: 51.07587584853172, Val Loss: 6.841833740472794\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 101, Train Loss: 51.02818349003792, Val Loss: 6.838173866271973\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 102, Train Loss: 50.93635152280331, Val Loss: 6.878064125776291\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 103, Train Loss: 50.77166114747524, Val Loss: 6.7980073392391205\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 104, Train Loss: 50.78393368422985, Val Loss: 6.823506206274033\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 105, Train Loss: 50.67511057853699, Val Loss: 6.885935962200165\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 106, Train Loss: 50.4859022796154, Val Loss: 6.815502047538757\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 107, Train Loss: 50.47004644572735, Val Loss: 6.863987892866135\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 108, Train Loss: 50.300325974822044, Val Loss: 6.766390353441238\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 109, Train Loss: 50.396811217069626, Val Loss: 6.934470981359482\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 110, Train Loss: 50.01221361756325, Val Loss: 6.760338395833969\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 111, Train Loss: 50.123374596238136, Val Loss: 6.864825189113617\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 112, Train Loss: 49.82307554781437, Val Loss: 6.744766384363174\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 113, Train Loss: 49.943103551864624, Val Loss: 6.945728212594986\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 114, Train Loss: 49.484747529029846, Val Loss: 6.790740787982941\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 115, Train Loss: 49.5213775485754, Val Loss: 6.765346527099609\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 116, Train Loss: 49.40625086426735, Val Loss: 6.58485621213913\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 117, Train Loss: 49.72860476374626, Val Loss: 6.811349093914032\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 118, Train Loss: 49.23035880923271, Val Loss: 6.766453832387924\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 119, Train Loss: 48.85286924242973, Val Loss: 6.715950518846512\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 120, Train Loss: 48.73329804837704, Val Loss: 6.752823293209076\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 121, Train Loss: 48.38597698509693, Val Loss: 6.608380526304245\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 122, Train Loss: 48.494749665260315, Val Loss: 6.747307330369949\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 123, Train Loss: 48.102055102586746, Val Loss: 6.645933240652084\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 124, Train Loss: 48.15332229435444, Val Loss: 6.768733710050583\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 125, Train Loss: 47.80400709807873, Val Loss: 6.740382760763168\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 126, Train Loss: 47.597573921084404, Val Loss: 6.553392112255096\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 127, Train Loss: 47.850287199020386, Val Loss: 6.766646504402161\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 128, Train Loss: 47.27780678868294, Val Loss: 6.5757085382938385\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 129, Train Loss: 47.46876457333565, Val Loss: 6.715889275074005\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 130, Train Loss: 47.1740143597126, Val Loss: 6.679235547780991\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 131, Train Loss: 47.04045341908932, Val Loss: 6.546240001916885\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 132, Train Loss: 47.23462226986885, Val Loss: 6.791312992572784\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 133, Train Loss: 46.65997830033302, Val Loss: 6.578948646783829\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 134, Train Loss: 46.7525691986084, Val Loss: 6.58701628446579\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 135, Train Loss: 46.771083906292915, Val Loss: 6.749416887760162\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 136, Train Loss: 46.26446668803692, Val Loss: 6.531308144330978\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 137, Train Loss: 46.53600476682186, Val Loss: 6.747686415910721\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 138, Train Loss: 46.08743533492088, Val Loss: 6.568078100681305\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 139, Train Loss: 46.23118580877781, Val Loss: 6.652185022830963\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 140, Train Loss: 46.06861147284508, Val Loss: 6.747585833072662\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 141, Train Loss: 45.67024026811123, Val Loss: 6.467445194721222\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 142, Train Loss: 46.108263447880745, Val Loss: 6.704013794660568\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 143, Train Loss: 45.67620298266411, Val Loss: 6.6635821759700775\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 144, Train Loss: 45.47282333672047, Val Loss: 6.5171800553798676\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 145, Train Loss: 45.70643374323845, Val Loss: 6.687890142202377\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 146, Train Loss: 45.37833182513714, Val Loss: 6.578665226697922\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 147, Train Loss: 45.49667537212372, Val Loss: 6.54263511300087\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 148, Train Loss: 45.593526631593704, Val Loss: 6.6585516929626465\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 149, Train Loss: 45.25318431854248, Val Loss: 6.558237075805664\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 150, Train Loss: 45.23962914943695, Val Loss: 6.640694886445999\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 151, Train Loss: 44.97233359515667, Val Loss: 6.747893393039703\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 152, Train Loss: 44.70312158763409, Val Loss: 6.556888371706009\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 153, Train Loss: 44.84858711063862, Val Loss: 6.577645093202591\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 154, Train Loss: 44.827738508582115, Val Loss: 6.729632019996643\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 155, Train Loss: 44.36119058728218, Val Loss: 6.461842507123947\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 156, Train Loss: 44.792958810925484, Val Loss: 6.6716404259204865\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 157, Train Loss: 44.48497271537781, Val Loss: 6.667851358652115\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 158, Train Loss: 44.16656735539436, Val Loss: 6.433223336935043\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 159, Train Loss: 44.57215216755867, Val Loss: 6.700327545404434\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 160, Train Loss: 44.18573488295078, Val Loss: 6.691257655620575\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 161, Train Loss: 44.027447775006294, Val Loss: 6.4929574728012085\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 162, Train Loss: 44.226003631949425, Val Loss: 6.628808677196503\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 163, Train Loss: 43.94937048852444, Val Loss: 6.6083090007305145\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 164, Train Loss: 43.91636726260185, Val Loss: 6.5314896404743195\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 165, Train Loss: 44.010519310832024, Val Loss: 6.683358550071716\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 166, Train Loss: 43.71937869489193, Val Loss: 6.619993597269058\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 167, Train Loss: 43.59125868976116, Val Loss: 6.415501087903976\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 168, Train Loss: 43.92884111404419, Val Loss: 6.635102480649948\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 169, Train Loss: 43.66203811764717, Val Loss: 6.581101566553116\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 170, Train Loss: 43.46637220680714, Val Loss: 6.553618758916855\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 171, Train Loss: 43.469937548041344, Val Loss: 6.508267521858215\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 172, Train Loss: 43.599601954221725, Val Loss: 6.642157584428787\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 173, Train Loss: 43.27482457458973, Val Loss: 6.555523127317429\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 174, Train Loss: 43.19274115562439, Val Loss: 6.519875228404999\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 175, Train Loss: 43.34698249399662, Val Loss: 6.640443652868271\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 176, Train Loss: 43.10954795777798, Val Loss: 6.552501171827316\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 177, Train Loss: 43.02550746500492, Val Loss: 6.492526978254318\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 178, Train Loss: 43.17729426920414, Val Loss: 6.694539338350296\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 179, Train Loss: 42.78963090479374, Val Loss: 6.457584500312805\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 180, Train Loss: 43.014991953969, Val Loss: 6.6481854021549225\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 181, Train Loss: 42.793400317430496, Val Loss: 6.503889262676239\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 182, Train Loss: 42.92888195812702, Val Loss: 6.657409518957138\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 183, Train Loss: 42.62011171877384, Val Loss: 6.555097550153732\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 184, Train Loss: 42.60626791417599, Val Loss: 6.478561609983444\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 185, Train Loss: 42.92198130488396, Val Loss: 6.667034178972244\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 186, Train Loss: 42.608126401901245, Val Loss: 6.64358988404274\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 187, Train Loss: 42.35895013809204, Val Loss: 6.551503390073776\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 188, Train Loss: 42.447197169065475, Val Loss: 6.582028716802597\n",
      "Time: 0.01s, CPU: 11.60%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 189, Train Loss: 42.33935312926769, Val Loss: 6.481073498725891\n",
      "Time: 0.01s, CPU: 12.50%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 190, Train Loss: 42.52909895777702, Val Loss: 6.691422164440155\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 191, Train Loss: 42.09141108393669, Val Loss: 6.369289755821228\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 192, Train Loss: 42.50467249751091, Val Loss: 6.67032390832901\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 193, Train Loss: 42.2206806987524, Val Loss: 6.707576215267181\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 194, Train Loss: 41.913817062973976, Val Loss: 6.441877484321594\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 195, Train Loss: 42.149057388305664, Val Loss: 6.572410315275192\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 196, Train Loss: 42.00053611397743, Val Loss: 6.649409383535385\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 197, Train Loss: 41.68383431434631, Val Loss: 6.375033259391785\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 198, Train Loss: 42.056584388017654, Val Loss: 6.436626613140106\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 199, Train Loss: 42.18808262050152, Val Loss: 6.631776541471481\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 200, Train Loss: 41.83861255645752, Val Loss: 6.537672579288483\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Epoch 201, Train Loss: 41.70731063187122, Val Loss: 6.411247104406357\n",
      "Time: 0.01s, CPU: 0.00%, Memory: 0.15GB, GPU: 0.00GB, GPU Util: 0.00%\n",
      "Early stopping at epoch 202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/gcn_2l_mutag.pth\n",
      "Average Time per Epoch: 0.01s\n",
      "Average CPU Usage: 0.88%\n",
      "Average Memory Usage: 0.14GB\n",
      "Average GPU Usage: 0.00GB\n",
      "Average GPU Utilization: 0.00%\n",
      "\n",
      "Total Training Time: 1.83s\n",
      "Max CPU Usage: 52.25%\n",
      "Max Memory Usage: 0.15GB\n",
      "Max GPU Usage: 0.00GB\n",
      "Max GPU Utilization: 0.00%\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "gcn2_mutag = GCN2Layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes)\n",
    "print(gcn2_mutag)\n",
    "print(f\"Total number of trainable parameters: {(gcn2_mutag.count_parameters())*2}\\n\")\n",
    "single_train(gcn2_mutag, mutag_train_loader, mutag_val_loader, lr=0.01, num_epochs=500, step_size=500, save_path='models/gcn_2l_mutag.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211\n",
      "Average Sensitivity (Recall): 0.8846\n",
      "Average Specificity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "single_test(gcn2_mutag, mutag_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCESN 1-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_1layer(\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 240\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 44.21705, Total Training Time: 0.71s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 2/50 -> Loss: 113.18212, Total Training Time: 0.27s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9231, Specificity: 0.5000\n",
      "\n",
      "Run 3/50 -> Loss: 56.12541, Total Training Time: 3.13s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 4/50 -> Loss: 55.84995, Total Training Time: 0.68s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9615, Specificity: 0.4167\n",
      "\n",
      "Run 5/50 -> Loss: 52.59511, Total Training Time: 2.96s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 6/50 -> Loss: 51.19816, Total Training Time: 3.49s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 7/50 -> Loss: 53.39885, Total Training Time: 0.16s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9615, Specificity: 0.4167\n",
      "\n",
      "Run 8/50 -> Loss: 51.99555, Total Training Time: 0.23s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 9/50 -> Loss: 50.73687, Total Training Time: 3.24s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 10/50 -> Loss: 53.43353, Total Training Time: 0.20s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 11/50 -> Loss: 53.78155, Total Training Time: 0.61s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9615, Specificity: 0.5000\n",
      "\n",
      "Run 12/50 -> Loss: 47.73735, Total Training Time: 0.60s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 13/50 -> Loss: 50.37670, Total Training Time: 3.29s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 14/50 -> Loss: 48.75328, Total Training Time: 0.24s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 15/50 -> Loss: 44.00976, Total Training Time: 3.55s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 16/50 -> Loss: 55.38593, Total Training Time: 2.89s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 17/50 -> Loss: 48.96019, Total Training Time: 2.97s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 18/50 -> Loss: 51.28547, Total Training Time: 0.30s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 19/50 -> Loss: 55.12257, Total Training Time: 2.87s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 20/50 -> Loss: 50.25170, Total Training Time: 3.40s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 21/50 -> Loss: 46.59572, Total Training Time: 2.88s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 22/50 -> Loss: 51.09080, Total Training Time: 0.66s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 23/50 -> Loss: 46.77257, Total Training Time: 0.54s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 24/50 -> Loss: 47.26396, Total Training Time: 3.15s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 25/50 -> Loss: 50.05290, Total Training Time: 2.90s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 26/50 -> Loss: 46.70610, Total Training Time: 3.39s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9615, Specificity: 0.6667\n",
      "\n",
      "Run 27/50 -> Loss: 48.70380, Total Training Time: 0.61s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.8846, Specificity: 0.6667\n",
      "\n",
      "Run 28/50 -> Loss: 51.31512, Total Training Time: 0.62s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 29/50 -> Loss: 48.08483, Total Training Time: 3.41s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 30/50 -> Loss: 75.00180, Total Training Time: 0.26s\n",
      "  Accuracy: 0.7632, Sensitivity: 0.9615, Specificity: 0.3333\n",
      "\n",
      "Run 31/50 -> Loss: 51.23151, Total Training Time: 0.62s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 32/50 -> Loss: 45.24900, Total Training Time: 0.61s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 33/50 -> Loss: 49.10937, Total Training Time: 0.60s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 34/50 -> Loss: 50.36740, Total Training Time: 0.25s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 35/50 -> Loss: 52.44746, Total Training Time: 0.16s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 36/50 -> Loss: 48.39770, Total Training Time: 0.54s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 37/50 -> Loss: 48.12773, Total Training Time: 3.73s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 38/50 -> Loss: 52.38744, Total Training Time: 2.89s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 39/50 -> Loss: 51.00644, Total Training Time: 0.37s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 40/50 -> Loss: 50.98688, Total Training Time: 3.23s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 41/50 -> Loss: 52.88801, Total Training Time: 2.37s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 42/50 -> Loss: 53.13492, Total Training Time: 0.21s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9231, Specificity: 0.5833\n",
      "\n",
      "Run 43/50 -> Loss: 50.20591, Total Training Time: 3.47s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 44/50 -> Loss: 80.98723, Total Training Time: 3.75s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 45/50 -> Loss: 55.90396, Total Training Time: 3.48s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8462, Specificity: 0.8333\n",
      "\n",
      "Run 46/50 -> Loss: 56.55444, Total Training Time: 0.67s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 47/50 -> Loss: 49.52119, Total Training Time: 3.43s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 48/50 -> Loss: 47.00383, Total Training Time: 0.17s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 49/50 -> Loss: 54.25631, Total Training Time: 0.30s\n",
      "  Accuracy: 0.7368, Sensitivity: 0.9231, Specificity: 0.3333\n",
      "\n",
      "Run 50/50 -> Loss: 50.92243, Total Training Time: 0.63s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.8626 ± 0.05, Avg Sensitivity: 0.9246 ± 0.03, Avg Specificity: 0.7283 ± 0.15\n",
      "  Max Accuracy: 0.9474, Max Sensitivity: 0.9615, Max Specificity: 0.9167\n",
      "  Avg Num Epoch:150.74, Avg Training Time: 1.71s, Avg Epoch Time: 0.01s\n",
      "  Avg CPU Usage: 0.91%, Avg GPU Usage: 0.00%, Avg Memory Usage: 0.09GB\n",
      "  Avg Max CPU Usage: 32.37%, Avg Max GPU Usage: 0.00GB, Avg Max Memory Usage: 0.09GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag = GCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=6)\n",
    "print(gcesn_mutag)\n",
    "print(f\"Total number of trainable parameters: {gcesn_mutag.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_mutag, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCESN 2-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCESN_2layer(\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=28, bias=True)\n",
      "  )\n",
      "  (ridge_layer_2): RidgeLayer(\n",
      "    (linear): Linear(in_features=28, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 856\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 42.21466, Total Training Time: 3.55s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 2/50 -> Loss: 44.74241, Total Training Time: 3.89s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 3/50 -> Loss: 35.83250, Total Training Time: 3.37s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 4/50 -> Loss: 39.63451, Total Training Time: 2.89s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 5/50 -> Loss: 42.56508, Total Training Time: 0.64s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 6/50 -> Loss: 48.36156, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 7/50 -> Loss: 38.95351, Total Training Time: 3.41s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 8/50 -> Loss: 42.52852, Total Training Time: 0.67s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 9/50 -> Loss: 44.07041, Total Training Time: 0.65s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 10/50 -> Loss: 39.20834, Total Training Time: 1.31s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 11/50 -> Loss: 44.35330, Total Training Time: 1.56s\n",
      "  Accuracy: 0.9737, Sensitivity: 0.9615, Specificity: 1.0000\n",
      "\n",
      "Run 12/50 -> Loss: 42.93986, Total Training Time: 3.35s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 13/50 -> Loss: 47.04458, Total Training Time: 3.67s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 14/50 -> Loss: 43.37051, Total Training Time: 3.26s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 15/50 -> Loss: 43.25307, Total Training Time: 0.92s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 16/50 -> Loss: 39.99408, Total Training Time: 2.97s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 17/50 -> Loss: 41.75925, Total Training Time: 3.53s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 18/50 -> Loss: 38.92400, Total Training Time: 1.19s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 19/50 -> Loss: 37.55547, Total Training Time: 1.38s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 20/50 -> Loss: 40.28004, Total Training Time: 0.67s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 21/50 -> Loss: 45.29652, Total Training Time: 0.65s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 22/50 -> Loss: 35.93979, Total Training Time: 3.52s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 23/50 -> Loss: 45.99084, Total Training Time: 0.50s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 24/50 -> Loss: 40.65270, Total Training Time: 3.58s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 25/50 -> Loss: 36.25129, Total Training Time: 2.87s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 26/50 -> Loss: 40.99401, Total Training Time: 1.21s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 27/50 -> Loss: 41.66623, Total Training Time: 1.16s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 28/50 -> Loss: 43.28981, Total Training Time: 0.87s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 29/50 -> Loss: 41.35800, Total Training Time: 2.86s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 30/50 -> Loss: 38.21036, Total Training Time: 3.33s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 31/50 -> Loss: 49.21655, Total Training Time: 2.89s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 32/50 -> Loss: 42.23558, Total Training Time: 0.64s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 33/50 -> Loss: 36.18392, Total Training Time: 2.82s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 34/50 -> Loss: 35.67856, Total Training Time: 3.32s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 35/50 -> Loss: 39.56556, Total Training Time: 1.15s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 36/50 -> Loss: 45.16608, Total Training Time: 0.64s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 37/50 -> Loss: 35.83336, Total Training Time: 3.34s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 38/50 -> Loss: 41.02365, Total Training Time: 2.79s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 39/50 -> Loss: 42.02609, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 40/50 -> Loss: 44.31101, Total Training Time: 3.36s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 41/50 -> Loss: 43.26171, Total Training Time: 1.15s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 42/50 -> Loss: 37.53979, Total Training Time: 3.00s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 43/50 -> Loss: 46.98266, Total Training Time: 3.71s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 44/50 -> Loss: 39.85322, Total Training Time: 0.53s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 45/50 -> Loss: 42.02378, Total Training Time: 0.61s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 46/50 -> Loss: 43.95970, Total Training Time: 2.31s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 47/50 -> Loss: 38.40361, Total Training Time: 0.68s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 48/50 -> Loss: 36.75767, Total Training Time: 3.37s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 49/50 -> Loss: 40.61892, Total Training Time: 2.96s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 50/50 -> Loss: 40.94913, Total Training Time: 3.33s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.9221 ± 0.02, Avg Sensitivity: 0.9138 ± 0.02, Avg Specificity: 0.9400 ± 0.06\n",
      "  Max Accuracy: 0.9737, Max Sensitivity: 0.9615, Max Specificity: 1.0000\n",
      "  Avg Num Epoch:189.20, Avg Training Time: 2.13s, Avg Epoch Time: 0.01s\n",
      "  Avg CPU Usage: 0.74%, Avg GPU Usage: 0.00%, Avg Memory Usage: 0.08GB\n",
      "  Avg Max CPU Usage: 34.26%, Avg Max GPU Usage: 0.00GB, Avg Max Memory Usage: 0.09GB\n"
     ]
    }
   ],
   "source": [
    "gcesn_mutag_2 = GCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, leaky_rate=0.9, num_iterations=1)\n",
    "print(gcesn_mutag_2)\n",
    "print(f\"Total number of trainable parameters: {gcesn_mutag_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(gcesn_mutag_2, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoupled GCESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_1layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (ridge_layer): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=14, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=14, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 352\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 48.30139, Total Training Time: 2.72s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 2/50 -> Loss: 53.02974, Total Training Time: 3.27s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.8846, Specificity: 0.5833\n",
      "\n",
      "Run 3/50 -> Loss: 40.13105, Total Training Time: 3.19s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9231, Specificity: 1.0000\n",
      "\n",
      "Run 4/50 -> Loss: 48.19694, Total Training Time: 3.17s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9615, Specificity: 0.7500\n",
      "\n",
      "Run 5/50 -> Loss: 67.84303, Total Training Time: 3.11s\n",
      "  Accuracy: 0.8158, Sensitivity: 0.9615, Specificity: 0.5000\n",
      "\n",
      "Run 6/50 -> Loss: 58.01248, Total Training Time: 0.62s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 7/50 -> Loss: 64.97096, Total Training Time: 2.62s\n",
      "  Accuracy: 0.7895, Sensitivity: 0.9615, Specificity: 0.4167\n",
      "\n",
      "Run 8/50 -> Loss: 56.06434, Total Training Time: 3.28s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.9231, Specificity: 0.7500\n",
      "\n",
      "Run 9/50 -> Loss: 46.24080, Total Training Time: 0.67s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 10/50 -> Loss: 46.82730, Total Training Time: 2.78s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 11/50 -> Loss: 52.31297, Total Training Time: 2.92s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.9231, Specificity: 0.6667\n",
      "\n",
      "Run 12/50 -> Loss: 57.52630, Total Training Time: 0.63s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 13/50 -> Loss: 59.20630, Total Training Time: 0.09s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 14/50 -> Loss: 58.06232, Total Training Time: 3.13s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 15/50 -> Loss: 58.25504, Total Training Time: 0.09s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 16/50 -> Loss: 57.53079, Total Training Time: 0.09s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 17/50 -> Loss: 56.77295, Total Training Time: 0.09s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 18/50 -> Loss: 58.12421, Total Training Time: 0.12s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 19/50 -> Loss: 55.36217, Total Training Time: 0.63s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 20/50 -> Loss: 56.42555, Total Training Time: 0.13s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 21/50 -> Loss: 55.48325, Total Training Time: 0.19s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 22/50 -> Loss: 55.96997, Total Training Time: 0.14s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 23/50 -> Loss: 55.88103, Total Training Time: 0.12s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 24/50 -> Loss: 56.18141, Total Training Time: 0.13s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 25/50 -> Loss: 55.46476, Total Training Time: 0.17s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 26/50 -> Loss: 54.46276, Total Training Time: 0.10s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 27/50 -> Loss: 54.70001, Total Training Time: 0.62s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 28/50 -> Loss: 54.83219, Total Training Time: 0.13s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.8846, Specificity: 0.9167\n",
      "\n",
      "Run 29/50 -> Loss: 55.50431, Total Training Time: 0.15s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 30/50 -> Loss: 55.40457, Total Training Time: 0.11s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.8846, Specificity: 1.0000\n",
      "\n",
      "Run 31/50 -> Loss: 54.53482, Total Training Time: 0.12s\n",
      "  Accuracy: 0.8684, Sensitivity: 0.8846, Specificity: 0.8333\n",
      "\n",
      "Run 32/50 -> Loss: 54.27790, Total Training Time: 0.47s\n",
      "  Accuracy: 0.8421, Sensitivity: 0.8846, Specificity: 0.7500\n",
      "\n",
      "Run 33/50 -> Loss: 44.14409, Total Training Time: 2.71s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 34/50 -> Loss: 39.63937, Total Training Time: 3.02s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 35/50 -> Loss: 38.97331, Total Training Time: 2.69s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 36/50 -> Loss: 39.03297, Total Training Time: 0.14s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 37/50 -> Loss: 39.37638, Total Training Time: 0.13s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 38/50 -> Loss: 39.13171, Total Training Time: 0.58s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 39/50 -> Loss: 40.69399, Total Training Time: 2.79s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 40/50 -> Loss: 39.50219, Total Training Time: 3.33s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 41/50 -> Loss: 36.47172, Total Training Time: 0.65s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 42/50 -> Loss: 36.73307, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 43/50 -> Loss: 41.02602, Total Training Time: 2.19s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 44/50 -> Loss: 36.68304, Total Training Time: 2.43s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 45/50 -> Loss: 37.49330, Total Training Time: 0.21s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 46/50 -> Loss: 37.45613, Total Training Time: 0.16s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 47/50 -> Loss: 40.24377, Total Training Time: 2.17s\n",
      "  Accuracy: 0.8947, Sensitivity: 0.9231, Specificity: 0.8333\n",
      "\n",
      "Run 48/50 -> Loss: 36.69938, Total Training Time: 2.13s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 49/50 -> Loss: 38.31512, Total Training Time: 3.54s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 50/50 -> Loss: 36.48940, Total Training Time: 0.58s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.8974 ± 0.04, Avg Sensitivity: 0.9092 ± 0.03, Avg Specificity: 0.8717 ± 0.13\n",
      "  Max Accuracy: 0.9474, Max Sensitivity: 0.9615, Max Specificity: 1.0000\n",
      "  Avg Num Epoch:123.42, Avg Training Time: 1.31s, Avg Epoch Time: 1.31s\n",
      "  Avg CPU Usage: 1.04%, Avg GPU Usage: 0.00%, Avg Memory Usage: 0.09GB\n",
      "  Avg Max CPU Usage: 31.00%, Avg Max GPU Usage: 0.00GB, Avg Max Memory Usage: 0.09GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag = decoupledGCESN_1layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "print(decoupled_gcesn_mutag)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_mutag.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_mutag, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoupledGCESN_2layer(\n",
      "  (gcn1): GCN (7 -> 14)\n",
      "  (ridge_layer_1): RidgeLayer(\n",
      "    (linear): Linear(in_features=14, out_features=28, bias=True)\n",
      "  )\n",
      "  (gcn2): GCN (14 -> 28)\n",
      "  (fc): Linear(in_features=28, out_features=2, bias=True)\n",
      ")\n",
      "Total number of trainable parameters: 1010\n",
      "\n",
      "\n",
      "Run 1/50 -> Loss: 46.30991, Total Training Time: 3.92s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 2/50 -> Loss: 39.94903, Total Training Time: 3.66s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 3/50 -> Loss: 40.64591, Total Training Time: 0.25s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 4/50 -> Loss: 40.00577, Total Training Time: 0.27s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 5/50 -> Loss: 39.61331, Total Training Time: 0.21s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 6/50 -> Loss: 38.92535, Total Training Time: 0.28s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9231, Specificity: 0.9167\n",
      "\n",
      "Run 7/50 -> Loss: 38.85359, Total Training Time: 0.33s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 8/50 -> Loss: 38.45775, Total Training Time: 0.25s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 9/50 -> Loss: 38.22301, Total Training Time: 0.28s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 10/50 -> Loss: 38.03806, Total Training Time: 0.43s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 11/50 -> Loss: 37.69220, Total Training Time: 0.35s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 12/50 -> Loss: 37.42990, Total Training Time: 0.31s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 13/50 -> Loss: 37.28215, Total Training Time: 0.28s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 14/50 -> Loss: 36.92471, Total Training Time: 0.45s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 15/50 -> Loss: 36.84979, Total Training Time: 0.27s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 16/50 -> Loss: 36.91764, Total Training Time: 0.41s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 17/50 -> Loss: 36.73961, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 18/50 -> Loss: 36.58003, Total Training Time: 3.29s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 19/50 -> Loss: 36.65646, Total Training Time: 1.12s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 20/50 -> Loss: 36.34968, Total Training Time: 0.54s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 21/50 -> Loss: 36.22428, Total Training Time: 0.97s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 22/50 -> Loss: 36.35518, Total Training Time: 0.19s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 23/50 -> Loss: 36.40190, Total Training Time: 0.42s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 24/50 -> Loss: 36.31333, Total Training Time: 0.36s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 25/50 -> Loss: 36.14922, Total Training Time: 0.24s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 26/50 -> Loss: 36.12462, Total Training Time: 0.21s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 27/50 -> Loss: 35.98914, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 28/50 -> Loss: 35.66418, Total Training Time: 0.29s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 29/50 -> Loss: 35.91058, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 30/50 -> Loss: 35.68715, Total Training Time: 0.17s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 31/50 -> Loss: 34.26408, Total Training Time: 0.75s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 32/50 -> Loss: 34.87274, Total Training Time: 0.26s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 33/50 -> Loss: 34.91402, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 34/50 -> Loss: 35.06191, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 35/50 -> Loss: 34.84702, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 36/50 -> Loss: 35.01687, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 37/50 -> Loss: 34.82212, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 38/50 -> Loss: 35.10269, Total Training Time: 0.21s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 39/50 -> Loss: 34.58932, Total Training Time: 0.31s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 40/50 -> Loss: 34.51214, Total Training Time: 0.18s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 41/50 -> Loss: 34.31605, Total Training Time: 0.25s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 42/50 -> Loss: 34.27640, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 43/50 -> Loss: 34.23825, Total Training Time: 0.23s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 44/50 -> Loss: 33.73193, Total Training Time: 0.17s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 45/50 -> Loss: 34.25110, Total Training Time: 0.20s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 46/50 -> Loss: 33.53797, Total Training Time: 0.33s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 47/50 -> Loss: 33.58895, Total Training Time: 0.28s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 48/50 -> Loss: 33.61269, Total Training Time: 0.27s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "\n",
      "Run 49/50 -> Loss: 33.09370, Total Training Time: 0.58s\n",
      "  Accuracy: 0.9211, Sensitivity: 0.9615, Specificity: 0.8333\n",
      "\n",
      "Run 50/50 -> Loss: 32.58357, Total Training Time: 0.22s\n",
      "  Accuracy: 0.9474, Sensitivity: 0.9615, Specificity: 0.9167\n",
      "Overall Results:\n",
      "  Avg Accuracy: 0.9437 ± 0.01, Avg Sensitivity: 0.9569 ± 0.01, Avg Specificity: 0.9150 ± 0.01\n",
      "  Max Accuracy: 0.9474, Max Sensitivity: 0.9615, Max Specificity: 0.9167\n",
      "  Avg Num Epoch:28.34, Avg Training Time: 0.52s, Avg Epoch Time: 0.02s\n",
      "  Avg CPU Usage: 2.57%, Avg GPU Usage: 0.00%, Avg Memory Usage: 0.09GB\n",
      "  Avg Max CPU Usage: 21.62%, Avg Max GPU Usage: 0.00GB, Avg Max Memory Usage: 0.09GB\n"
     ]
    }
   ],
   "source": [
    "decoupled_gcesn_mutag_2 = decoupledGCESN_2layer(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=2)\n",
    "print(decoupled_gcesn_mutag_2)\n",
    "print(f\"Total number of trainable parameters: {decoupled_gcesn_mutag_2.count_parameters()}\\n\")\n",
    "\n",
    "multi_train_test(decoupled_gcesn_mutag_2, mutag_train_loader, mutag_val_loader, mutag_test_loader,\n",
    "                lr=0.001, num_epochs=500, patience=10, step_size=100, gamma=0.1, \n",
    "                num_runs=50, binary_classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableGCESN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, num_classes, leaky_rate=0.9, num_iterations=5, ridge_alpha=0.9):\n",
    "        super(TrainableGCESN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.leaky_rate = leaky_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.spectral_radius = 0.9\n",
    "        self.Win = nn.Parameter(torch.rand((self.in_features, self.hidden_features)) * 2 - 1)\n",
    "        self.W = nn.Parameter(torch.rand((self.hidden_features, self.hidden_features)) * 2 - 1)\n",
    "        self.adjust_spectral_radius()\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_features, num_classes)\n",
    "    \n",
    "    def adjust_spectral_radius(self):\n",
    "        eigenvalues, _ = torch.linalg.eig(self.W)\n",
    "        rhoW = max(abs(eigenvalues))\n",
    "        self.W.data *= self.spectral_radius / rhoW\n",
    "\n",
    "    def forward(self, x, adj, batch):\n",
    "        h = torch.mm(x, self.Win)\n",
    "        for _ in range(self.num_iterations):\n",
    "            h_new = F.relu(torch.mm(adj, torch.mm(h, self.W)))\n",
    "            h = (1 - self.leaky_rate) * h + self.leaky_rate * h_new\n",
    "        h = global_mean_pool(h, batch)\n",
    "        h = self.fc(h)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Custom optimizer hook to adjust spectral radius after each update\n",
    "class SpectralRadiusOptimizerHook:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self):\n",
    "        self.model.adjust_spectral_radius()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 324\n",
      "Epoch 1, Loss: 0.47106048464775085\n",
      "Epoch 2, Loss: 0.4943133294582367\n",
      "Epoch 3, Loss: 0.43529012799263\n",
      "Epoch 4, Loss: 0.4312138557434082\n",
      "Epoch 5, Loss: 0.6660427451133728\n",
      "Epoch 6, Loss: 0.6354702711105347\n",
      "Epoch 7, Loss: 0.21317103505134583\n",
      "Epoch 8, Loss: 0.423541784286499\n",
      "Epoch 9, Loss: 0.2784121334552765\n",
      "Epoch 10, Loss: 0.2851869463920593\n",
      "Epoch 11, Loss: 0.4774807393550873\n",
      "Epoch 12, Loss: 0.51896733045578\n",
      "Epoch 13, Loss: 0.5635321736335754\n",
      "Epoch 14, Loss: 0.3366321623325348\n",
      "Epoch 15, Loss: 0.7568030953407288\n",
      "Epoch 16, Loss: 0.33130908012390137\n",
      "Epoch 17, Loss: 0.3954927921295166\n",
      "Epoch 18, Loss: 0.3759712874889374\n",
      "Epoch 19, Loss: 0.414899617433548\n",
      "Epoch 20, Loss: 0.44557079672813416\n",
      "Epoch 21, Loss: 0.8758074045181274\n",
      "Epoch 22, Loss: 0.5403642058372498\n",
      "Epoch 23, Loss: 0.2616855204105377\n",
      "Epoch 24, Loss: 0.44877901673316956\n",
      "Epoch 25, Loss: 0.5747477412223816\n",
      "Epoch 26, Loss: 0.44454026222229004\n",
      "Epoch 27, Loss: 0.4778769612312317\n",
      "Epoch 28, Loss: 0.5526243448257446\n",
      "Epoch 29, Loss: 0.527807891368866\n",
      "Epoch 30, Loss: 0.4151429831981659\n",
      "Epoch 31, Loss: 0.2835470139980316\n",
      "Epoch 32, Loss: 0.41895586252212524\n",
      "Epoch 33, Loss: 0.5667827129364014\n",
      "Epoch 34, Loss: 0.44770747423171997\n",
      "Epoch 35, Loss: 0.3679157793521881\n",
      "Epoch 36, Loss: 0.47245025634765625\n",
      "Epoch 37, Loss: 0.5543714165687561\n",
      "Epoch 38, Loss: 0.7180190682411194\n",
      "Epoch 39, Loss: 0.582395613193512\n",
      "Epoch 40, Loss: 0.299607515335083\n",
      "Epoch 41, Loss: 0.6520316004753113\n",
      "Epoch 42, Loss: 0.6448686718940735\n",
      "Epoch 43, Loss: 0.7224906086921692\n",
      "Epoch 44, Loss: 0.20954537391662598\n",
      "Epoch 45, Loss: 0.2581987977027893\n",
      "Epoch 46, Loss: 0.6186656355857849\n",
      "Epoch 47, Loss: 0.5536263585090637\n",
      "Epoch 48, Loss: 0.35683131217956543\n",
      "Epoch 49, Loss: 0.22771945595741272\n",
      "Epoch 50, Loss: 0.38250353932380676\n",
      "Epoch 51, Loss: 0.472661554813385\n",
      "Epoch 52, Loss: 0.2961040437221527\n",
      "Epoch 53, Loss: 0.3206367790699005\n",
      "Epoch 54, Loss: 0.9097366333007812\n",
      "Epoch 55, Loss: 0.5992730259895325\n",
      "Epoch 56, Loss: 0.3960731029510498\n",
      "Epoch 57, Loss: 0.29329681396484375\n",
      "Epoch 58, Loss: 0.4367664158344269\n",
      "Epoch 59, Loss: 0.4312770664691925\n",
      "Epoch 60, Loss: 0.2568778693675995\n",
      "Epoch 61, Loss: 0.43792954087257385\n",
      "Epoch 62, Loss: 0.1336582601070404\n",
      "Epoch 63, Loss: 0.8578090667724609\n",
      "Epoch 64, Loss: 0.45917510986328125\n",
      "Epoch 65, Loss: 0.5135233998298645\n",
      "Epoch 66, Loss: 0.3839487135410309\n",
      "Epoch 67, Loss: 0.584281325340271\n",
      "Epoch 68, Loss: 0.3836314380168915\n",
      "Epoch 69, Loss: 0.3078940510749817\n",
      "Epoch 70, Loss: 0.48087891936302185\n",
      "Epoch 71, Loss: 0.35308220982551575\n",
      "Epoch 72, Loss: 0.4129669964313507\n",
      "Epoch 73, Loss: 0.5344143509864807\n",
      "Epoch 74, Loss: 0.7004008293151855\n",
      "Epoch 75, Loss: 0.3477460443973541\n",
      "Epoch 76, Loss: 0.6236547827720642\n",
      "Epoch 77, Loss: 0.5903012156486511\n",
      "Epoch 78, Loss: 0.35187163949012756\n",
      "Epoch 79, Loss: 0.43107739090919495\n",
      "Epoch 80, Loss: 0.32359758019447327\n",
      "Epoch 81, Loss: 0.6781545877456665\n",
      "Epoch 82, Loss: 0.41161349415779114\n",
      "Epoch 83, Loss: 0.2529607117176056\n",
      "Epoch 84, Loss: 0.6072749495506287\n",
      "Epoch 85, Loss: 0.3443160653114319\n",
      "Epoch 86, Loss: 0.15342767536640167\n",
      "Epoch 87, Loss: 0.3409441411495209\n",
      "Epoch 88, Loss: 0.3424381911754608\n",
      "Epoch 89, Loss: 0.2886173129081726\n",
      "Epoch 90, Loss: 0.4430535137653351\n",
      "Epoch 91, Loss: 0.3725956082344055\n",
      "Epoch 92, Loss: 0.06413258612155914\n",
      "Epoch 93, Loss: 0.6534980535507202\n",
      "Epoch 94, Loss: 0.36209821701049805\n",
      "Epoch 95, Loss: 0.48502781987190247\n",
      "Epoch 96, Loss: 0.22018401324748993\n",
      "Epoch 97, Loss: 0.7025379538536072\n",
      "Epoch 98, Loss: 0.320448637008667\n",
      "Epoch 99, Loss: 0.28693652153015137\n",
      "Epoch 100, Loss: 0.42359083890914917\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = TrainableGCESN(mutag_num_features, 2*mutag_num_features, mutag_num_classes, num_iterations=6)\n",
    "print(f'Total trainable parameters: {model.count_parameters()}')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "spectral_hook = SpectralRadiusOptimizerHook(model)\n",
    "for epoch in range(100):\n",
    "    for data in mutag_train_loader:\n",
    "        x, edge_index, batch, y = data.x.to(device), data.edge_index.to(device), data.batch.to(device), data.y.to(device)\n",
    "        adj = utils.to_dense_adj(edge_index).squeeze(0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, adj, batch)\n",
    "        \n",
    "        # Ensure target tensor `y` matches the batch size of `output`\n",
    "        loss = F.nll_loss(output, y)\n",
    "        # loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        spectral_hook()\n",
    "        \n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8684\n",
      "Average Sensitivity (Recall): 0.9615\n",
      "Average Specificity: 0.6667\n"
     ]
    }
   ],
   "source": [
    "single_test(model, mutag_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
