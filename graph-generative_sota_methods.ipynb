{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-geometric\n",
    "%pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import psutil\n",
    "import pynvml\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'using {device}')\n",
    "\n",
    "def set_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = '/Users/gitaayusalsabila/Documents/0thesis/code/sandbox/dataset/'\n",
    "dataset_path = '/notebooks/dataset/'\n",
    "\n",
    "\n",
    "def data_cleansing(dataset):\n",
    "    # Replace negative values with 0\n",
    "    dataset[dataset < 0] = 0\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    dataset = np.nan_to_num(dataset, nan=0)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def check_and_drop_invalid_graphs(graph_dataset):\n",
    "    num_graphs, num_timepoints, num_nodes, _ = graph_dataset.shape\n",
    "    num_dimensions = 1\n",
    "    \n",
    "    valid_graphs = []\n",
    "\n",
    "    for i in range(num_graphs):\n",
    "        is_valid = True\n",
    "        for t in range(num_timepoints):\n",
    "            adj_matrix = graph_dataset[i, t, :, :]\n",
    "            num_edges = np.sum(adj_matrix > 0)\n",
    "            if num_edges == 0:\n",
    "                is_valid = False\n",
    "                break\n",
    "        \n",
    "        if is_valid:\n",
    "            valid_graphs.append(i)\n",
    "    \n",
    "    cleaned_dataset = graph_dataset[valid_graphs, :, :, :]\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "def split_data(adj_data, features_data, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, random_seed=None):\n",
    "    assert adj_data.shape[0] == features_data.shape[0], \"Adjacency and features data must have the same number of samples\"\n",
    "    assert np.isclose(train_ratio + val_ratio + test_ratio, 1.0), \"The sum of train, val and test ratios must be 1\"\n",
    "\n",
    "    num_samples = adj_data.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_end = int(train_ratio * num_samples)\n",
    "    val_end = train_end + int(val_ratio * num_samples)\n",
    "\n",
    "    train_indices = indices[:train_end]\n",
    "    val_indices = indices[train_end:val_end]\n",
    "    test_indices = indices[val_end:]\n",
    "\n",
    "    adj_train = torch.tensor(adj_data[train_indices], dtype=torch.float32)\n",
    "    adj_val = torch.tensor(adj_data[val_indices], dtype=torch.float32)\n",
    "    adj_test = torch.tensor(adj_data[test_indices], dtype=torch.float32)\n",
    "\n",
    "    features_train = torch.tensor(features_data[train_indices], dtype=torch.float32)\n",
    "    features_val = torch.tensor(features_data[val_indices], dtype=torch.float32)\n",
    "    features_test = torch.tensor(features_data[test_indices], dtype=torch.float32)\n",
    "\n",
    "    return adj_train, adj_val, adj_test, features_train, features_val, features_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Dataset: Number of Samples= 100,  Number of Times= 3, Number of Nodes= 35, Number of Features= 8\n",
      "EMCI-AD Dataset: Number of Samples= 67,  Number of Times= 2, Number of Nodes= 35, Number of Features= 8\n",
      "SLIM160 Dataset: Number of Samples= 109,  Number of Times= 3, Number of Nodes = 160, Number of Features = 8\n"
     ]
    }
   ],
   "source": [
    "## Simulated Dataset\n",
    "simulated_adj = np.load(dataset_path + 'simulated_adj.npy')\n",
    "simulated_adj = simulated_adj[:, :, :, :,0] #only take the domain 1\n",
    "simulated_features = np.load(dataset_path + 'simulated_laplacian_features.npy')\n",
    "simulated_features = simulated_features[:, :, :, :,0]\n",
    "simulated_num_samples, simulated_num_time, simulated_num_nodes, simulated_num_features = simulated_features.shape \n",
    "print(f'Simulated Dataset: Number of Samples= {simulated_num_samples},  Number of Times= {simulated_num_time}, Number of Nodes= {simulated_num_nodes}, Number of Features= {simulated_num_features}')\n",
    "simulated_adj_train, simulated_adj_val, simulated_adj_test, simulated_features_train, simulated_features_val, simulated_features_test = split_data(simulated_adj, simulated_features)\n",
    "\n",
    "## OASIS Dataset\n",
    "oasis = np.load(dataset_path + 'oasis_adj.npy')\n",
    "oasis_cleaned = data_cleansing(oasis)\n",
    "oasis_adj = check_and_drop_invalid_graphs(oasis_cleaned)\n",
    "oasis_features = np.load(dataset_path + 'oasis_laplacian_features.npy') \n",
    "oasis_num_samples, oasis_num_time, oasis_num_nodes, oasis_num_features = oasis_features.shape \n",
    "print(f'OASIS Dataset: Number of Samples= {oasis_num_samples},  Number of Times= {oasis_num_time}, Number of Nodes= {oasis_num_nodes}, Number of Features= {oasis_num_features}')\n",
    "oasis_adj_train, oasis_adj_val, oasis_adj_test, oasis_features_train, oasis_features_val, oasis_features_test = split_data(oasis_adj, oasis_features)\n",
    "\n",
    "## EMCI-AD Dataset\n",
    "emci = np.load(dataset_path + 'emci-ad_adj.npy')\n",
    "emci_cleaned = data_cleansing(emci)\n",
    "emci_adj = check_and_drop_invalid_graphs(emci_cleaned)\n",
    "emci_features = np.load(dataset_path + 'emci-ad_laplacian_features.npy') \n",
    "emci_num_samples, emci_num_time, emci_num_nodes, emci_num_features = emci_features.shape \n",
    "print(f'EMCI-AD Dataset: Number of Samples= {emci_num_samples},  Number of Times= {emci_num_time}, Number of Nodes= {emci_num_nodes}, Number of Features= {emci_num_features}')\n",
    "emci_adj_train, emci_adj_val, emci_adj_test, emci_features_train, emci_features_val, emci_features_test = split_data(emci_adj, emci_features)\n",
    "\n",
    "## SLIM160 Dataset\n",
    "slim160 = np.load(dataset_path + 'slim160_adj.npy')\n",
    "slim160_cleaned = data_cleansing(slim160)\n",
    "slim160_adj = check_and_drop_invalid_graphs(slim160_cleaned)\n",
    "slim160_features = np.load(dataset_path + 'slim160_laplacian_features_8.npy') \n",
    "slim160_num_samples, slim160_num_time, slim160_num_nodes, slim160_num_features = slim160_features.shape \n",
    "print(f'SLIM160 Dataset: Number of Samples= {slim160_num_samples},  Number of Times= {slim160_num_time}, Number of Nodes = {slim160_num_nodes}, Number of Features = {slim160_num_features}')\n",
    "slim160_adj_train, slim160_adj_val, slim160_adj_test, slim160_features_train, slim160_features_val, slim160_features_test = split_data(slim160_adj, slim160_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucledian_distance(x, target_size):\n",
    "  repeated_out = x.repeat(target_size,1,1)\n",
    "  repeated_t = torch.transpose(repeated_out, 0, 1)\n",
    "  diff = torch.abs(repeated_out - repeated_t)\n",
    "  return torch.sum(diff, 2)\n",
    "\n",
    "def frobenious_distance(test_sample,predicted):\n",
    "  diff = torch.abs(test_sample - predicted)\n",
    "  dif = diff*diff\n",
    "  sum_of_all = diff.sum()\n",
    "  d = torch.sqrt(sum_of_all)\n",
    "  return d\n",
    "\n",
    "## Model\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_state):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.weight = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.weight_h = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.tanh = Tanh()\n",
    "        self.hidden_state = hidden_state\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h = self.hidden_state\n",
    "        y = self.tanh(self.weight(x.to(device)) + self.weight_h(h))\n",
    "        self.hidden_state = y.detach()\n",
    "        return y\n",
    "\n",
    "class RBGM(nn.Module):\n",
    "    def __init__(self, conv_size, hidden_state_size):\n",
    "        super(RBGM, self).__init__()\n",
    "        shape = torch.Size((hidden_state_size, hidden_state_size))\n",
    "        hidden_state = torch.rand(shape, device=device)\n",
    "        self.conv_size = conv_size\n",
    "        self.rnn = nn.Sequential(RNNCell(1,hidden_state_size, hidden_state), ReLU())\n",
    "        self.gnn_conv = NNConv(self.conv_size, self.conv_size, self.rnn, aggr='mean', root_weight=True, bias = True)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr, _, _ = create_edge_index_attribute(data)\n",
    "        x1 = F.relu(self.gnn_conv(data, edge_index.to(device), edge_attr.to(device)))\n",
    "        x1 = eucledian_distance(x1, self.conv_size)\n",
    "        return x1\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "def train_rbgm(model_1, model_2, train_adj, num_epochs=200, lr=0.0001, save_path='models/RBGM/rgbm_model', tp_c = 10):\n",
    "    # Loss Function\n",
    "    mael = torch.nn.L1Loss().to(device)\n",
    "    tp = torch.nn.MSELoss().to(device)\n",
    "\n",
    "    optimizer_1 = torch.optim.Adam(model_1.parameters(), lr = lr)\n",
    "    optimizer_2 = torch.optim.Adam(model_2.parameters(), lr = lr)\n",
    "\n",
    "    training_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss_1, total_loss_2 = 0.0, 0.0 \n",
    "        tp_loss_1, tp_loss_2, gen_loss_1, gen_loss_2 = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_top_loss = 0\n",
    "        \n",
    "        set_seed(42)\n",
    "        model_1.train()\n",
    "        model_2.train()\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for i in tqdm(range(train_adj.size(0)), desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        # for i, data in enumerate(h_data_train_loader):\n",
    "            data = train_adj\n",
    "            \n",
    "            data_t0 = data[i, 0]\n",
    "            data_t1 = data[i, 1]\n",
    "            data_t2 = data[i, 2]\n",
    "            \n",
    "            #Time Point 1\n",
    "            optimizer_1.zero_grad()\n",
    "            out_1 = model_1(data_t0)\n",
    "            \n",
    "            tpl_1 = tp(out_1.sum(dim=-1), data_t1.sum(dim=-1))\n",
    "            tp_loss_1 += tpl_1.item()\n",
    "            genl_1 = mael(out_1, data_t1)\n",
    "            gen_loss_1 += genl_1.item()\n",
    "            \n",
    "            loss_1 = genl_1 + (tp_c * tpl_1)\n",
    "            total_loss_1 += loss_1.item()\n",
    "            loss_1.backward()\n",
    "            optimizer_1.step()\n",
    "            \n",
    "            #Time Point 2\n",
    "            optimizer_2.zero_grad()\n",
    "            out_2 = model_2(data_t1)\n",
    "            \n",
    "            tpl_2 = tp(out_2.sum(dim=-1), data_t2.sum(dim=-1))\n",
    "            tp_loss_2 += tpl_2.item()\n",
    "            \n",
    "            genl_2 = mael(out_2, data_t2)\n",
    "            gen_loss_2 += genl_2.item()\n",
    "            \n",
    "            loss_2 = genl_2 + tp_c * tpl_2\n",
    "            total_loss_2 += loss_2.item()\n",
    "            loss_2.backward()\n",
    "            optimizer_2.step()\n",
    "\n",
    "            #All Training Loss\n",
    "            epoch_loss      = total_loss_1 + total_loss_2\n",
    "            epoch_gen_loss  = gen_loss_1 + gen_loss_2\n",
    "            epoch_tp_loss   = tp_loss_1 + tp_loss_2    \n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=None) / 100 * psutil.virtual_memory().total / (1024**3))  # CPU usage in GB\n",
    "        memory_usage.append(psutil.virtual_memory().used / (1024**3))  # Memory usage in GB\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            gpu_usage.append(torch.cuda.memory_allocated(device) / (1024**3))  # GPU usage in GB\n",
    "        else:\n",
    "            gpu_usage.append(0)\n",
    "\n",
    "        epoch_loss /= train_adj.size(0)\n",
    "        epoch_gen_loss /= train_adj.size(0)\n",
    "        epoch_top_loss /= train_adj.size(0)\n",
    "        training_loss.append(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}, Generative Loss: {epoch_gen_loss}, Topological Loss: {epoch_tp_loss}')\n",
    "        print(f'Time: {epoch_time[-1]:.2f}s, CPU: {cpu_usage[-1]:.2f}GB, Memorzy: {memory_usage[-1]:.2f}GB, GPU: {gpu_usage[-1]:.2f}GB\\n')\n",
    "        \n",
    "\n",
    "    # Plot the training loss\n",
    "    plt.plot(training_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    save_path_1 = save_path + '_model_1.pth'\n",
    "    save_path_2 = save_path + '_model_2.pth'\n",
    "    torch.save(model_1.state_dict(), save_path_1)\n",
    "    torch.save(model_2.state_dict(), save_path_2)\n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "    print(f'Average Time per Epoch: {np.mean(epoch_time):.2f}s')\n",
    "    print(f'Average CPU Usage: {np.mean(cpu_usage):.2f}GB')\n",
    "    print(f'Average Memory Usage: {np.mean(memory_usage):.2f}GB')\n",
    "    print(f'Average GPU Usage: {np.mean(gpu_usage):.2f}GB')\n",
    "\n",
    "    print(f'\\nTotal Training Time: {np.sum(epoch_time):.2f}s')\n",
    "    print(f'Max CPU Usage: {np.max(cpu_usage):.2f}GB')\n",
    "    print(f'Max Memory Usage: {np.max(memory_usage):.2f}GB')\n",
    "    print(f'Max GPU Usage: {np.max(gpu_usage):.2f}GB')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvoGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoGenerator(nn.Module):\n",
    "    def __init__(self, conv_size, hidden_size):\n",
    "        super(EvoGenerator, self).__init__()\n",
    "        self.conv_size = conv_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        lin = Sequential(Linear(1, self.hidden_size), ReLU())\n",
    "        self.conv1 = NNConv(self.conv_size, self.conv_size, lin, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(self.conv_size, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        lin = Sequential(Linear(1, self.conv_size), ReLU())\n",
    "        self.conv2 = NNConv(self.conv_size, 1, lin, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(1, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        lin = Sequential(Linear(1, self.conv_size), ReLU())\n",
    "        self.conv3 = NNConv(1, self.conv_size, lin, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv33 = BatchNorm(self.conv_size, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x1 = torch.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "        \n",
    "        x1 = (x1 + x1.T) / 2.0\n",
    "        x1.fill_diagonal_(fill_value=0)\n",
    "        x2 = torch.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\n",
    "        x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "        x3 = torch.cat([torch.sigmoid(self.conv33(self.conv3(x2, edge_index, edge_attr))), x1], dim=1)\n",
    "        x4 = x3[:, 0:self.conv_size]\n",
    "        x5 = x3[:, self.conv_size:self.conv_size*2]\n",
    "\n",
    "        x6 = (x4 + x5) / 2\n",
    "        x6 = (x6 + x6.T) / 2.0\n",
    "        x6.fill_diagonal_(fill_value=0)\n",
    "        return x6\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class EvoDiscriminator(nn.Module):\n",
    "    def __init__(self, conv_size, hidden_size):\n",
    "        super(EvoDiscriminator, self).__init__()\n",
    "        self.conv_size = conv_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        lin = Sequential(Linear(2, self.hidden_size), ReLU())\n",
    "        self.conv1 = NNConv(self.conv_size, self.conv_size, lin, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(self.conv_size, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        lin = Sequential(Linear(2, self.conv_size), ReLU())\n",
    "        self.conv2 = NNConv(self.conv_size, 1, lin, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(1, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "    def forward(self, data, data_to_translate):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        edge_attr_data_to_translate = data_to_translate.edge_attr\n",
    "\n",
    "        edge_attr_data_to_translate_reshaped = edge_attr_data_to_translate.view(self.hidden_size, 1)\n",
    "\n",
    "        gen_input = torch.cat((edge_attr, edge_attr_data_to_translate_reshaped), -1)\n",
    "        x = F.relu(self.conv11(self.conv1(x, edge_index, gen_input)))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv22(self.conv2(x, edge_index, gen_input)))\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_evograph(generator_1, discriminator_1, generator_2, discriminator_2, train_adj, \n",
    "                    num_epochs=500, lr_g=0.01, lr_d=0.0002, save_path='models/EvoGraph/evograph_model', \n",
    "                    tp_c=0.00, g_c=2.0, i_c=2.0,kl_c=0.001):\n",
    "    \n",
    "    adversarial_loss= torch.nn.BCELoss().to(device)\n",
    "    identity_loss   = torch.nn.L1Loss().to(device)  # Will be used in training\n",
    "    msel            = torch.nn.MSELoss().to(device)\n",
    "    mael            = torch.nn.L1Loss().to(device)  # Not to be used in training (Measure generator success)\n",
    "    tp              = torch.nn.MSELoss().to(device) # Used for node strength\n",
    "\n",
    "    i_coeff = 2.0\n",
    "    g_coeff = 2.0\n",
    "    kl_coeff = 0.001\n",
    "    tp_coeff = 0.0\n",
    "\n",
    "    num_nodes = train_adj.shape[2]\n",
    "    optimizer_G1 = torch.optim.AdamW(generator_1.parameters(), lr=lr_g, betas=(0.5, 0.999), weight_decay=0.0)\n",
    "    optimizer_D1 = torch.optim.AdamW(discriminator_1.parameters(), lr=lr_d, betas=(0.5, 0.999), weight_decay=0.0)\n",
    "    optimizer_G2 = torch.optim.AdamW(generator_2.parameters(), lr=lr_g, betas=(0.5, 0.999), weight_decay=0.0)\n",
    "    optimizer_D2 = torch.optim.AdamW(discriminator_2.parameters(), lr=lr_d, betas=(0.5, 0.999), weight_decay=0.0)\n",
    "\n",
    "    total_step = train_adj.shape[0]\n",
    "    data_size = total_step\n",
    "    \n",
    "    real_label = torch.ones(num_nodes, 1).to(device)\n",
    "    fake_label = torch.zeros(num_nodes, 1).to(device)\n",
    "\n",
    "    real_losses1, fake_losses1, mse_losses1, mae_losses1 = list(), list(), list(), list()\n",
    "    real_losses2, fake_losses2, mse_losses2, mae_losses2 = list(), list(), list(), list()\n",
    "\n",
    "    k1_losses, k2_losses = list(), list()\n",
    "    tp_losses_1_tr,  tp_losses_2_tr  = list(), list()\n",
    "    gan_losses_1_tr, gan_losses_2_tr = list(), list()\n",
    "\n",
    "    training_loss = []\n",
    "    epoch_time = []\n",
    "    cpu_usage = []\n",
    "    memory_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    d_loss = []\n",
    "    g_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Reporting\n",
    "        r1, f1, d1, g1, mse_l1, mae_l1 = 0, 0, 0, 0, 0, 0\n",
    "        r2, f2, d2, g2, mse_l2, mae_l2 = 0, 0, 0, 0, 0, 0\n",
    "        k1_train, k2_train  = 0.0, 0.0\n",
    "        tp1_tr, tp2_tr      = 0.0, 0.0\n",
    "        gan1_tr, gan2_tr    = 0.0, 0.0\n",
    "\n",
    "        # Train\n",
    "        generator_1.train()\n",
    "        discriminator_1.train()\n",
    "        generator_2.train()\n",
    "        discriminator_2.train()\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for i in tqdm(range(data_size), desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            data = train_adj.to(device)\n",
    "            \n",
    "            edge_idx_t0, edge_att_t0, _, _ = create_edge_index_attribute(data[i, 0])\n",
    "            data_t0 = Data(x=data[i, 0], edge_attr=edge_att_t0, edge_index=edge_idx_t0).to(device)\n",
    "            edge_idx_t1, edge_att_t1, _, _ = create_edge_index_attribute(data[i, 1])\n",
    "            data_t1 = Data(x=data[i, 1], edge_attr=edge_att_t1, edge_index=edge_idx_t1).to(device)\n",
    "            edge_idx_t2, edge_att_t2, _, _ = create_edge_index_attribute(data[i, 2])\n",
    "            data_t2 = Data(x=data[i, 2], edge_attr=edge_att_t2, edge_index=edge_idx_t2).to(device)\n",
    "            \n",
    "            ########################################################################################\n",
    "            ################################### 1st Part Training ##################################\n",
    "            ########################################################################################\n",
    "            # Train the discriminator\n",
    "            optimizer_D1.zero_grad()\n",
    "            fake_t1 = generator_1(data_t0).detach()\n",
    "            edge_idx_f1, edge_att_f1, _, _ = create_edge_index_attribute(fake_t1)\n",
    "            fake_data_t1 = Data(x=fake_t1, edge_attr=edge_att_f1, edge_index=edge_idx_f1).to(device)\n",
    "\n",
    "            # data      : Real source and real target\n",
    "            # fake_data : Real source and generated target\n",
    "            real_loss_1 = adversarial_loss(discriminator_1(data_t1, data_t0), real_label)\n",
    "            fake_loss_1 = adversarial_loss(discriminator_1(fake_data_t1, data_t0), fake_label)\n",
    "            loss_D1 = torch.mean(real_loss_1 + fake_loss_1) / 2\n",
    "            r1 += real_loss_1.item()\n",
    "            f1 += fake_loss_1.item()\n",
    "            d1 += loss_D1.item()\n",
    "\n",
    "            loss_D1.backward(retain_graph=True)\n",
    "            optimizer_D1.step()\n",
    "\n",
    "\n",
    "            # Train the generator\n",
    "            optimizer_G1.zero_grad()\n",
    "\n",
    "            # Adversarial Loss\n",
    "            fake_data_t1.x = generator_1(data_t0)\n",
    "            gan_loss_1 = torch.mean(adversarial_loss(discriminator_1(fake_data_t1, data_t0), real_label))\n",
    "            gan1_tr += gan_loss_1.item()\n",
    "\n",
    "            # KL Loss\n",
    "            kl_loss_1 = kl.kl_divergence(normal.Normal(fake_data_t1.x.mean(dim=1), fake_data_t1.x.std(dim=1)),\n",
    "                                       normal.Normal(data_t1.x.mean(dim=1), data_t1.x.std(dim=1))).sum()\n",
    "\n",
    "            # Topology Loss\n",
    "            tp_loss_1 = tp(fake_data_t1.x.sum(dim=-1), data_t0.x.sum(dim=-1))\n",
    "            tp1_tr += tp_loss_1.item()\n",
    "\n",
    "            # Identity Loss is included in the end\n",
    "            swapped_data = generator_1(data_t1)\n",
    "            # print(f'swapped_data shape: {swapped_data.shape}, swapped_data type: {type(swapped_data)}')\n",
    "            loss_G1 = (i_coeff * identity_loss(generator_1(data_t1), data_t1.x)) + (g_coeff * gan_loss_1) + (kl_coeff * kl_loss_1) + (tp_coeff * tp_loss_1)\n",
    "            g1 += loss_G1.item()\n",
    "            \n",
    "            loss_G1.backward(retain_graph=True)\n",
    "            optimizer_G1.step()\n",
    "            \n",
    "            k1_train += kl_loss_1.item()\n",
    "            mse_l1 += msel(generator_1(data_t0), data_t1.x).item()\n",
    "            mae_l1 += mael(generator_1(data_t0), data_t1.x).item()\n",
    "\n",
    "            ########################################################################################\n",
    "            ################################### 2nd Part Training ##################################\n",
    "            ########################################################################################\n",
    "            # Train the discriminator\n",
    "            optimizer_D2.zero_grad()\n",
    "            fake_t2 = generator_2(data_t1).detach()\n",
    "            edge_idx_f2, edge_att_f2, _, _ = create_edge_index_attribute(fake_t2)\n",
    "            fake_data_t2 = Data(x=fake_t2, edge_attr=edge_att_f2, edge_index=edge_idx_f2).to(device)\n",
    "\n",
    "            # data      : Real source and real target\n",
    "            # fake_data : Real source and generated target\n",
    "            real_loss_2 = adversarial_loss(discriminator_2(data_t2, data_t1), real_label)\n",
    "            fake_loss_2 = adversarial_loss(discriminator_2(fake_data_t2, data_t1), fake_label)\n",
    "            loss_D2 = torch.mean(real_loss_2 + fake_loss_2) / 2\n",
    "            r2 += real_loss_2.item()\n",
    "            f2 += fake_loss_2.item()\n",
    "            d2 += loss_D2.item()\n",
    "\n",
    "            loss_D2.backward(retain_graph=True)\n",
    "            optimizer_D2.step()\n",
    "\n",
    "\n",
    "            # Train the generator\n",
    "            optimizer_G2.zero_grad()\n",
    "\n",
    "            # Adversarial Loss\n",
    "            fake_data_t2.x = generator_2(data_t1)\n",
    "            gan_loss_2 = torch.mean(adversarial_loss(discriminator_2(fake_data_t2, data_t1), real_label))\n",
    "            gan2_tr += gan_loss_2.item()\n",
    "\n",
    "            # KL Loss\n",
    "            kl_loss_2 = kl.kl_divergence(normal.Normal(fake_data_t2.x.mean(dim=1), fake_data_t2.x.std(dim=1)),\n",
    "                                       normal.Normal(data_t2.x.mean(dim=1), data_t2.x.std(dim=1))).sum()\n",
    "\n",
    "            # Topology Loss\n",
    "            tp_loss_2 = tp(fake_data_t2.x.sum(dim=-1), data_t1.x.sum(dim=-1))\n",
    "            tp2_tr += tp_loss_2.item()\n",
    "\n",
    "            # Identity Loss is included in the end\n",
    "            loss_G2 = (i_coeff * identity_loss(generator_2(data_t2), data_t2.x)) + (g_coeff * gan_loss_2) + (kl_coeff * kl_loss_2) + (tp_coeff * tp_loss_2)\n",
    "            g2 += loss_G2.item()\n",
    "            \n",
    "            loss_G2.backward(retain_graph=True)\n",
    "            optimizer_G2.step()\n",
    "            \n",
    "            k2_train += kl_loss_2.item()\n",
    "            mse_l2 += msel(generator_2(data_t0), data_t2.x).item()\n",
    "            mae_l2 += mael(generator_2(data_t0), data_t2.x).item()\n",
    "\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time.append(epoch_end_time - epoch_start_time)\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=None) / 100 * psutil.virtual_memory().total / (1024**3))  # CPU usage in GB\n",
    "        memory_usage.append(psutil.virtual_memory().used / (1024**3))  # Memory usage in GB\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            gpu_usage.append(torch.cuda.memory_allocated(device) / (1024**3))  # GPU usage in GB\n",
    "        else:\n",
    "            gpu_usage.append(0)\n",
    "\n",
    "        d1 /= total_step\n",
    "        g1 /= total_step\n",
    "        d2 /= total_step\n",
    "        g2 /= total_step\n",
    "\n",
    "        d_loss.append(d1 + d2) \n",
    "        g_loss.append(g1 + g2)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        print(f'D1 Loss: {d1:.5f}, G1 Loss: {g1:.5f}, R1 Loss: {r1/total_step:.5f}, F1 Loss: {f1/total_step:.5f}, MSE: {mse_l1/total_step:.5f}, MAE: {mae_l1/total_step:.5f}')\n",
    "        print(f'D2 Loss: {d2:.5f}, G2 Loss: {g2:.5f}, R2 Loss: {r2/total_step:.5f}, F2 Loss: {f2/total_step:.5f}, MSE: {mse_l2/total_step:.5f}, MAE: {mae_l2/total_step:.5f}')\n",
    "        print(f'Time: {epoch_time[-1]:.2f}s, CPU: {cpu_usage[-1]:.2f}GB, Memory: {memory_usage[-1]:.2f}GB, GPU: {gpu_usage[-1]:.2f}GB\\n')\n",
    "        \n",
    "    # Plot the training losses\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, d_loss, label='Discriminator Loss (D)', marker='o', linestyle='-', color='b')\n",
    "    plt.plot(epochs, g_loss, label='Generator Loss (G)', marker='x', linestyle='-', color='r')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Line Chart of Discriminator Loss (D) and Generator Loss (G) per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    save_path_generator1 = save_path + '_model_generator1.pth'\n",
    "    torch.save(generator_1.state_dict(), save_path_generator1)\n",
    "    save_path_discriminator1 = save_path + '_model_discriminator1.pth'\n",
    "    torch.save(discriminator_1.state_dict(), save_path_discriminator1)\n",
    "\n",
    "    save_path_generator2 = save_path + '_model_generator2.pth'\n",
    "    torch.save(generator_2.state_dict(), save_path_generator2)\n",
    "    save_path_discriminator2 = save_path + '_model_discriminator2.pth'\n",
    "    torch.save(discriminator_2.state_dict(), save_path_discriminator2)\n",
    "    \n",
    "    \n",
    "    print(f'Model saved to {save_path}')\n",
    "\n",
    "    print(f'Average Time per Epoch: {np.mean(epoch_time):.2f}s')\n",
    "    print(f'Average CPU Usage: {np.mean(cpu_usage):.2f}GB')\n",
    "    print(f'Average Memory Usage: {np.mean(memory_usage):.2f}GB')\n",
    "    print(f'Average GPU Usage: {np.mean(gpu_usage):.2f}GB')\n",
    "\n",
    "    print(f'\\nTotal Training Time: {np.sum(epoch_time):.2f}s')\n",
    "    print(f'Max CPU Usage: {np.max(cpu_usage):.2f}GB')\n",
    "    print(f'Max Memory Usage: {np.max(memory_usage):.2f}GB')\n",
    "    print(f'Max GPU Usage: {np.max(gpu_usage):.2f}GB')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_task_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
